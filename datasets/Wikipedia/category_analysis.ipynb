{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook analyzes the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time, json, os, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_dir = '/media/mohit/E6A87A13A879E30B/WikipediaDataset/data/'\n",
    "NUM_PARTITIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 lines. 667920 categories found till now.\n",
      "Processed 2000000 lines. 907769 categories found till now.\n",
      "Finished part 0 in 1.015238106250763 minutes\n",
      "Processed 3000000 lines. 561780 categories found till now.\n",
      "Processed 4000000 lines. 844842 categories found till now.\n",
      "Finished part 1 in 1.8206225077311198 minutes\n",
      "Processed 5000000 lines. 417658 categories found till now.\n",
      "Processed 6000000 lines. 767803 categories found till now.\n",
      "Finished part 2 in 2.534004008769989 minutes\n",
      "Processed 7000000 lines. 474404 categories found till now.\n",
      "Processed 8000000 lines. 794315 categories found till now.\n",
      "Finished part 3 in 3.25574977795283 minutes\n",
      "Processed 9000000 lines. 503628 categories found till now.\n",
      "Processed 10000000 lines. 862307 categories found till now.\n",
      "Finished part 4 in 3.999419860045115 minutes\n",
      "Processed 11000000 lines. 505264 categories found till now.\n",
      "Processed 12000000 lines. 861881 categories found till now.\n",
      "Finished part 5 in 4.74501971801122 minutes\n",
      "Processed 13000000 lines. 488636 categories found till now.\n",
      "Processed 14000000 lines. 855068 categories found till now.\n",
      "Finished part 6 in 5.511939390500387 minutes\n",
      "Processed 15000000 lines. 432982 categories found till now.\n",
      "Processed 16000000 lines. 798166 categories found till now.\n",
      "Finished part 7 in 6.190997882684072 minutes\n",
      "Processed 17000000 lines. 472763 categories found till now.\n",
      "Processed 18000000 lines. 847965 categories found till now.\n",
      "Finished part 8 in 6.873469841480255 minutes\n",
      "Processed 19000000 lines. 419295 categories found till now.\n",
      "Processed 20000000 lines. 767809 categories found till now.\n",
      "Finished part 9 in 7.500650183359782 minutes\n"
     ]
    }
   ],
   "source": [
    "# combine information from full dataset and save category pages from the processed summaries\n",
    "\n",
    "\"\"\"\n",
    "prepare category page details\n",
    "1. List of sub-categories\n",
    "2. List of articles\n",
    "3. List of parent categories\n",
    "4. List of internal links on category page\n",
    "\n",
    "To parallelize, \n",
    "1. dump data for each partition separately\n",
    "2. process categories based on their hashes in N parts and dumps complete info for each category\n",
    "\"\"\"\n",
    "\n",
    "def get_empty_category_data():\n",
    "    return {\n",
    "        'sub_categories': set(),\n",
    "        'internal_links': [],\n",
    "        'articles': [],\n",
    "        'parent_categories': set()\n",
    "    }\n",
    "\n",
    "start_time = time.time()\n",
    "processed_line_count = 0\n",
    "for i in range(10):\n",
    "    category_name_to_data = {}\n",
    "    with open(data_root_dir + f'processed_summaries/part-{i}.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            page = json.loads(line)\n",
    "            namespace = page['namespace']\n",
    "            title = page['title']\n",
    "            page_id = page['page_id']\n",
    "            redirect_title = page['redirect_title']\n",
    "\n",
    "            if namespace == 14:\n",
    "                category_name_to_data[title] = category_name_to_data.get(title, get_empty_category_data())\n",
    "                category_data = category_name_to_data[title]\n",
    "                category_data['internal_links'].extend(page['internal_links'])\n",
    "                category_data['parent_categories'].update(page['categories'])\n",
    "                for parent_category in page['categories']:\n",
    "                    category_name_to_data[parent_category] = category_name_to_data.get(parent_category, get_empty_category_data())\n",
    "                    category_name_to_data[parent_category]['sub_categories'].add(title)\n",
    "            elif namespace == 0 and not redirect_title:\n",
    "                for category in page['categories']:\n",
    "                    category_name_to_data[category] = category_name_to_data.get(category, get_empty_category_data())\n",
    "                    category_name_to_data[category]['articles'].append((title, page_id))\n",
    "            processed_line_count += 1\n",
    "            if processed_line_count % 1000000 == 0:\n",
    "                print(f\"Processed {processed_line_count} lines. {len(category_name_to_data)} categories found till now.\")\n",
    "    os.makedirs(data_root_dir+'tmp/', exist_ok=True)\n",
    "    with open(data_root_dir + f'tmp/partial_category_page_data_part-{i}.txt', 'w') as out_f:\n",
    "        for cat_name, data in category_name_to_data.items():\n",
    "            data[\"category_name\"] = cat_name\n",
    "            data['sub_categories'] = list(data['sub_categories'])\n",
    "            data['parent_categories'] = list(data['parent_categories'])\n",
    "            out_f.write(json.dumps(data)+\"\\n\")\n",
    "    print(f\"Finished part {i} in {(time.time() - start_time) / 60} minutes\")\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished part 9 in 1.2498801310857137 minutes\n",
      "Finished part 9 in 2.463934926191966 minutes\n",
      "Finished part 9 in 3.403915067513784 minutes\n",
      "Finished part 9 in 4.313976740837097 minutes\n",
      "Finished part 9 in 5.202356016635894 minutes\n",
      "Finished part 9 in 6.087162383397421 minutes\n",
      "Finished part 9 in 6.9568502386411035 minutes\n",
      "Finished part 9 in 7.881366391976674 minutes\n",
      "Finished part 9 in 8.744562872250874 minutes\n",
      "Finished part 9 in 9.610439304510752 minutes\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(data_root_dir+'category_pages/', exist_ok=True)\n",
    "start_time = time.time()\n",
    "for partition in range(NUM_PARTITIONS):\n",
    "    category_name_to_data = {}\n",
    "    for i in range(10):\n",
    "        with open(data_root_dir + f'tmp/partial_category_page_data_part-{i}.txt', 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip()\n",
    "                if line==\"\": continue\n",
    "                data = json.loads(line)\n",
    "                if (hash(data['category_name']) % NUM_PARTITIONS) != partition:\n",
    "                    continue\n",
    "                if data['category_name'] not in category_name_to_data:\n",
    "                    category_name_to_data[data['category_name']] = get_empty_category_data()\n",
    "                category_data = category_name_to_data[data['category_name']]\n",
    "                category_data['sub_categories'].update(data['sub_categories'])\n",
    "                category_data['parent_categories'].update(data['parent_categories'])\n",
    "                category_data['internal_links'].extend(data['internal_links'])\n",
    "                category_data['articles'].extend(data['articles'])\n",
    "    with open(data_root_dir + f'category_pages/part-{partition}.txt', 'w') as out_f:\n",
    "        for cat_name, data in category_name_to_data.items():\n",
    "            data[\"category_name\"] = cat_name\n",
    "            data['sub_categories'] = list(data['sub_categories'])\n",
    "            data['parent_categories'] = list(data['parent_categories'])\n",
    "            out_f.write(json.dumps(data)+\"\\n\")\n",
    "    print(f\"Finished part {partition} in {(time.time() - start_time) / 60} minutes\")\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check for a few category pages that they look good\n",
    "\n",
    "# select_categories = [\"Platonic solids\", \"Sampling (statistics)\", \"Drama films\"]\n",
    "# randomly_sampled_category_pages = []\n",
    "# select_category_pages = []\n",
    "\n",
    "# start_time = time.time()\n",
    "# for partition in range(NUM_PARTITIONS):\n",
    "#     with open(data_root_dir + f'category_pages/part-{partition}.txt', 'r') as f:\n",
    "#         lines = [line for line in f.readlines() if line!='']\n",
    "#         for line in lines:\n",
    "#             data = json.loads(line)\n",
    "#             if data['category_name'] in select_categories:\n",
    "#                 select_category_pages.append(data)\n",
    "#         for line in random.sample(lines, 10):\n",
    "#             randomly_sampled_category_pages.append(json.loads(line))\n",
    "#     print(f\"Processed till part {partition} in {(time.time() - start_time) / 60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select_category_pages[2]\n",
    "# randomly_sampled_category_pages[67]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
