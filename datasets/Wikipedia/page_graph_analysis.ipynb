{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains data prep and analysis for the wikipedia page graph.\n",
    "\n",
    "Page graph is a directed graph where each node represents a page and each edge represents a hyperlink between two pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time, json, os, random, traceback, pyperclip, importlib, queue\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import src.wiki_analysis_utils as wiki_analysis_utils\n",
    "\n",
    "_ = importlib.reload(wiki_analysis_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_dir = r'C:\\Users\\mohitvyas\\MyDesktop\\WikipediaDataset\\data\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a small dataset for experimentation before jumping into the deep end.\n",
    "\n",
    "1. Start with all categories tagged on: https://en.wikipedia.org/wiki/Coriander\n",
    "2. ~~Do a BFS on undirected version of category graph starting from the above categories to obtain the category set of interest.~~ (This approach results in ~80% of all categories which is unhelpful. Intuition is to obtian small subgraph that is very similar to the seed page. Something like \"set of all wiki articles about plant species like coriander\")\n",
    "    - ~~Alternative approach-1: do BFS on parent directed graph to obtain all ancenstors. Then do BFS on child directed graph to obtain all descendants of these ancestors.~~ (This also doesn't work)\n",
    "    - Alternative approach-2: same as approach 1, except, keep only ancestors with <= N descendants. So total 2 constraints on anscenstors: (i) has <= N descendants (ii) contains at least 1 seed category in its descendants.\n",
    "3. Get all articles that has at least 1 category from the category set of interest.\n",
    "4. Keep only edges that are between articles in the above set to obtain the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_article_name = 'Coriander'\n",
    "target_dir = data_root_dir + 'PageSubGraphs\\\\' + \\\n",
    "    wiki_analysis_utils.normalized_page_name(target_article_name).replace(' ', '_') + '\\\\'\n",
    "os.makedirs(target_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_name_to_page_id, page_id_to_page_name, error_counts = \\\n",
    "    wiki_analysis_utils.load_page_name_to_id_map(data_root_dir, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load category name to id mappings from the category pages\n",
    "categories, failure_counts = wiki_analysis_utils\\\n",
    "    .load_category_name_to_id_map(data_root_dir, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_page_ids = set([page_name_to_page_id[wiki_analysis_utils.normalized_page_name(target_article_name)]])\n",
    "seed_categories = []\n",
    "silent = True\n",
    "start_time = time.time()\n",
    "for partition in range(10):\n",
    "    with open(data_root_dir + f'category_pages/part-{partition}.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            if line=='': continue\n",
    "            data = json.loads(line)\n",
    "            if 'category_id' in data:\n",
    "                for _, page_id in data['articles']:\n",
    "                    if page_id in target_page_ids:\n",
    "                        seed_categories.append(data['category_id'])\n",
    "                        break\n",
    "    if not silent:\n",
    "        print(f\"Processed till part {partition} in {(time.time() - start_time) / 60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 seed categories\n",
      "[\n",
      "    \"plants described in 1753\",\n",
      "    \"spices\",\n",
      "    \"indian spices\",\n",
      "    \"edible apiaceae\",\n",
      "    \"herbs\",\n",
      "    \"medicinal plants\",\n",
      "    \"plants used in native american cuisine\",\n",
      "    \"apioideae\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "seed_category_names = [categories['id_to_name'][cat_id] for cat_id in seed_categories]\n",
    "print (f\"Found {len(seed_categories)} seed categories\")\n",
    "print (json.dumps(seed_category_names, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500000 edges in 0.8095912933349609 minutes\n",
      "Loaded 1000000 edges in 1.434980277220408 minutes\n",
      "Loaded 1500000 edges in 2.0305140217145285 minutes\n",
      "Loaded 2000000 edges in 2.5617505431175234 minutes\n",
      "Loaded 2500000 edges in 3.1015053470929463 minutes\n",
      "Loaded 3000000 edges in 3.608826283613841 minutes\n",
      "Loaded 3500000 edges in 4.113653659820557 minutes\n",
      "Loaded 4000000 edges in 4.683315519491831 minutes\n",
      "Loaded 4500000 edges in 5.220084122816721 minutes\n"
     ]
    }
   ],
   "source": [
    "# load directed graphs for parent-child category relationships\n",
    "parent_graph_adj_lists = {}\n",
    "child_graph_adj_lists = {}\n",
    "n_edges_loaded = 0\n",
    "start_time = time.time()\n",
    "silent=False\n",
    "for _, row in pd.read_csv(data_root_dir + 'category_id_to_parent_category_ids.tsv', sep='\\t').iterrows():\n",
    "    cid1 = row['CategoryId']\n",
    "    cid2 = row['ParentCategoryId']\n",
    "    if cid1 not in parent_graph_adj_lists:\n",
    "        parent_graph_adj_lists[cid1] = []\n",
    "    parent_graph_adj_lists[cid1].append(cid2)\n",
    "    if cid2 not in child_graph_adj_lists:\n",
    "        child_graph_adj_lists[cid2] = []\n",
    "    child_graph_adj_lists[cid2].append(cid1)\n",
    "    n_edges_loaded += 1\n",
    "    if n_edges_loaded % 500000 == 0 and not silent:\n",
    "        print(f\"Loaded {n_edges_loaded} edges in {(time.time() - start_time) / 60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the graph is a DAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 8 seed categories\n",
      "Found 18895 ancestor categories\n"
     ]
    }
   ],
   "source": [
    "# BFS on the parent graph to find all the ancestors of the seed categories\n",
    "print (f\"Starting with {len(seed_categories)} seed categories\")\n",
    "ancestors = set(seed_categories)\n",
    "nodes_to_visit = queue.Queue()\n",
    "for seed_category in seed_categories:\n",
    "    nodes_to_visit.put(seed_category)\n",
    "while not nodes_to_visit.empty():\n",
    "    node = nodes_to_visit.get()\n",
    "    if node in parent_graph_adj_lists:\n",
    "        for parent in parent_graph_adj_lists[node]:\n",
    "            if parent not in ancestors:\n",
    "                ancestors.add(parent)\n",
    "                nodes_to_visit.put(parent)\n",
    "print(f\"Found {len(ancestors)} ancestor categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 222 root ancestors\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m ancestor, idx \u001b[38;5;241m=\u001b[39m call_stack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ancestor \u001b[38;5;129;01min\u001b[39;00m child_graph_adj_lists:\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mchild_graph_adj_lists\u001b[49m[ancestor]):\n\u001b[0;32m     15\u001b[0m         child \u001b[38;5;241m=\u001b[39m child_graph_adj_lists[ancestor][idx]\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ancestor_to_descendant_count:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# count number of descendants of each ancestor\n",
    "# first identify root ancestors (i.e. ones with no parents in the ancestor set)\n",
    "root_ancestors = set([ancestor for ancestor in ancestors if ancestor not in parent_graph_adj_lists])\n",
    "print(f\"Found {len(root_ancestors)} root ancestors\")\n",
    "\n",
    "node_to_descendant_count = {}\n",
    "node_to_seed_category_descendant_count = {}\n",
    "\n",
    "# recursively count descendants of each ancestor\n",
    "# simulate recursion with a stack to avoid stack overflow\n",
    "call_stack = [[root, 0] for root in root_ancestors]\n",
    "while len(call_stack) > 0:\n",
    "    node, idx = call_stack[-1]\n",
    "    if node in child_graph_adj_lists:\n",
    "        if idx < len(child_graph_adj_lists[node]):\n",
    "            child = child_graph_adj_lists[node][idx]\n",
    "            if child not in node_to_descendant_count:\n",
    "                call_stack.append([child, 0])\n",
    "            else:\n",
    "                call_stack[-1][1] += 1\n",
    "        else:\n",
    "            count = 1\n",
    "            for child in child_graph_adj_lists[node]:\n",
    "                count += node_to_descendant_count[child]\n",
    "            node_to_descendant_count[node] = count\n",
    "            call_stack.pop()\n",
    "    else:\n",
    "        node_to_descendant_count[node] = 1\n",
    "        call_stack.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42726194"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(call_stack)\n",
    "# len(ancestor_to_descendant_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
