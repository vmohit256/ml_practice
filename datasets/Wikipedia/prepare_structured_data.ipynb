{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook prepares structured data to fed for further analysis.\n",
    "It creates these datasets:\n",
    "\n",
    "Page based:\n",
    "\n",
    "page_info.parquet: \n",
    "PageId, PageName, TextLength, NumUniqueWords, NumFiles, \n",
    "NumExternalLinks, NumInfoBoxes, NumSections, Categories\n",
    "\n",
    "this is spark dataframe and partitioned on PageId.\n",
    "\n",
    "page_links.tsv:\n",
    "SourcePageId, DestinationPageId\n",
    "\n",
    "page_inlink_counts.tsv:\n",
    "PageId, NumInlinks\n",
    "\n",
    "Category based:\n",
    "\n",
    "-> Category pages\n",
    "\n",
    "These are jsons (one per line) with the following structure:\n",
    "{\n",
    "    \"category_name\": \"Category name\",\n",
    "    \"sub_categories\": [\"Subcategory 1\", \"Subcategory 2\", ...],\n",
    "    \"articles\": [(\"Article Title 1\", page_id_1), (\"Article Title 2\", page_id_2), ...],\n",
    "    \"parent_categories\": [\"Parent Category 1\", \"Parent Category 2\", ...],\n",
    "    \"internal_links\": [\"link 1\", \"link 2\", ...]\n",
    "}\n",
    "\n",
    "-> category_id_to_parent_category_ids.tsv\n",
    "\n",
    "This is a tsv file with the following structure:\n",
    "CategoryId, ParentCategoryId\n",
    "\n",
    "Run DFS on the category tree (starting from categories with no parents) to get the following data:\n",
    "\n",
    "-> category_id_to_dfs_metadata.tsv\n",
    "\n",
    "This is a tsv file with the following structure:\n",
    "CategoryId, DiscoveryTime, FinishingTime, Depth, NumDescendants\n",
    "\n",
    "-> category_edges_dfs_classification.tsv\n",
    "\n",
    "This is a tsv file with the following structure:\n",
    "CategoryId, SubCategoryId, EdgeType\n",
    "\n",
    "EdgeType is one of the following: TreeEdge, BackEdge, ForwardEdge, CrossEdge\n",
    "\n",
    "-> category_id_to_page_ids.tsv\n",
    "\n",
    "This is a tsv file with the following structure:\n",
    "CategoryId, PageId\n",
    "\n",
    "Note: this mapping is as-is from the category json. If a page is in category c then it may \n",
    "or may not be in its parent categories.\n",
    "\n",
    "-> category_id_to_stats.tsv\n",
    "\n",
    "This is a tsv file with the following structure:\n",
    "CategoryId, NumPages, NumChildCategories, NumParentCategories, NumDescendantCategories\n",
    "\n",
    "Note: NumPages is the sum of number of pages in the category and its descendants.\n",
    "\"\"\"\n",
    "_ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time, json, os, random, traceback, pyperclip, importlib\n",
    "import src.wiki_analysis_utils as wiki_analysis_utils\n",
    "from collections import Counter\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "import json\n",
    "\n",
    "_ = importlib.reload(wiki_analysis_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_dir = r'C:\\Users\\mohitvyas\\MyDesktop\\WikipediaDataset\\data\\\\'\n",
    "NUM_DASK_PARTITIONS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"PrepareStructuredData\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "numbers_rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "squared_numbers_rdd = numbers_rdd.map(lambda x: x*x)\n",
    "print(squared_numbers_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o90.saveAsTable.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 119 in stage 3.0 failed 1 times, most recent failure: Lost task 119.0 in stage 3.0 (TID 136) (MININT-FGLE9V0 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:511)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:229)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:183)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:700)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:678)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:571)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 22\u001b[0m\n\u001b[0;32m     16\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\\\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;241m.\u001b[39mtextFile(data_root_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_summaries/part-*.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;241m.\u001b[39mflatMap(process_line)\\\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;241m.\u001b[39mtoDF(schema)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Write the DataFrame to a Parquet file, partitioned by PageId\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbucketBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPageId\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msortBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPageId\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_root_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstructured_data/page_info\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpage_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mohitvyas\\Miniconda3\\envs\\py39\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1586\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[1;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m-> 1586\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mohitvyas\\Miniconda3\\envs\\py39\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\mohitvyas\\Miniconda3\\envs\\py39\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\mohitvyas\\Miniconda3\\envs\\py39\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o90.saveAsTable.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 119 in stage 3.0 failed 1 times, most recent failure: Lost task 119.0 in stage 3.0 (TID 136) (MININT-FGLE9V0 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:511)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:229)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:183)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:700)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:678)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:571)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save page_info.parquet: \n",
    "PageName, RedirectTitle, PageId, TextLength, NumInternalLinks, NumUniqueWords, NumFiles,\n",
    "NumExternalLinks, NumInfoBoxes, NumSections\n",
    "\"\"\"\n",
    "\n",
    "schema = [\"PageName\", \"RedirectTitle\", \"PageId\", \"TextLength\", \"NumInternalLinks\", \"NumUniqueWords\", \"NumFiles\", \"NumExternalLinks\", \"NumInfoBoxes\", \"NumSections\", \"Categories\"]\n",
    "\n",
    "def process_line(line):\n",
    "    page = json.loads(line)\n",
    "    if page['namespace'] == 0:\n",
    "        return [(page['title'], page.get('redirect_title', None), page['page_id'], page.get('text_length', None), len(page.get('internal_links', [])), page.get('num_unique_words', None), page.get('number_of_files', None), page.get('number_of_external_links', None), page.get('number_of_info_boxes', None), page.get('number_of_sections', None), json.dumps(page.get('categories', [])))]\n",
    "    return []\n",
    "\n",
    "# Read and process the files\n",
    "df = spark.sparkContext\\\n",
    "    .textFile(data_root_dir + 'processed_summaries/part-*.txt')\\\n",
    "    .flatMap(process_line)\\\n",
    "    .toDF(schema)\n",
    "\n",
    "# Write the DataFrame to a Parquet file, partitioned by PageId\n",
    "df.write\\\n",
    "    .bucketBy(20, \"PageId\")\\\n",
    "    .sortBy(\"PageId\")\\\n",
    "    .option(\"path\", data_root_dir + f'structured_data/page_info')\\\n",
    "    .saveAsTable(\"page_info\", format='parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 lines. Fail count: 0. Success count: 1000000. Time taken: 0.03520254294077555 minutes.\n",
      "Processed 2000000 lines. Fail count: 0. Success count: 2000000. Time taken: 0.0746461828549703 minutes.\n",
      "Processed 3000000 lines. Fail count: 0. Success count: 3000000. Time taken: 0.11801429986953735 minutes.\n",
      "Processed 4000000 lines. Fail count: 0. Success count: 4000000. Time taken: 0.17254188060760497 minutes.\n",
      "Processed 5000000 lines. Fail count: 0. Success count: 5000000. Time taken: 0.224252720673879 minutes.\n",
      "Processed 6000000 lines. Fail count: 0. Success count: 6000000. Time taken: 0.27663596868515017 minutes.\n",
      "Processed 7000000 lines. Fail count: 0. Success count: 7000000. Time taken: 0.3174879471460978 minutes.\n",
      "Processed 8000000 lines. Fail count: 0. Success count: 8000000. Time taken: 0.36652742624282836 minutes.\n",
      "Processed 9000000 lines. Fail count: 0. Success count: 9000000. Time taken: 0.4145349780718485 minutes.\n",
      "Processed 10000000 lines. Fail count: 0. Success count: 10000000. Time taken: 0.467132031917572 minutes.\n",
      "Processed 11000000 lines. Fail count: 0. Success count: 11000000. Time taken: 0.5194503982861837 minutes.\n",
      "Processed 12000000 lines. Fail count: 0. Success count: 12000000. Time taken: 0.5855878750483196 minutes.\n",
      "Processed 13000000 lines. Fail count: 0. Success count: 13000000. Time taken: 0.6316748340924581 minutes.\n",
      "Processed 14000000 lines. Fail count: 0. Success count: 14000000. Time taken: 0.682768185933431 minutes.\n",
      "Processed 15000000 lines. Fail count: 0. Success count: 15000000. Time taken: 0.7293232719103495 minutes.\n",
      "Processed 16000000 lines. Fail count: 0. Success count: 16000000. Time taken: 0.7772458314895629 minutes.\n",
      "Processed 17000000 lines. Fail count: 0. Success count: 17000000. Time taken: 0.8252223014831543 minutes.\n",
      "Failed to parse 0 / 17753527 (0.0%) lines.\n"
     ]
    }
   ],
   "source": [
    "page_name_to_page_id, page_id_to_page_name, error_counts = \\\n",
    "    wiki_analysis_utils.load_page_name_to_id_map(data_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16812294, 17753527, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(page_name_to_page_id), len(page_id_to_page_name), len(error_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 lines. Fail count: 70. Success count: 999930. Time taken: 0.03364824851353963 minutes.\n",
      "Processed 2000000 lines. Fail count: 204. Success count: 1999796. Time taken: 0.06828335920969646 minutes.\n",
      "Processed 3000000 lines. Fail count: 391. Success count: 2999609. Time taken: 0.10596334934234619 minutes.\n",
      "Processed 4000000 lines. Fail count: 587. Success count: 3999413. Time taken: 0.1513728141784668 minutes.\n",
      "Processed 5000000 lines. Fail count: 770. Success count: 4999230. Time taken: 0.19528584082921346 minutes.\n",
      "Processed 6000000 lines. Fail count: 956. Success count: 5999044. Time taken: 0.23620920578638713 minutes.\n",
      "Processed 7000000 lines. Fail count: 1182. Success count: 6998818. Time taken: 0.2848101774851481 minutes.\n",
      "Processed 8000000 lines. Fail count: 1470. Success count: 7998530. Time taken: 0.33345146973927814 minutes.\n",
      "Processed 9000000 lines. Fail count: 1709. Success count: 8998291. Time taken: 0.3767818530400594 minutes.\n",
      "Processed 10000000 lines. Fail count: 1973. Success count: 9998027. Time taken: 0.42707310914993285 minutes.\n",
      "Processed 11000000 lines. Fail count: 2603. Success count: 10997397. Time taken: 0.46978972752889 minutes.\n",
      "Processed 12000000 lines. Fail count: 2918. Success count: 11997082. Time taken: 0.5130100170771281 minutes.\n",
      "Processed 13000000 lines. Fail count: 3418. Success count: 12996582. Time taken: 0.5690348029136658 minutes.\n",
      "Processed 14000000 lines. Fail count: 27284. Success count: 13972716. Time taken: 0.6685067017873129 minutes.\n",
      "Processed 15000000 lines. Fail count: 28227. Success count: 14971773. Time taken: 0.7207252621650696 minutes.\n",
      "Processed 16000000 lines. Fail count: 28801. Success count: 15971199. Time taken: 0.7705979307492574 minutes.\n",
      "Processed 17000000 lines. Fail count: 29377. Success count: 16970623. Time taken: 0.829608698685964 minutes.\n"
     ]
    }
   ],
   "source": [
    "# load source_page_id to destination_page_id for redirect pages\n",
    "# this will be used to resolve redirects in the cells below\n",
    "\n",
    "redirect_pid_to_pid, error_counts = \\\n",
    "    wiki_analysis_utils.load_page_redirect_mapping(data_root_dir, page_name_to_page_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10904676, 3690970, 29764)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(redirect_pid_to_pid), len(set(redirect_pid_to_pid.values())), len(error_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 lines. 520258 pages found till now.\n",
      "Processed 2000000 lines. 967758 pages found till now.\n",
      "Finished part 0 in 7.403570981820424 minutes\n",
      "Processed 3000000 lines. 1375964 pages found till now.\n",
      "Processed 4000000 lines. 1777375 pages found till now.\n",
      "Finished part 1 in 10.953632120291392 minutes\n",
      "Processed 5000000 lines. 2103953 pages found till now.\n",
      "Processed 6000000 lines. 2488972 pages found till now.\n",
      "Finished part 2 in 13.118326369921366 minutes\n",
      "Processed 7000000 lines. 2860437 pages found till now.\n",
      "Processed 8000000 lines. 3191915 pages found till now.\n",
      "Finished part 3 in 15.521819802125295 minutes\n",
      "Processed 9000000 lines. 3503469 pages found till now.\n",
      "Processed 10000000 lines. 3842350 pages found till now.\n",
      "Finished part 4 in 17.97135885953903 minutes\n",
      "Processed 11000000 lines. 4156171 pages found till now.\n",
      "Processed 12000000 lines. 4498292 pages found till now.\n",
      "Finished part 5 in 19.871130939324697 minutes\n",
      "Processed 13000000 lines. 4834557 pages found till now.\n",
      "Processed 14000000 lines. 5113268 pages found till now.\n",
      "Finished part 6 in 21.65992443561554 minutes\n",
      "Processed 15000000 lines. 5398390 pages found till now.\n",
      "Processed 16000000 lines. 5697508 pages found till now.\n",
      "Finished part 7 in 23.301409379641214 minutes\n",
      "Processed 17000000 lines. 6000460 pages found till now.\n",
      "Processed 18000000 lines. 6302727 pages found till now.\n",
      "Finished part 8 in 25.027618098258973 minutes\n",
      "Processed 19000000 lines. 6544333 pages found till now.\n",
      "Processed 20000000 lines. 6794797 pages found till now.\n",
      "Finished part 9 in 26.439934555689494 minutes\n",
      "Failure counts: {'Unknown Page': 11618884}\n",
      "11618884 / 229492654 (5.062856608909146%) links couldn't be captured.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save page_links.tsv:\n",
    "SourcePageId, DestinationPageId\n",
    "\"\"\"\n",
    "\n",
    "start_time = time.time()\n",
    "processed_line_count = 0\n",
    "num_pages_processed = 0\n",
    "failure_counts = {\n",
    "    'Unknown Page': 0\n",
    "}\n",
    "total_link_count = 0\n",
    "log_level = 'ERROR'\n",
    "with open(data_root_dir + 'page_links.tsv', 'w') as page_links_f:\n",
    "    page_links_f.write(\"SourcePageId\\tDestinationPageId\\n\")\n",
    "    for i in range(10):\n",
    "        with open(data_root_dir + f'processed_summaries/part-{i}.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                page = json.loads(line)\n",
    "                namespace = page['namespace']\n",
    "                title = page['title']\n",
    "                page_id = page['page_id']\n",
    "                redirect_title = page['redirect_title']\n",
    "\n",
    "                if namespace == 0 and not redirect_title:\n",
    "                    for link in page['internal_links']:\n",
    "                        link = wiki_analysis_utils.normalized_page_name(link)\n",
    "                        if link.startswith(':category:'):\n",
    "                            continue\n",
    "                        total_link_count += 1\n",
    "                        if link not in page_name_to_page_id:\n",
    "                            if log_level in ['WARN', 'INFO']:\n",
    "                                print(f\"Page {link} in page {page_id} not found in page name to id mapping.\")\n",
    "                            failure_counts['Unknown Page'] += 1\n",
    "                            continue\n",
    "                        dest_page_id = page_name_to_page_id[link]\n",
    "                        while dest_page_id in redirect_pid_to_pid:\n",
    "                            dest_page_id = redirect_pid_to_pid[dest_page_id]\n",
    "                        page_links_f.write(f\"{page_id}\\t{dest_page_id}\\n\")\n",
    "                    num_pages_processed += 1\n",
    "                processed_line_count += 1\n",
    "                if processed_line_count % 1000000 == 0:\n",
    "                    print(f\"Processed {processed_line_count} lines. {num_pages_processed} pages found till now.\")\n",
    "                    #break\n",
    "\n",
    "        print(f\"Finished part {i} in {(time.time() - start_time) / 60} minutes\")\n",
    "        #break\n",
    "\n",
    "print (f\"Failure counts: {failure_counts}\")\n",
    "print (f\"{sum(failure_counts.values())} / {total_link_count} ({(sum(failure_counts.values()) / total_link_count) * 100}%) links couldn't be captured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303    826\n",
       "316    802\n",
       "307    549\n",
       "308    514\n",
       "305    428\n",
       "12     387\n",
       "290    242\n",
       "39     146\n",
       "309    106\n",
       "Name: SourcePageId, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = pd.read_csv(data_root_dir + 'page_links.tsv', sep='\\t', nrows=4000)\n",
    "pdf['SourcePageName'] = pdf['SourcePageId'].map(page_id_to_page_name)\n",
    "pdf['DestinationPageName'] = pdf['DestinationPageId'].map(page_id_to_page_name)\n",
    "pdf['SourcePageId'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a (cyrillic)',\n",
       " 'a (cyrillic)',\n",
       " 'a (indic)',\n",
       " 'a with breve (cyrillic)',\n",
       " 'a-list',\n",
       " 'aardvark',\n",
       " 'abo blood group system',\n",
       " 'afrikaans',\n",
       " 'aleph',\n",
       " 'aleph',\n",
       " 'aleph',\n",
       " 'aleph number',\n",
       " 'algebra',\n",
       " 'allograph',\n",
       " 'alpha',\n",
       " 'alpha',\n",
       " 'alphabet',\n",
       " 'anarchist symbolism',\n",
       " 'ancient greece',\n",
       " 'angstrom',\n",
       " 'ansuz (rune)',\n",
       " 'apple',\n",
       " 'argentine austral',\n",
       " 'argentine austral',\n",
       " 'armenian alphabet',\n",
       " 'article (grammar)',\n",
       " 'ascii',\n",
       " 'asymmetry',\n",
       " 'at sign',\n",
       " 'at sign',\n",
       " 'australian english',\n",
       " 'ayb (armenian letter)',\n",
       " 'ayb (armenian letter)',\n",
       " 'azerbaijani language',\n",
       " 'bar (diacritic)',\n",
       " 'bashkir language',\n",
       " 'bengali alphabet',\n",
       " 'blackletter',\n",
       " 'bra',\n",
       " 'bulgarian language',\n",
       " 'carolingian minuscule',\n",
       " 'caron',\n",
       " 'catalan dialects',\n",
       " 'catalan dialects',\n",
       " 'catalan language',\n",
       " 'chemnitz dialect',\n",
       " 'chuvash language',\n",
       " 'close-mid front unrounded vowel',\n",
       " 'code point',\n",
       " 'combining character',\n",
       " 'consonant',\n",
       " 'coptic script',\n",
       " 'cursive',\n",
       " 'cyrillic script',\n",
       " 'czech language',\n",
       " 'danish language',\n",
       " 'decimal',\n",
       " 'diacritic',\n",
       " 'diphthong',\n",
       " 'dot (diacritic)',\n",
       " 'double grave accent',\n",
       " 'dutch dialects',\n",
       " 'dutch language',\n",
       " 'eau (trigraph)',\n",
       " 'egyptian hieroglyphs',\n",
       " 'emilian dialects',\n",
       " 'enclosed alphanumerics',\n",
       " 'enclosed alphanumerics',\n",
       " 'enclosed alphanumerics',\n",
       " 'english alphabet',\n",
       " 'english alphabet',\n",
       " 'english articles',\n",
       " 'english language',\n",
       " 'english language in northern england',\n",
       " 'english language in southern england',\n",
       " 'english orthography',\n",
       " 'english-language vowel changes before historic /r/',\n",
       " 'etruscan alphabet',\n",
       " 'etruscan civilization',\n",
       " 'finnish language',\n",
       " 'first-order logic',\n",
       " 'french language',\n",
       " 'french orthography',\n",
       " 'galician language',\n",
       " 'general american english',\n",
       " 'geometry',\n",
       " 'german language',\n",
       " 'german orthography',\n",
       " 'geez script',\n",
       " 'glottal stop',\n",
       " 'glyph',\n",
       " 'gothic alphabet',\n",
       " 'gothic alphabet',\n",
       " 'great vowel shift',\n",
       " 'greek alphabet',\n",
       " 'greek dark ages',\n",
       " 'gujarati script',\n",
       " 'halfwidth and fullwidth forms',\n",
       " 'hamza',\n",
       " 'handwriting',\n",
       " 'hexadecimal',\n",
       " 'homoglyph',\n",
       " 'hungarian language',\n",
       " 'indo-european studies',\n",
       " 'indonesian language',\n",
       " 'insular script',\n",
       " 'international phonetic alphabet',\n",
       " 'inverted breve',\n",
       " 'iso/iec 8859',\n",
       " 'italian language',\n",
       " 'italian peninsula',\n",
       " 'italic type',\n",
       " 'italic type',\n",
       " 'kaingang language',\n",
       " 'kazakh language',\n",
       " 'kedah malay',\n",
       " 'kurdish language',\n",
       " 'latin',\n",
       " 'latin',\n",
       " 'latin alpha',\n",
       " 'latin alpha',\n",
       " 'latin alphabet',\n",
       " 'latin script',\n",
       " 'letter (alphabet)',\n",
       " 'letter case',\n",
       " 'letter case',\n",
       " 'letter case',\n",
       " 'ligature (writing)',\n",
       " 'limburgish',\n",
       " 'line (geometry)',\n",
       " 'line (geometry)',\n",
       " 'line segment',\n",
       " 'list of latin-script digraphs',\n",
       " 'list of latin-script digraphs',\n",
       " 'list of latin-script digraphs',\n",
       " 'list of latin-script digraphs',\n",
       " 'lithuanian language',\n",
       " 'luxembourgish',\n",
       " 'maastrichtian dialect',\n",
       " 'malay language',\n",
       " 'mapuche language',\n",
       " 'mathematical alphanumeric symbols',\n",
       " 'merovingian script',\n",
       " 'mid central vowel',\n",
       " 'middle english phonology',\n",
       " 'motivation',\n",
       " 'near-open central vowel',\n",
       " 'near-open front unrounded vowel',\n",
       " 'new zealand english',\n",
       " 'norwegian language',\n",
       " 'obsolete and nonstandard symbols in the international phonetic alphabet',\n",
       " 'old italic scripts',\n",
       " 'old italic scripts',\n",
       " 'open back rounded vowel',\n",
       " 'open back unrounded vowel',\n",
       " 'open central unrounded vowel',\n",
       " 'open front unrounded vowel',\n",
       " 'open-mid back rounded vowel',\n",
       " 'open-mid back unrounded vowel',\n",
       " 'ordinal indicator',\n",
       " 'ordinal indicator',\n",
       " 'perak malay',\n",
       " 'phoenician alphabet',\n",
       " 'phonetic transcription',\n",
       " 'pictogram',\n",
       " 'pinyin',\n",
       " 'polish language',\n",
       " 'portuguese language',\n",
       " 'portuguese orthography',\n",
       " 'precomposed character',\n",
       " 'proto-sinaitic script',\n",
       " 'r-colored vowel',\n",
       " 'received pronunciation',\n",
       " 'ren descartes',\n",
       " 'ring (diacritic)',\n",
       " 'ring (diacritic)',\n",
       " 'roman empire',\n",
       " 'runes',\n",
       " 'russian language',\n",
       " 'saanich dialect',\n",
       " 'saanich dialect',\n",
       " 'samuel johnson',\n",
       " 'schwa (cyrillic)',\n",
       " 'serif',\n",
       " 'spanish language',\n",
       " 'spanish orthography',\n",
       " 'standard chinese',\n",
       " 'stavangersk',\n",
       " 'swedish language',\n",
       " 'syllable',\n",
       " 'tagalog language',\n",
       " 'tau (disambiguation)',\n",
       " 'terengganu malay',\n",
       " 'teuthonista',\n",
       " 'the times of israel',\n",
       " 'transylvanian varieties of romanian',\n",
       " 'triangle',\n",
       " 'turkish alphabet',\n",
       " 'turned a',\n",
       " 'turned a',\n",
       " 'turned v',\n",
       " 'turned v',\n",
       " 'ugaritic',\n",
       " 'ugaritic alphabet',\n",
       " 'ukrainian language',\n",
       " 'ulster irish',\n",
       " 'uncial script',\n",
       " 'uncial script',\n",
       " 'unicode',\n",
       " 'unicode subscripts and superscripts',\n",
       " 'universal quantification',\n",
       " 'uralic phonetic alphabet',\n",
       " 'variable (mathematics)',\n",
       " 'visigothic script',\n",
       " 'vowel',\n",
       " 'west frisian language',\n",
       " 'x-sampa',\n",
       " 'ya (cyrillic)',\n",
       " 'zetaraka dialect',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internal_links = pdf.query('SourcePageId==290')['DestinationPageName'].to_list()\n",
    "print (len(internal_links))\n",
    "pyperclip.copy(json.dumps(sorted(internal_links), indent=2))\n",
    "sorted(internal_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000000 lines. Time taken: 0.2552937984466553 minutes.\n",
      "Processed 20000000 lines. Time taken: 0.4910214861234029 minutes.\n",
      "Processed 30000000 lines. Time taken: 0.7426513274510701 minutes.\n",
      "Processed 40000000 lines. Time taken: 1.0243276198705038 minutes.\n",
      "Processed 50000000 lines. Time taken: 1.3267032305399578 minutes.\n",
      "Processed 60000000 lines. Time taken: 1.6137513518333435 minutes.\n",
      "Processed 70000000 lines. Time taken: 1.9017590920130412 minutes.\n",
      "Processed 80000000 lines. Time taken: 2.191321623325348 minutes.\n",
      "Processed 90000000 lines. Time taken: 2.490134263038635 minutes.\n",
      "Processed 100000000 lines. Time taken: 2.7884846766789755 minutes.\n",
      "Processed 110000000 lines. Time taken: 3.0721547563870746 minutes.\n",
      "Processed 120000000 lines. Time taken: 3.351078001658122 minutes.\n",
      "Processed 130000000 lines. Time taken: 3.6386491854985556 minutes.\n",
      "Processed 140000000 lines. Time taken: 3.9113255540529885 minutes.\n",
      "Processed 150000000 lines. Time taken: 4.21607338587443 minutes.\n",
      "Processed 160000000 lines. Time taken: 4.496213320891062 minutes.\n",
      "Processed 170000000 lines. Time taken: 4.777543584505717 minutes.\n",
      "Processed 180000000 lines. Time taken: 5.058374945322672 minutes.\n",
      "Processed 190000000 lines. Time taken: 5.3447688817977905 minutes.\n",
      "Processed 200000000 lines. Time taken: 5.633587229251861 minutes.\n",
      "Processed 210000000 lines. Time taken: 5.937698400020599 minutes.\n",
      "Processed 217873771 lines. Time taken: 6.183644851048787 minutes.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save page_inlink_counts.tsv:\n",
    "PageId, NumInlinks\n",
    "\"\"\"\n",
    "\n",
    "page_inlink_counts = {}\n",
    "start_time = time.time()\n",
    "line_no = 0\n",
    "with open(data_root_dir + 'page_links.tsv', 'r') as f:\n",
    "    for line in f:\n",
    "        line_no += 1\n",
    "        if line_no==1: continue\n",
    "        source_page, destination_page = line.split('\\t')\n",
    "        destination_page = int(destination_page)\n",
    "        page_inlink_counts[destination_page] = page_inlink_counts.get(destination_page, 0) + 1\n",
    "        if line_no % 10000000 == 0:\n",
    "            print(f\"Processed {line_no} lines. Time taken: {(time.time() - start_time) / 60} minutes.\")\n",
    "            # break\n",
    "\n",
    "print (f\"Processed {line_no} lines. Time taken: {(time.time() - start_time) / 60} minutes.\")\n",
    "\n",
    "with open(data_root_dir + 'page_inlink_counts.tsv', 'w') as f:\n",
    "    f.write(\"PageId\\tNumInlinks\\n\")\n",
    "    for page_id, num_inlinks in sorted(page_inlink_counts.items()):\n",
    "        f.write(f\"{page_id}\\t{num_inlinks}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6349743"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(page_inlink_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 lines in 0.6364568471908569 minutes.\n",
      "Processed 2000000 lines in 1.1012573758761088 minutes.\n",
      "Dumping data for part 0 containing 934933 category partial jsons.\n",
      "Finished part 0 in 1.5530202349026998 minutes\n",
      "Processed 3000000 lines in 1.9946431159973144 minutes.\n",
      "Processed 4000000 lines in 2.4322856227556864 minutes.\n",
      "Dumping data for part 1 containing 885607 category partial jsons.\n",
      "Finished part 1 in 2.8973546028137207 minutes\n",
      "Processed 5000000 lines in 3.1667434334754945 minutes.\n",
      "Processed 6000000 lines in 3.5835325280825296 minutes.\n",
      "Dumping data for part 2 containing 830371 category partial jsons.\n",
      "Finished part 2 in 3.9203997294108075 minutes\n",
      "Processed 7000000 lines in 4.218890984853108 minutes.\n",
      "Processed 8000000 lines in 4.613288362820943 minutes.\n",
      "Dumping data for part 3 containing 855587 category partial jsons.\n",
      "Finished part 3 in 4.940384646256764 minutes\n",
      "Processed 9000000 lines in 5.235450784365336 minutes.\n",
      "Processed 10000000 lines in 5.667127799987793 minutes.\n",
      "Dumping data for part 4 containing 945175 category partial jsons.\n",
      "Finished part 4 in 6.02795999844869 minutes\n",
      "Processed 11000000 lines in 6.366511865456899 minutes.\n",
      "Processed 12000000 lines in 6.740390900770823 minutes.\n",
      "Dumping data for part 5 containing 963213 category partial jsons.\n",
      "Finished part 5 in 7.116111703713735 minutes\n",
      "Processed 13000000 lines in 7.420632270971934 minutes.\n",
      "Processed 14000000 lines in 7.824355951944987 minutes.\n",
      "Dumping data for part 6 containing 980604 category partial jsons.\n",
      "Finished part 6 in 8.205620956420898 minutes\n",
      "Processed 15000000 lines in 8.525156577428183 minutes.\n",
      "Processed 16000000 lines in 8.873788162072499 minutes.\n",
      "Dumping data for part 7 containing 879264 category partial jsons.\n",
      "Finished part 7 in 9.267593530813853 minutes\n",
      "Processed 17000000 lines in 9.57073496580124 minutes.\n",
      "Processed 18000000 lines in 9.993673006693522 minutes.\n",
      "Dumping data for part 8 containing 900920 category partial jsons.\n",
      "Finished part 8 in 10.310697865486144 minutes\n",
      "Processed 19000000 lines in 10.641562215487163 minutes.\n",
      "Processed 20000000 lines in 10.99091682434082 minutes.\n",
      "Dumping data for part 9 containing 795220 category partial jsons.\n",
      "Finished part 9 in 11.249552460511525 minutes\n"
     ]
    }
   ],
   "source": [
    "# combine information from full dataset and save category pages from the processed summaries\n",
    "\n",
    "\"\"\"\n",
    "prepare category page details\n",
    "1. List of sub-categories\n",
    "2. List of articles\n",
    "3. List of parent categories\n",
    "4. List of internal links on category page\n",
    "\n",
    "To parallelize, \n",
    "1. dump data for each partition separately\n",
    "2. process categories based on their hashes in N parts and dumps complete info for each category\n",
    "\"\"\"\n",
    "\n",
    "def get_empty_category_data():\n",
    "    return {\n",
    "        'sub_categories': set(),\n",
    "        'internal_links': [],\n",
    "        'articles': [],\n",
    "        'parent_categories': set()\n",
    "    }\n",
    "\n",
    "start_time = time.time()\n",
    "processed_line_count = 0\n",
    "norm = wiki_analysis_utils.normalized_page_name\n",
    "for i in range(10):\n",
    "    category_name_to_data = {}\n",
    "    with open(data_root_dir + f'processed_summaries/part-{i}.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            page = json.loads(line)\n",
    "            namespace = page['namespace']\n",
    "            title = page['title']\n",
    "            page_id = page['page_id']\n",
    "            redirect_title = page['redirect_title']\n",
    "\n",
    "            if namespace == 14:\n",
    "                category_name = norm(title)\n",
    "                category_name_to_data[category_name] = category_name_to_data\\\n",
    "                    .get(category_name, get_empty_category_data())\n",
    "                category_data = category_name_to_data[category_name]\n",
    "                category_data['internal_links'].extend(map(norm, page['internal_links']))\n",
    "                category_data['parent_categories'].update(map(norm, page['categories']))\n",
    "                category_data['category_id'] = int(page_id)\n",
    "                for parent_category in page['categories']:\n",
    "                    parent_category = norm(parent_category)\n",
    "                    category_name_to_data[parent_category] = category_name_to_data\\\n",
    "                        .get(parent_category, get_empty_category_data())\n",
    "                    category_name_to_data[parent_category]['sub_categories'].add(category_name)\n",
    "            elif namespace == 0 and not redirect_title:\n",
    "                for category in page['categories']:\n",
    "                    category = norm(category)\n",
    "                    category_name_to_data[category] = category_name_to_data\\\n",
    "                        .get(category, get_empty_category_data())\n",
    "                    page_id = int(page_id)\n",
    "                    page_id = redirect_pid_to_pid.get(page_id, page_id)\n",
    "                    category_name_to_data[category]['articles'].append((title, page_id))\n",
    "            processed_line_count += 1\n",
    "            if processed_line_count % 1000000 == 0:\n",
    "                print(f\"Processed {processed_line_count} lines in {(time.time() - start_time) / 60} minutes.\")\n",
    "    os.makedirs(data_root_dir+'tmp/', exist_ok=True)\n",
    "    print (f\"Dumping data for part {i} containing {len(category_name_to_data)} category partial jsons.\")\n",
    "    with open(data_root_dir + f'tmp/partial_category_page_data_part-{i}.txt', 'w') as out_f:\n",
    "        for cat_name, data in category_name_to_data.items():\n",
    "            data[\"category_name\"] = cat_name\n",
    "            data['sub_categories'] = list(data['sub_categories'])\n",
    "            data['parent_categories'] = list(data['parent_categories'])\n",
    "            out_f.write(json.dumps(data)+\"\\n\")\n",
    "    print(f\"Finished part {i} in {(time.time() - start_time) / 60} minutes\")\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished part 0 in 2.061130122343699 minutes\n",
      "Finished part 1 in 3.9293649673461912 minutes\n",
      "Finished part 2 in 5.823503355185191 minutes\n",
      "Finished part 3 in 8.542663621902467 minutes\n",
      "Finished part 4 in 10.408614214261373 minutes\n",
      "Finished part 5 in 12.252984670797984 minutes\n",
      "Finished part 6 in 14.039662718772888 minutes\n",
      "Finished part 7 in 15.66956444978714 minutes\n",
      "Finished part 8 in 17.287773271401722 minutes\n",
      "Finished part 9 in 18.950868968168894 minutes\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(data_root_dir+'category_pages/', exist_ok=True)\n",
    "start_time = time.time()\n",
    "for partition in range(NUM_PARTITIONS):\n",
    "    category_name_to_data = {}\n",
    "    for i in range(10):\n",
    "        with open(data_root_dir + f'tmp/partial_category_page_data_part-{i}.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line==\"\": continue\n",
    "                data = json.loads(line)\n",
    "                if (hash(data['category_name']) % NUM_PARTITIONS) != partition:\n",
    "                    continue\n",
    "                if data['category_name'] not in category_name_to_data:\n",
    "                    category_name_to_data[data['category_name']] = get_empty_category_data()\n",
    "                category_data = category_name_to_data[data['category_name']]\n",
    "                if 'category_id' in data:\n",
    "                    category_data['category_id'] = data['category_id']\n",
    "                category_data['sub_categories'].update(data['sub_categories'])\n",
    "                category_data['parent_categories'].update(data['parent_categories'])\n",
    "                category_data['internal_links'].extend(data['internal_links'])\n",
    "                category_data['articles'].extend(data['articles'])\n",
    "    with open(data_root_dir + f'category_pages/part-{partition}.txt', 'w') as out_f:\n",
    "        for cat_name, data in category_name_to_data.items():\n",
    "            data[\"category_name\"] = cat_name\n",
    "            data['sub_categories'] = list(data['sub_categories'])\n",
    "            data['parent_categories'] = list(data['parent_categories'])\n",
    "            out_f.write(json.dumps(data)+\"\\n\")\n",
    "    print(f\"Finished part {partition} in {(time.time() - start_time) / 60} minutes\")\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed till part 0 in 0.07508935133616129 minutes\n",
      "Processed till part 1 in 0.1482577443122864 minutes\n",
      "Processed till part 2 in 0.2128618319829305 minutes\n",
      "Processed till part 3 in 0.2742927074432373 minutes\n",
      "Processed till part 4 in 0.35173648595809937 minutes\n",
      "Processed till part 5 in 0.47813774744669596 minutes\n",
      "Processed till part 6 in 0.5374097108840943 minutes\n",
      "Processed till part 7 in 0.595884398619334 minutes\n",
      "Processed till part 8 in 0.6622572024663289 minutes\n",
      "Processed till part 9 in 0.7481602390607198 minutes\n"
     ]
    }
   ],
   "source": [
    "# check for a few category pages that they look good\n",
    "\n",
    "selected_categories = [\"Platonic solids\", \"Sampling (statistics)\", \"Drama films\", \"Comedy novels\"]\n",
    "category_pages = wiki_analysis_utils.load_category_pages(data_root_dir, selected_categories=selected_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sub_categories': ['sampling techniques',\n",
       "  'empirical evidence',\n",
       "  'sample statistics',\n",
       "  'survey methodology'],\n",
       " 'internal_links': [],\n",
       " 'articles': [['Census', 6889],\n",
       "  ['Sampling bias', 17692],\n",
       "  ['Rock paper scissors', 27032],\n",
       "  ['Statistical unit', 27580],\n",
       "  ['Stratified sampling', 27596],\n",
       "  ['Infrastructure bias', 47280],\n",
       "  ['Sampling (statistics)', 160361],\n",
       "  ['Autodidacticism', 255591],\n",
       "  ['Opinion poll', 277315],\n",
       "  ['Margin of error', 277379],\n",
       "  ['Self-selection bias', 292154],\n",
       "  ['Lottery machine', 379930],\n",
       "  ['Selection bias', 394392],\n",
       "  ['Coin flipping', 494410],\n",
       "  ['Sampling distribution', 520670],\n",
       "  ['Recall bias', 1360950],\n",
       "  ['Survivorship bias', 1745325],\n",
       "  ['Sample size determination', 1776839],\n",
       "  ['Sampling error', 1955561],\n",
       "  ['Sampling frame', 2050041],\n",
       "  ['Odds and evens (hand game)', 2234844],\n",
       "  ['Sampling fraction', 2719222],\n",
       "  [\"Whipple's index\", 4039291],\n",
       "  ['Selective recruitment', 5054888],\n",
       "  ['Scale analysis (statistics)', 6055749],\n",
       "  ['Expander walk sampling', 6245532],\n",
       "  ['Multi-attribute global inference of quality', 7257602],\n",
       "  ['Acquiescence bias', 9092040],\n",
       "  ['Imperfect induction', 9350286],\n",
       "  ['Replication (statistics)', 10306262],\n",
       "  ['Sampling risk', 10533559],\n",
       "  ['Statistical benchmarking', 12106854],\n",
       "  ['Healthy user bias', 13408012],\n",
       "  ['Sampling design', 15061620],\n",
       "  [\"Gy's sampling theory\", 15132999],\n",
       "  ['Inherent bias', 15515980],\n",
       "  ['Kleroterion', 15626359],\n",
       "  ['Item tree analysis', 16569923],\n",
       "  ['Heckman correction', 16728089],\n",
       "  ['Acceptable quality limit', 17216134],\n",
       "  ['Balanced repeated replication', 18286082],\n",
       "  ['Sortition', 19288053],\n",
       "  ['Correct sampling', 20878968],\n",
       "  ['Sampling probability', 20900922],\n",
       "  ['Horsengoggle', 21348069],\n",
       "  ['Microdata (statistics)', 21725173],\n",
       "  ['Oversampling and undersampling in data analysis', 22101888],\n",
       "  ['Civic lottery', 22163364],\n",
       "  ['Unmatched count', 24479528],\n",
       "  ['Acceptance sampling', 25719356],\n",
       "  ['Judgment sample', 26271750],\n",
       "  ['Coverage error', 26513269],\n",
       "  ['HorvitzThompson estimator', 28014764],\n",
       "  ['Lot quality assurance sampling', 31548264],\n",
       "  ['Measuring attractiveness by a categorical-based evaluation technique',\n",
       "   45481361],\n",
       "  ['Variables sampling plan', 46818101],\n",
       "  [\"Cohen's h\", 47072912],\n",
       "  ['Prior-independent mechanism', 50984744],\n",
       "  ['Flow sampling', 53547121],\n",
       "  ['Stock sampling', 54248805],\n",
       "  ['Failure bias', 57231221],\n",
       "  ['NQuery Sample Size Software', 57449714],\n",
       "  ['Sufficient similarity', 60469653],\n",
       "  ['Stratified randomization', 63588277],\n",
       "  ['Strong and weak sampling', 66207446],\n",
       "  ['Common source bias', 68025570],\n",
       "  ['Drawing lots (decision making)', 74306685],\n",
       "  ['Area sampling frame', 75539112]],\n",
       " 'parent_categories': ['statistical methods', 'data collection in research'],\n",
       " 'category_id': 6536652,\n",
       " 'category_name': 'sampling (statistics)'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_pages['selected'][1]\n",
    "# category_pages['random'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load category name to id mappings from the category pages\n",
    "categories, failure_counts = wiki_analysis_utils.load_category_name_to_id_map(data_root_dir, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed till part 0 in 0.12108764251073202 minutes\n",
      "Processed till part 1 in 0.23310102224349977 minutes\n",
      "Processed till part 2 in 0.3451609174410502 minutes\n",
      "Processed till part 3 in 0.4603490591049194 minutes\n",
      "Processed till part 4 in 0.6193542083104452 minutes\n",
      "Processed till part 5 in 0.7521270553270976 minutes\n",
      "Processed till part 6 in 0.9076026757558187 minutes\n",
      "Processed till part 7 in 1.0632949948310852 minutes\n",
      "Processed till part 8 in 1.2504449168841043 minutes\n",
      "Processed till part 9 in 1.5214941342671713 minutes\n",
      "Failure counts: {'Unknown Category': 22359, 'Unknown ParentCategory': 170572}\n",
      "Total counts: {'Category': 2369155, 'ParentCategory': 4766447}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "-> category_id_to_parent_category_ids.tsv\n",
    "\n",
    "This is a tsv file with the following structure:\n",
    "CategoryId, ParentCategoryId\n",
    "\"\"\"\n",
    "\n",
    "category_id_to_data = {}\n",
    "\n",
    "start_time = time.time()\n",
    "failure_counts = {\n",
    "    'Unknown Category': 0,\n",
    "    'Unknown ParentCategory': 0\n",
    "}\n",
    "total_counts = {\n",
    "    'Category': 0,\n",
    "    'ParentCategory': 0\n",
    "}\n",
    "log_level = 'ERROR'\n",
    "norm = wiki_analysis_utils.normalized_page_name\n",
    "with open(data_root_dir + 'category_id_to_parent_category_ids.tsv', 'w') as out_f:\n",
    "    out_f.write(\"CategoryId\\tParentCategoryId\\n\")\n",
    "    for partition in range(NUM_PARTITIONS):\n",
    "        with open(data_root_dir + f'category_pages/part-{partition}.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                if line=='': continue\n",
    "                category_json = json.loads(line)\n",
    "                total_counts['Category'] += 1\n",
    "                category_id = category_json.get('category_id', None)\n",
    "                if category_id is None:\n",
    "                    if log_level in ['WARN', 'INFO']:\n",
    "                        print(f\"Category id not present in {line}.\")\n",
    "                    failure_counts['Unknown Category'] += 1\n",
    "                    continue\n",
    "                for parent_category in set(map(norm, category_json['parent_categories'])):\n",
    "                    total_counts['ParentCategory'] += 1\n",
    "                    parent_category_id = categories['name_to_id'].get(parent_category, None)\n",
    "                    if parent_category_id is None:\n",
    "                        if log_level in ['WARN', 'INFO']:\n",
    "                            print(f\"Parent category {parent_category} not found in category name to id mapping.\")\n",
    "                        failure_counts['Unknown ParentCategory'] += 1\n",
    "                        continue\n",
    "                    out_f.write(f\"{category_id}\\t{parent_category_id}\\n\")\n",
    "        print(f\"Processed till part {partition} in {(time.time() - start_time) / 60} minutes\")\n",
    "\n",
    "print (f\"Failure counts: {failure_counts}\")\n",
    "print (f\"Total counts: {total_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500000 edges in 0.49964474042256674 minutes\n",
      "Loaded 1000000 edges in 1.0060873905817667 minutes\n",
      "Loaded 1500000 edges in 1.493864639600118 minutes\n",
      "Loaded 2000000 edges in 2.1164226730664573 minutes\n",
      "Loaded 2500000 edges in 2.6897119919459027 minutes\n",
      "Loaded 3000000 edges in 3.182141168912252 minutes\n",
      "Loaded 3500000 edges in 3.659866400559743 minutes\n",
      "Loaded 4000000 edges in 4.230882132053376 minutes\n",
      "Loaded 4500000 edges in 4.730266630649567 minutes\n"
     ]
    }
   ],
   "source": [
    "parent_graph_adj_lists, child_graph_adj_lists = wiki_analysis_utils\\\n",
    "    .load_category_graph(data_root_dir, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1790399, 866913)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parent_graph_adj_lists), len(child_graph_adj_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 556397 root categories\n",
      "There are 72888 root categories with children\n",
      "There are 483509 root categories with no children\n",
      "There are 483509 root categories with no children\n"
     ]
    }
   ],
   "source": [
    "# lots of categories are isolated, i.e. they have no parent or child categories\n",
    "# this is mainly due to parsing errors due to unexpanded templates\n",
    "root_categories = [cat for cat in categories['id_to_name'] if cat not in parent_graph_adj_lists]\n",
    "print (f\"There are {len(root_categories)} root categories\")\n",
    "\n",
    "roots_with_children = [cat for cat in root_categories if cat in child_graph_adj_lists]\n",
    "print (f\"There are {len(roots_with_children)} root categories with children\")\n",
    "\n",
    "roots_with_no_children = [cat for cat in root_categories if cat not in child_graph_adj_lists]\n",
    "print (f\"There are {len(roots_with_no_children)} root categories with no children\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample 10 random categories from the root categories of different types to see what they are\n",
    "# sample_roots_children = random.sample(roots_with_children, 10)\n",
    "# print (\"Root categories with children:\")\n",
    "# for cat in sample_roots_children:\n",
    "#     print (cat, categories['id_to_name'][cat], len(child_graph_adj_lists[cat]))\n",
    "\n",
    "# print (\"\\nRoot categories without children\")\n",
    "# sample_roots_no_children = random.sample(roots_with_no_children, 10)\n",
    "# for cat in sample_roots_no_children:\n",
    "#     print (cat, categories['id_to_name'][cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, lets first understand the category DAG a little:\n",
    "\n",
    "1. Verify that there are no cycles in the category DAG. \n",
    "    - label each edges as forward, backward, cross, or tree edges.\n",
    "    - count and investigate. Remove the cycles (if any)\n",
    "2. Do the root categorizes share a lot of descendants or it is a loose collection of small DAGs?\n",
    "    - show distribution of number of descendants for roots\n",
    "3. How deep is the DAG?\n",
    "    - show distribution of depth of nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2346789 categories discovered based on root categories. (99.99970172098469%)\n"
     ]
    }
   ],
   "source": [
    "dfs_color = {}\n",
    "dfs_discovery_time = {}\n",
    "dfs_finishing_time = {}\n",
    "dfs_depth = {}\n",
    "edge_to_type = {}\n",
    "\n",
    "def dfs_visit(category_id, dfs_time, depth):\n",
    "    dfs_color[category_id] = 'GRAY'\n",
    "    dfs_time += 1\n",
    "    dfs_discovery_time[category_id] = dfs_time\n",
    "    dfs_depth[category_id] = depth\n",
    "    for child_category_id in child_graph_adj_lists.get(category_id, []):\n",
    "        if dfs_color.get(child_category_id, 'WHITE') == 'WHITE':\n",
    "            edge_to_type[(category_id, child_category_id)] = 'TreeEdge'\n",
    "            dfs_time = dfs_visit(child_category_id, dfs_time, depth + 1)\n",
    "        elif dfs_color[child_category_id] == 'GRAY':\n",
    "            edge_to_type[(category_id, child_category_id)] = 'BackEdge'\n",
    "        elif dfs_discovery_time[category_id] < dfs_discovery_time[child_category_id]:\n",
    "            edge_to_type[(category_id, child_category_id)] = 'ForwardEdge'\n",
    "        else:\n",
    "            edge_to_type[(category_id, child_category_id)] = 'CrossEdge'\n",
    "    dfs_color[category_id] = 'BLACK'\n",
    "    dfs_time += 1\n",
    "    dfs_finishing_time[category_id] = dfs_time\n",
    "    return dfs_time\n",
    "\n",
    "dfs_time = 0\n",
    "for root_category in root_categories:\n",
    "    if dfs_color.get(root_category, 'WHITE') == 'WHITE':\n",
    "        dfs_time = dfs_visit(root_category, dfs_time, 0)\n",
    "\n",
    "print (f\"{len(dfs_color)} categories discovered based on root categories. ({len(dfs_color) / len(categories['id_to_name']) * 100}%)\")\n",
    "# there are some very few isolated cycles in the graph. So we need to run dfs on them as well\n",
    "for category_id in categories['id_to_name']:\n",
    "    if dfs_color.get(category_id, 'WHITE') == 'WHITE':\n",
    "        dfs_time = dfs_visit(category_id, dfs_time, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate number of descendants for each category\n",
    "dfs_num_descendants = {}\n",
    "\n",
    "def dfs_num_descendants_visit(cid):\n",
    "    num_descendants = 1\n",
    "    for child_cid in child_graph_adj_lists.get(cid, []):\n",
    "        if child_cid not in dfs_num_descendants and edge_to_type[(cid, child_cid)] == 'TreeEdge':\n",
    "            dfs_num_descendants_visit(child_cid)\n",
    "            num_descendants += dfs_num_descendants[child_cid]\n",
    "    dfs_num_descendants[cid] = num_descendants\n",
    "\n",
    "for cid in categories['id_to_name']:\n",
    "    if cid not in dfs_num_descendants:\n",
    "        dfs_num_descendants_visit(cid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "718360"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(dfs_num_descendants.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'CrossEdge': 2763815, 'TreeEdge': 1790392, 'ForwardEdge': 37798, 'BackEdge': 3862})\n"
     ]
    }
   ],
   "source": [
    "# analyze edge types\n",
    "edge_type_counts = Counter(edge_to_type.values())\n",
    "# wow! there are some back edges in the graph. \n",
    "# Fortunate not a lot of them. Same for forward edges.\n",
    "print (edge_type_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look at some cycles to see if they actually exist in the data\n",
    "cycles = []\n",
    "for edge, edge_type in edge_to_type.items():\n",
    "    if edge_type == 'BackEdge':\n",
    "        u, v = edge\n",
    "        # trace v -> u path by following tree edges starting from u\n",
    "        path = [u]\n",
    "        while path[-1] != v:\n",
    "            for parent in parent_graph_adj_lists[path[-1]]:\n",
    "                if edge_to_type[parent, path[-1]] == 'TreeEdge':\n",
    "                    path.append(parent)\n",
    "                    break\n",
    "        path.append(u)\n",
    "        cycles.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CycleLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3862.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>276.170637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>255.122625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>225.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>473.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>976.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CycleLength\n",
       "count  3862.000000\n",
       "mean    276.170637\n",
       "std     255.122625\n",
       "min       2.000000\n",
       "25%      14.000000\n",
       "50%     225.000000\n",
       "75%     473.000000\n",
       "max     976.000000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see distribution of cycle lengths\n",
    "# wow! median is 225! what are these crazy chains of categories?\n",
    "pd.DataFrame([len(cycle) for cycle in cycles], columns=['CycleLength']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2209407, 728172, 743165, 62779982, 2360882, 36231667, 11908750, 3199407, 743167, 31292581, 742730, 43934264, 42630814, 37654739, 36513227, 30939454, 945188, 2209407]\n",
      "['languages of kuwait', 'culture of kuwait', 'kuwait', 'eastern arabia', 'persian gulf', 'bodies of water of bahrain', 'landforms of bahrain', 'geography of bahrain', 'bahrain', 'member states of the arab league', 'arab league', 'pan-arabist organizations', 'arab nationalist organizations', 'arab organizations', 'arab culture', 'arabic-language culture', 'arabic language', 'languages of kuwait']\n"
     ]
    }
   ],
   "source": [
    "cycle_idx = 345\n",
    "cycle_cat_names = [categories['id_to_name'][cat] for cat in cycles[cycle_idx]]\n",
    "print (cycles[cycle_idx])\n",
    "print (cycle_cat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cycle examples in catagory graph:\n",
    "\n",
    "[36248715, 62373302, 36248715]\n",
    "['kootenay ice players', 'winnipeg ice players', 'kootenay ice players']\n",
    "\n",
    "[4216522, 4285903, 38997059, 4216009, 4299011, 4216522]\n",
    "['colorado desert', 'deserts of the lower colorado river valley', 'natural history of the lower colorado river valley', 'lower colorado river valley', 'colorado river', 'colorado desert']\n",
    "\n",
    "[2209407, 728172, 743165, 62779982, 2360882, 36231667, 11908750, 3199407, 743167, 31292581, 742730, 43934264, 42630814, 37654739, 36513227, 30939454, 945188, 2209407]\n",
    "['languages of kuwait', 'culture of kuwait', 'kuwait', 'eastern arabia', 'persian gulf', 'bodies of water of bahrain', 'landforms of bahrain', 'geography of bahrain', 'bahrain', 'member states of the arab league', 'arab league', 'pan-arabist organizations', 'arab nationalist organizations', 'arab organizations', 'arab culture', 'arabic-language culture', 'arabic language', 'languages of kuwait']\n",
    "\"\"\"\n",
    "_=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look at some forward edges\n",
    "forward_edge_dup_paths = []\n",
    "for edge, edge_type in edge_to_type.items():\n",
    "    if edge_type == 'ForwardEdge':\n",
    "        u, v = edge\n",
    "        # trace u -> v path by following tree edges starting from u\n",
    "        path = [v]\n",
    "        while path[-1] != u:\n",
    "            for parent in parent_graph_adj_lists[path[-1]]:\n",
    "                if edge_to_type[parent, path[-1]] == 'TreeEdge':\n",
    "                    path.append(parent)\n",
    "                    break\n",
    "        forward_edge_dup_paths.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PathLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>37798.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.992830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>120.625247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>979.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PathLength\n",
       "count  37798.000000\n",
       "mean      38.992830\n",
       "std      120.625247\n",
       "min        3.000000\n",
       "25%        3.000000\n",
       "50%        3.000000\n",
       "75%        4.000000\n",
       "max      979.000000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see distribution of forward edge tree path lengths\n",
    "# forward edges don't hop too much. Median is 3. These look much more benign.\n",
    "pd.DataFrame([len(path) for path in forward_edge_dup_paths], columns=['PathLength']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68451063, 75485509, 3021025]\n",
      "['residential schools in saskatchewan', 'former schools in saskatchewan', 'schools in saskatchewan']\n"
     ]
    }
   ],
   "source": [
    "path_idx = 0\n",
    "path_cat_names = [categories['id_to_name'][cat] for cat in forward_edge_dup_paths[path_idx]]\n",
    "print (forward_edge_dup_paths[path_idx])\n",
    "print (path_cat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NumDescendants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.346796e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.414041e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.997034e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.183600e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       NumDescendants\n",
       "count    2.346796e+06\n",
       "mean     1.414041e+02\n",
       "std      7.997034e+03\n",
       "min      1.000000e+00\n",
       "25%      1.000000e+00\n",
       "50%      1.000000e+00\n",
       "75%      1.000000e+00\n",
       "max      7.183600e+05"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see distribution of number of descendants\n",
    "# as expected, most categories have no descendants and some have a lot\n",
    "pd.DataFrame(list(dfs_num_descendants.values()), columns=['NumDescendants']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_descendants = []\n",
    "for cid, num_descendants in dfs_num_descendants.items():\n",
    "    if num_descendants > 1:\n",
    "        cats_descendants.append({\n",
    "            'CategoryName': categories['id_to_name'][cid],\n",
    "            'NumDescendants': num_descendants\n",
    "        })\n",
    "cats_descendants = pd.DataFrame(cats_descendants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CategoryName</th>\n",
       "      <th>NumDescendants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41070</th>\n",
       "      <td>federal government of the united states</td>\n",
       "      <td>26332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138044</th>\n",
       "      <td>problem solving</td>\n",
       "      <td>25747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108099</th>\n",
       "      <td>liberalism by country</td>\n",
       "      <td>25606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108095</th>\n",
       "      <td>republicanism by country</td>\n",
       "      <td>25580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108093</th>\n",
       "      <td>republics</td>\n",
       "      <td>25564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324052</th>\n",
       "      <td>postcold war era</td>\n",
       "      <td>25510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30067</th>\n",
       "      <td>islands of the pacific ocean</td>\n",
       "      <td>25366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137889</th>\n",
       "      <td>decision-making</td>\n",
       "      <td>25285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137877</th>\n",
       "      <td>policy</td>\n",
       "      <td>25261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137874</th>\n",
       "      <td>social policy</td>\n",
       "      <td>25246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   CategoryName  NumDescendants\n",
       "41070   federal government of the united states           26332\n",
       "138044                          problem solving           25747\n",
       "108099                    liberalism by country           25606\n",
       "108095                 republicanism by country           25580\n",
       "108093                                republics           25564\n",
       "324052                        postcold war era           25510\n",
       "30067              islands of the pacific ocean           25366\n",
       "137889                          decision-making           25285\n",
       "137877                                   policy           25261\n",
       "137874                            social policy           25246"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_descendants.sort_values('NumDescendants', ascending=False).head(1000).tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2346796.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>339.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>275.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>364.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>532.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1001.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Depth\n",
       "count 2346796.00\n",
       "mean      339.28\n",
       "std       275.89\n",
       "min         0.00\n",
       "25%         1.00\n",
       "50%       364.00\n",
       "75%       532.00\n",
       "max      1001.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show distribution of depth\n",
    "# display without scientific notation\n",
    "with pd.option_context('float_format', '{:.2f}'.format):\n",
    "    display(pd.DataFrame(list(dfs_depth.values()), columns=['Depth']).describe())\n",
    "\n",
    "# category depth is very high. Median is 364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all dfs related data\n",
    "\n",
    "# category_id_to_dfs_metadata.tsv: CategoryId, DiscoveryTime, FinishingTime, Depth, NumDescendants\n",
    "with open(data_root_dir + 'category_id_to_dfs_metadata.tsv', 'w') as f:\n",
    "    f.write(\"CategoryId\\tDiscoveryTime\\tFinishingTime\\tDepth\\tNumDescendants\\n\")\n",
    "    for category_id in dfs_color:\n",
    "        f.write(f\"{category_id}\\t{dfs_discovery_time[category_id]}\\t{dfs_finishing_time[category_id]}\\t{dfs_depth[category_id]}\\t{dfs_num_descendants[category_id]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_edges_dfs_classification.tsv: CategoryId, SubCategoryId, EdgeType\n",
    "with open(data_root_dir + 'category_edges_dfs_classification.tsv', 'w') as f:\n",
    "    f.write(\"CategoryId\\tSubCategoryId\\tEdgeType\\n\")\n",
    "    for (category_id, sub_category_id), edge_type in edge_to_type.items():\n",
    "        f.write(f\"{category_id}\\t{sub_category_id}\\t{edge_type}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed till part 0 in 0.14392542441685993 minutes\n",
      "Processed till part 1 in 0.25730487902959187 minutes\n",
      "Processed till part 2 in 0.3715030550956726 minutes\n",
      "Processed till part 3 in 0.49416927099227903 minutes\n",
      "Processed till part 4 in 0.6536876241366069 minutes\n",
      "Processed till part 5 in 0.7914778232574463 minutes\n",
      "Processed till part 6 in 0.9500364224116008 minutes\n",
      "Processed till part 7 in 1.083427627881368 minutes\n",
      "Processed till part 8 in 1.232020664215088 minutes\n",
      "Processed till part 9 in 1.3414887547492982 minutes\n",
      "Failure counts: {'Unknown Category': 22359, 'Unknown SubCategory': 0, 'Unknown ParentCategory': 170572}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Obtain and save these mappings:\n",
    "\n",
    "category_id_to_page_ids.tsv: CategoryId, PageId\n",
    "category_id_to_stats.tsv: CategoryId, NumPages, NumChildCategories, NumParentCategories, NumDescendantCategories\n",
    "\"\"\"\n",
    "\n",
    "category_id_to_data = {}\n",
    "\n",
    "start_time = time.time()\n",
    "failure_counts = {\n",
    "    'Unknown Category': 0,\n",
    "    'Unknown SubCategory': 0,\n",
    "    'Unknown ParentCategory': 0\n",
    "}\n",
    "log_level = 'ERROR'\n",
    "for partition in range(NUM_PARTITIONS):\n",
    "    with open(data_root_dir + f'category_pages/part-{partition}.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            if line=='': continue\n",
    "            category_json = json.loads(line)\n",
    "            category_data = {\n",
    "                'page_ids': [],\n",
    "                'sub_category_ids': [],\n",
    "                'parent_category_ids': []\n",
    "            }\n",
    "            category_id = categories['name_to_id'].get(category_json['category_name'], None)\n",
    "            if category_id is None:\n",
    "                if log_level in ['WARN', 'INFO']:\n",
    "                    print(f\"Category {category_json['category_name']} not found in category name to id mapping.\")\n",
    "                failure_counts['Unknown Category'] += 1\n",
    "                continue\n",
    "            category_id_to_data[category_id] = category_data\n",
    "            for _, page_id in category_json['articles']:\n",
    "                category_data['page_ids'].append(page_id)\n",
    "            for sub_category in category_json['sub_categories']:\n",
    "                sub_category_id = categories['name_to_id'].get(sub_category, None)\n",
    "                if sub_category_id is None:\n",
    "                    if log_level in ['WARN', 'INFO']:\n",
    "                        print(f\"Sub category {sub_category} not found in category name to id mapping.\")\n",
    "                    failure_counts['Unknown SubCategory'] += 1\n",
    "                    continue\n",
    "                category_data['sub_category_ids'].append(sub_category_id)\n",
    "            for parent_category in category_json['parent_categories']:\n",
    "                parent_category_id = categories['name_to_id'].get(parent_category, None)\n",
    "                if parent_category_id is None:\n",
    "                    if log_level in ['WARN', 'INFO']:\n",
    "                        print(f\"Parent category {parent_category} not found in category name to id mapping.\")\n",
    "                    failure_counts['Unknown ParentCategory'] += 1\n",
    "                    continue\n",
    "                category_data['parent_category_ids'].append(parent_category_id)\n",
    "    print(f\"Processed till part {partition} in {(time.time() - start_time) / 60} minutes\")\n",
    "\n",
    "print (f\"Failure counts: {failure_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save these mappings to tsv files\n",
    "\n",
    "with open(data_root_dir + 'category_id_to_stats.tsv', 'w') as f:\n",
    "    f.write(\"CategoryId\\tNumPages\\tNumChildCategories\\tNumParentCategories\\tNumDescendantCategories\\n\")\n",
    "    for category_id, data in category_id_to_data.items():\n",
    "        num_descendant_categories = dfs_num_descendants[category_id]\n",
    "        f.write(f\"{category_id}\\t{len(data['page_ids'])}\\t{len(data['sub_category_ids'])}\\t{len(data['parent_category_ids'])}\\t{num_descendant_categories}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_root_dir + 'category_id_to_page_ids.tsv', 'w') as f:\n",
    "    f.write(\"CategoryId\\tPageId\\n\")\n",
    "    for category_id, data in category_id_to_data.items():\n",
    "        for page_id in data['page_ids']:\n",
    "            f.write(f\"{category_id}\\t{page_id}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
