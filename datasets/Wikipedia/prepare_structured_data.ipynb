{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook prepares structured data to fed for further analysis.\n",
    "It creates these datasets:\n",
    "\n",
    "Page based:\n",
    "\n",
    "page_info.tsv: \n",
    "PageName, PageId, TextLength, NumUniqueWords, NumFiles, \n",
    "NumExternalLinks, NumInfoBoxes, NumSections\n",
    "\n",
    "page_links.tsv:\n",
    "SourcePageId, DestinationPageId\n",
    "\n",
    "page_inlink_counts.tsv:\n",
    "PageId, NumInlinks\n",
    "\n",
    "Category based:\n",
    "\n",
    "-> Category pages\n",
    "\n",
    "These are jsons (one per line) with the following structure:\n",
    "{\n",
    "    \"category_name\": \"Category name\",\n",
    "    \"sub_categories\": [\"Subcategory 1\", \"Subcategory 2\", ...],\n",
    "    \"articles\": [(\"Article Title 1\", page_id_1), (\"Article Title 2\", page_id_2), ...],\n",
    "    \"parent_categories\": [\"Parent Category 1\", \"Parent Category 2\", ...],\n",
    "    \"internal_links\": [\"link 1\", \"link 2\", ...]\n",
    "}\n",
    "\n",
    "-> category_id_to_sub_category_ids.tsv\n",
    "\n",
    "This is a tsv file with the following structure:\n",
    "CategoryId, DescendantCategoryId, ConnectingChain\n",
    "\n",
    "Connecting chain is a comma separated list of category ids that connect the two categories.\n",
    "\n",
    "-> category_id_to_parent_category_ids.tsv\n",
    "\n",
    "This is a tsv file with the following structure:\n",
    "CategoryId, ParentCategoryId\n",
    "\n",
    "-> category_id_to_page_ids.tsv\n",
    "\n",
    "This is a tsv file with the following structure:\n",
    "CategoryId, PageId\n",
    "\n",
    "-> category_id_to_stats.tsv\n",
    "\n",
    "This is a tsv file with the following structure:\n",
    "CategoryId, NumArticles, NumChildcategories, NumParentCategories\n",
    "\n",
    "\"\"\"\n",
    "_ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time, json, os, random, traceback, pyperclip, importlib\n",
    "import src.wiki_analysis_utils as wiki_analysis_utils\n",
    "\n",
    "_ = importlib.reload(wiki_analysis_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_dir = r'D:\\WikipediaDataset\\data\\\\'\n",
    "NUM_PARTITIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 lines. 962044 pages found till now.\n",
      "Processed 2000000 lines. 1916891 pages found till now.\n",
      "Finished part 0 in 0.7270979086558024 minutes\n",
      "Processed 3000000 lines. 2859405 pages found till now.\n",
      "Processed 4000000 lines. 3790782 pages found till now.\n",
      "Finished part 1 in 1.297219415505727 minutes\n",
      "Processed 5000000 lines. 4730403 pages found till now.\n",
      "Processed 6000000 lines. 5650925 pages found till now.\n",
      "Finished part 2 in 1.7902512073516845 minutes\n",
      "Processed 7000000 lines. 6570415 pages found till now.\n",
      "Processed 8000000 lines. 7474023 pages found till now.\n",
      "Finished part 3 in 2.322950883706411 minutes\n",
      "Processed 9000000 lines. 8350942 pages found till now.\n",
      "Processed 10000000 lines. 9212477 pages found till now.\n",
      "Finished part 4 in 2.8843417684237163 minutes\n",
      "Processed 11000000 lines. 10047260 pages found till now.\n",
      "Processed 12000000 lines. 10918180 pages found till now.\n",
      "Finished part 5 in 3.4488520979881288 minutes\n",
      "Processed 13000000 lines. 11752593 pages found till now.\n",
      "Processed 14000000 lines. 12594046 pages found till now.\n",
      "Finished part 6 in 4.024058508872986 minutes\n",
      "Processed 15000000 lines. 13419946 pages found till now.\n",
      "Processed 16000000 lines. 14270535 pages found till now.\n",
      "Finished part 7 in 4.511818341414133 minutes\n",
      "Processed 17000000 lines. 15112581 pages found till now.\n",
      "Processed 18000000 lines. 15948874 pages found till now.\n",
      "Finished part 8 in 4.986333763599395 minutes\n",
      "Processed 19000000 lines. 16823808 pages found till now.\n",
      "Processed 20000000 lines. 17667034 pages found till now.\n",
      "Finished part 9 in 5.431774465243022 minutes\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save page_info.tsv: \n",
    "PageName, RedirectTitle, PageId, TextLength, NumInternalLinks, NumUniqueWords, NumFiles,\n",
    "NumExternalLinks, NumInfoBoxes, NumSections\n",
    "\"\"\"\n",
    "\n",
    "start_time = time.time()\n",
    "processed_line_count = 0\n",
    "num_pages_processed = 0\n",
    "with open(data_root_dir + 'page_info.tsv', 'w') as page_info_f:\n",
    "    page_info_f.write(\"PageName\\tRedirectTitle\\tPageId\\tTextLength\\tNumInternalLinks\\tNumUniqueWords\\tNumFiles\\tNumExternalLinks\\tNumInfoBoxes\\tNumSections\\n\")\n",
    "    for i in range(10):\n",
    "        with open(data_root_dir + f'processed_summaries/part-{i}.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                page = json.loads(line)\n",
    "                namespace = page['namespace']\n",
    "                title = page['title']\n",
    "                page_id = page['page_id']\n",
    "                redirect_title = page['redirect_title']\n",
    "\n",
    "                if namespace == 0:\n",
    "                    if redirect_title:\n",
    "                        page_info_f.write(f\"{title}\\t{redirect_title}\\t{page_id}\\t\\t\\t\\t\\t\\t\\t\\n\")\n",
    "                    else:\n",
    "                        page_info_f.write(f\"{title}\\t\\t{page_id}\\t{page['text_length']}\\t{len(page['internal_links'])}\\t{page['num_unique_words']}\\t{page['number_of_files']}\\t{page['number_of_external_links']}\\t{page['number_of_info_boxes']}\\t{page['number_of_sections']}\\n\")\n",
    "                    num_pages_processed += 1\n",
    "                processed_line_count += 1\n",
    "                if processed_line_count % 1000000 == 0:\n",
    "                    print(f\"Processed {processed_line_count} lines. {num_pages_processed} pages found till now.\")\n",
    "\n",
    "        print(f\"Finished part {i} in {(time.time() - start_time) / 60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 lines. Fail count: 0. Success count: 1000000. Time taken: 0.03520254294077555 minutes.\n",
      "Processed 2000000 lines. Fail count: 0. Success count: 2000000. Time taken: 0.0746461828549703 minutes.\n",
      "Processed 3000000 lines. Fail count: 0. Success count: 3000000. Time taken: 0.11801429986953735 minutes.\n",
      "Processed 4000000 lines. Fail count: 0. Success count: 4000000. Time taken: 0.17254188060760497 minutes.\n",
      "Processed 5000000 lines. Fail count: 0. Success count: 5000000. Time taken: 0.224252720673879 minutes.\n",
      "Processed 6000000 lines. Fail count: 0. Success count: 6000000. Time taken: 0.27663596868515017 minutes.\n",
      "Processed 7000000 lines. Fail count: 0. Success count: 7000000. Time taken: 0.3174879471460978 minutes.\n",
      "Processed 8000000 lines. Fail count: 0. Success count: 8000000. Time taken: 0.36652742624282836 minutes.\n",
      "Processed 9000000 lines. Fail count: 0. Success count: 9000000. Time taken: 0.4145349780718485 minutes.\n",
      "Processed 10000000 lines. Fail count: 0. Success count: 10000000. Time taken: 0.467132031917572 minutes.\n",
      "Processed 11000000 lines. Fail count: 0. Success count: 11000000. Time taken: 0.5194503982861837 minutes.\n",
      "Processed 12000000 lines. Fail count: 0. Success count: 12000000. Time taken: 0.5855878750483196 minutes.\n",
      "Processed 13000000 lines. Fail count: 0. Success count: 13000000. Time taken: 0.6316748340924581 minutes.\n",
      "Processed 14000000 lines. Fail count: 0. Success count: 14000000. Time taken: 0.682768185933431 minutes.\n",
      "Processed 15000000 lines. Fail count: 0. Success count: 15000000. Time taken: 0.7293232719103495 minutes.\n",
      "Processed 16000000 lines. Fail count: 0. Success count: 16000000. Time taken: 0.7772458314895629 minutes.\n",
      "Processed 17000000 lines. Fail count: 0. Success count: 17000000. Time taken: 0.8252223014831543 minutes.\n",
      "Failed to parse 0 / 17753527 (0.0%) lines.\n"
     ]
    }
   ],
   "source": [
    "page_name_to_page_id, page_id_to_page_name, error_counts = \\\n",
    "    wiki_analysis_utils.load_page_name_to_id_map(data_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16812294, 17753527, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(page_name_to_page_id), len(page_id_to_page_name), len(error_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 lines. Fail count: 70. Success count: 999930. Time taken: 0.03364824851353963 minutes.\n",
      "Processed 2000000 lines. Fail count: 204. Success count: 1999796. Time taken: 0.06828335920969646 minutes.\n",
      "Processed 3000000 lines. Fail count: 391. Success count: 2999609. Time taken: 0.10596334934234619 minutes.\n",
      "Processed 4000000 lines. Fail count: 587. Success count: 3999413. Time taken: 0.1513728141784668 minutes.\n",
      "Processed 5000000 lines. Fail count: 770. Success count: 4999230. Time taken: 0.19528584082921346 minutes.\n",
      "Processed 6000000 lines. Fail count: 956. Success count: 5999044. Time taken: 0.23620920578638713 minutes.\n",
      "Processed 7000000 lines. Fail count: 1182. Success count: 6998818. Time taken: 0.2848101774851481 minutes.\n",
      "Processed 8000000 lines. Fail count: 1470. Success count: 7998530. Time taken: 0.33345146973927814 minutes.\n",
      "Processed 9000000 lines. Fail count: 1709. Success count: 8998291. Time taken: 0.3767818530400594 minutes.\n",
      "Processed 10000000 lines. Fail count: 1973. Success count: 9998027. Time taken: 0.42707310914993285 minutes.\n",
      "Processed 11000000 lines. Fail count: 2603. Success count: 10997397. Time taken: 0.46978972752889 minutes.\n",
      "Processed 12000000 lines. Fail count: 2918. Success count: 11997082. Time taken: 0.5130100170771281 minutes.\n",
      "Processed 13000000 lines. Fail count: 3418. Success count: 12996582. Time taken: 0.5690348029136658 minutes.\n",
      "Processed 14000000 lines. Fail count: 27284. Success count: 13972716. Time taken: 0.6685067017873129 minutes.\n",
      "Processed 15000000 lines. Fail count: 28227. Success count: 14971773. Time taken: 0.7207252621650696 minutes.\n",
      "Processed 16000000 lines. Fail count: 28801. Success count: 15971199. Time taken: 0.7705979307492574 minutes.\n",
      "Processed 17000000 lines. Fail count: 29377. Success count: 16970623. Time taken: 0.829608698685964 minutes.\n"
     ]
    }
   ],
   "source": [
    "# load source_page_id to destination_page_id for redirect pages\n",
    "# this will be used to resolve redirects in the cells below\n",
    "\n",
    "redirect_pid_to_pid, error_counts = \\\n",
    "    wiki_analysis_utils.load_page_redirect_mapping(data_root_dir, page_name_to_page_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10904676, 3690970, 29764)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(redirect_pid_to_pid), len(set(redirect_pid_to_pid.values())), len(error_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 lines. 520258 pages found till now.\n",
      "Processed 2000000 lines. 967758 pages found till now.\n",
      "Finished part 0 in 7.403570981820424 minutes\n",
      "Processed 3000000 lines. 1375964 pages found till now.\n",
      "Processed 4000000 lines. 1777375 pages found till now.\n",
      "Finished part 1 in 10.953632120291392 minutes\n",
      "Processed 5000000 lines. 2103953 pages found till now.\n",
      "Processed 6000000 lines. 2488972 pages found till now.\n",
      "Finished part 2 in 13.118326369921366 minutes\n",
      "Processed 7000000 lines. 2860437 pages found till now.\n",
      "Processed 8000000 lines. 3191915 pages found till now.\n",
      "Finished part 3 in 15.521819802125295 minutes\n",
      "Processed 9000000 lines. 3503469 pages found till now.\n",
      "Processed 10000000 lines. 3842350 pages found till now.\n",
      "Finished part 4 in 17.97135885953903 minutes\n",
      "Processed 11000000 lines. 4156171 pages found till now.\n",
      "Processed 12000000 lines. 4498292 pages found till now.\n",
      "Finished part 5 in 19.871130939324697 minutes\n",
      "Processed 13000000 lines. 4834557 pages found till now.\n",
      "Processed 14000000 lines. 5113268 pages found till now.\n",
      "Finished part 6 in 21.65992443561554 minutes\n",
      "Processed 15000000 lines. 5398390 pages found till now.\n",
      "Processed 16000000 lines. 5697508 pages found till now.\n",
      "Finished part 7 in 23.301409379641214 minutes\n",
      "Processed 17000000 lines. 6000460 pages found till now.\n",
      "Processed 18000000 lines. 6302727 pages found till now.\n",
      "Finished part 8 in 25.027618098258973 minutes\n",
      "Processed 19000000 lines. 6544333 pages found till now.\n",
      "Processed 20000000 lines. 6794797 pages found till now.\n",
      "Finished part 9 in 26.439934555689494 minutes\n",
      "Failure counts: {'Unknown Page': 11618884}\n",
      "11618884 / 229492654 (5.062856608909146%) links couldn't be captured.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save page_links.tsv:\n",
    "SourcePageId, DestinationPageId\n",
    "\"\"\"\n",
    "\n",
    "start_time = time.time()\n",
    "processed_line_count = 0\n",
    "num_pages_processed = 0\n",
    "failure_counts = {\n",
    "    'Unknown Page': 0\n",
    "}\n",
    "total_link_count = 0\n",
    "log_level = 'ERROR'\n",
    "with open(data_root_dir + 'page_links.tsv', 'w') as page_links_f:\n",
    "    page_links_f.write(\"SourcePageId\\tDestinationPageId\\n\")\n",
    "    for i in range(10):\n",
    "        with open(data_root_dir + f'processed_summaries/part-{i}.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                page = json.loads(line)\n",
    "                namespace = page['namespace']\n",
    "                title = page['title']\n",
    "                page_id = page['page_id']\n",
    "                redirect_title = page['redirect_title']\n",
    "\n",
    "                if namespace == 0 and not redirect_title:\n",
    "                    for link in page['internal_links']:\n",
    "                        link = wiki_analysis_utils.normalized_page_name(link)\n",
    "                        if link.startswith(':category:'):\n",
    "                            continue\n",
    "                        total_link_count += 1\n",
    "                        if link not in page_name_to_page_id:\n",
    "                            if log_level in ['WARN', 'INFO']:\n",
    "                                print(f\"Page {link} in page {page_id} not found in page name to id mapping.\")\n",
    "                            failure_counts['Unknown Page'] += 1\n",
    "                            continue\n",
    "                        dest_page_id = page_name_to_page_id[link]\n",
    "                        dest_page_id = redirect_pid_to_pid.get(dest_page_id, dest_page_id)\n",
    "                        page_links_f.write(f\"{page_id}\\t{dest_page_id}\\n\")\n",
    "                    num_pages_processed += 1\n",
    "                processed_line_count += 1\n",
    "                if processed_line_count % 1000000 == 0:\n",
    "                    print(f\"Processed {processed_line_count} lines. {num_pages_processed} pages found till now.\")\n",
    "                    #break\n",
    "\n",
    "        print(f\"Finished part {i} in {(time.time() - start_time) / 60} minutes\")\n",
    "        #break\n",
    "\n",
    "print (f\"Failure counts: {failure_counts}\")\n",
    "print (f\"{sum(failure_counts.values())} / {total_link_count} ({(sum(failure_counts.values()) / total_link_count) * 100}%) links couldn't be captured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303    826\n",
       "316    802\n",
       "307    549\n",
       "308    514\n",
       "305    428\n",
       "12     387\n",
       "290    242\n",
       "39     146\n",
       "309    106\n",
       "Name: SourcePageId, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = pd.read_csv(data_root_dir + 'page_links.tsv', sep='\\t', nrows=4000)\n",
    "pdf['SourcePageName'] = pdf['SourcePageId'].map(page_id_to_page_name)\n",
    "pdf['DestinationPageName'] = pdf['DestinationPageId'].map(page_id_to_page_name)\n",
    "pdf['SourcePageId'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a (cyrillic)',\n",
       " 'a (cyrillic)',\n",
       " 'a (indic)',\n",
       " 'a with breve (cyrillic)',\n",
       " 'a-list',\n",
       " 'aardvark',\n",
       " 'abo blood group system',\n",
       " 'afrikaans',\n",
       " 'aleph',\n",
       " 'aleph',\n",
       " 'aleph',\n",
       " 'aleph number',\n",
       " 'algebra',\n",
       " 'allograph',\n",
       " 'alpha',\n",
       " 'alpha',\n",
       " 'alphabet',\n",
       " 'anarchist symbolism',\n",
       " 'ancient greece',\n",
       " 'angstrom',\n",
       " 'ansuz (rune)',\n",
       " 'apple',\n",
       " 'argentine austral',\n",
       " 'argentine austral',\n",
       " 'armenian alphabet',\n",
       " 'article (grammar)',\n",
       " 'ascii',\n",
       " 'asymmetry',\n",
       " 'at sign',\n",
       " 'at sign',\n",
       " 'australian english',\n",
       " 'ayb (armenian letter)',\n",
       " 'ayb (armenian letter)',\n",
       " 'azerbaijani language',\n",
       " 'bar (diacritic)',\n",
       " 'bashkir language',\n",
       " 'bengali alphabet',\n",
       " 'blackletter',\n",
       " 'bra',\n",
       " 'bulgarian language',\n",
       " 'carolingian minuscule',\n",
       " 'caron',\n",
       " 'catalan dialects',\n",
       " 'catalan dialects',\n",
       " 'catalan language',\n",
       " 'chemnitz dialect',\n",
       " 'chuvash language',\n",
       " 'close-mid front unrounded vowel',\n",
       " 'code point',\n",
       " 'combining character',\n",
       " 'consonant',\n",
       " 'coptic script',\n",
       " 'cursive',\n",
       " 'cyrillic script',\n",
       " 'czech language',\n",
       " 'danish language',\n",
       " 'decimal',\n",
       " 'diacritic',\n",
       " 'diphthong',\n",
       " 'dot (diacritic)',\n",
       " 'double grave accent',\n",
       " 'dutch dialects',\n",
       " 'dutch language',\n",
       " 'eau (trigraph)',\n",
       " 'egyptian hieroglyphs',\n",
       " 'emilian dialects',\n",
       " 'enclosed alphanumerics',\n",
       " 'enclosed alphanumerics',\n",
       " 'enclosed alphanumerics',\n",
       " 'english alphabet',\n",
       " 'english alphabet',\n",
       " 'english articles',\n",
       " 'english language',\n",
       " 'english language in northern england',\n",
       " 'english language in southern england',\n",
       " 'english orthography',\n",
       " 'english-language vowel changes before historic /r/',\n",
       " 'etruscan alphabet',\n",
       " 'etruscan civilization',\n",
       " 'finnish language',\n",
       " 'first-order logic',\n",
       " 'french language',\n",
       " 'french orthography',\n",
       " 'galician language',\n",
       " 'general american english',\n",
       " 'geometry',\n",
       " 'german language',\n",
       " 'german orthography',\n",
       " 'geʽez script',\n",
       " 'glottal stop',\n",
       " 'glyph',\n",
       " 'gothic alphabet',\n",
       " 'gothic alphabet',\n",
       " 'great vowel shift',\n",
       " 'greek alphabet',\n",
       " 'greek dark ages',\n",
       " 'gujarati script',\n",
       " 'halfwidth and fullwidth forms',\n",
       " 'hamza',\n",
       " 'handwriting',\n",
       " 'hexadecimal',\n",
       " 'homoglyph',\n",
       " 'hungarian language',\n",
       " 'indo-european studies',\n",
       " 'indonesian language',\n",
       " 'insular script',\n",
       " 'international phonetic alphabet',\n",
       " 'inverted breve',\n",
       " 'iso/iec 8859',\n",
       " 'italian language',\n",
       " 'italian peninsula',\n",
       " 'italic type',\n",
       " 'italic type',\n",
       " 'kaingang language',\n",
       " 'kazakh language',\n",
       " 'kedah malay',\n",
       " 'kurdish language',\n",
       " 'latin',\n",
       " 'latin',\n",
       " 'latin alpha',\n",
       " 'latin alpha',\n",
       " 'latin alphabet',\n",
       " 'latin script',\n",
       " 'letter (alphabet)',\n",
       " 'letter case',\n",
       " 'letter case',\n",
       " 'letter case',\n",
       " 'ligature (writing)',\n",
       " 'limburgish',\n",
       " 'line (geometry)',\n",
       " 'line (geometry)',\n",
       " 'line segment',\n",
       " 'list of latin-script digraphs',\n",
       " 'list of latin-script digraphs',\n",
       " 'list of latin-script digraphs',\n",
       " 'list of latin-script digraphs',\n",
       " 'lithuanian language',\n",
       " 'luxembourgish',\n",
       " 'maastrichtian dialect',\n",
       " 'malay language',\n",
       " 'mapuche language',\n",
       " 'mathematical alphanumeric symbols',\n",
       " 'merovingian script',\n",
       " 'mid central vowel',\n",
       " 'middle english phonology',\n",
       " 'motivation',\n",
       " 'near-open central vowel',\n",
       " 'near-open front unrounded vowel',\n",
       " 'new zealand english',\n",
       " 'norwegian language',\n",
       " 'obsolete and nonstandard symbols in the international phonetic alphabet',\n",
       " 'old italic scripts',\n",
       " 'old italic scripts',\n",
       " 'open back rounded vowel',\n",
       " 'open back unrounded vowel',\n",
       " 'open central unrounded vowel',\n",
       " 'open front unrounded vowel',\n",
       " 'open-mid back rounded vowel',\n",
       " 'open-mid back unrounded vowel',\n",
       " 'ordinal indicator',\n",
       " 'ordinal indicator',\n",
       " 'perak malay',\n",
       " 'phoenician alphabet',\n",
       " 'phonetic transcription',\n",
       " 'pictogram',\n",
       " 'pinyin',\n",
       " 'polish language',\n",
       " 'portuguese language',\n",
       " 'portuguese orthography',\n",
       " 'precomposed character',\n",
       " 'proto-sinaitic script',\n",
       " 'r-colored vowel',\n",
       " 'received pronunciation',\n",
       " 'rené descartes',\n",
       " 'ring (diacritic)',\n",
       " 'ring (diacritic)',\n",
       " 'roman empire',\n",
       " 'runes',\n",
       " 'russian language',\n",
       " 'saanich dialect',\n",
       " 'saanich dialect',\n",
       " 'samuel johnson',\n",
       " 'schwa (cyrillic)',\n",
       " 'serif',\n",
       " 'spanish language',\n",
       " 'spanish orthography',\n",
       " 'standard chinese',\n",
       " 'stavangersk',\n",
       " 'swedish language',\n",
       " 'syllable',\n",
       " 'tagalog language',\n",
       " 'tau (disambiguation)',\n",
       " 'terengganu malay',\n",
       " 'teuthonista',\n",
       " 'the times of israel',\n",
       " 'transylvanian varieties of romanian',\n",
       " 'triangle',\n",
       " 'turkish alphabet',\n",
       " 'turned a',\n",
       " 'turned a',\n",
       " 'turned v',\n",
       " 'turned v',\n",
       " 'ugaritic',\n",
       " 'ugaritic alphabet',\n",
       " 'ukrainian language',\n",
       " 'ulster irish',\n",
       " 'uncial script',\n",
       " 'uncial script',\n",
       " 'unicode',\n",
       " 'unicode subscripts and superscripts',\n",
       " 'universal quantification',\n",
       " 'uralic phonetic alphabet',\n",
       " 'variable (mathematics)',\n",
       " 'visigothic script',\n",
       " 'vowel',\n",
       " 'west frisian language',\n",
       " 'x-sampa',\n",
       " 'ya (cyrillic)',\n",
       " 'zeta–raška dialect',\n",
       " 'à',\n",
       " 'á',\n",
       " 'â',\n",
       " 'â',\n",
       " 'â',\n",
       " 'â',\n",
       " 'â',\n",
       " 'â',\n",
       " 'ã',\n",
       " 'ä',\n",
       " 'ä',\n",
       " 'å',\n",
       " 'æ',\n",
       " 'ā',\n",
       " 'ă',\n",
       " 'ă',\n",
       " 'ă',\n",
       " 'ă',\n",
       " 'ă',\n",
       " 'ă',\n",
       " 'ą',\n",
       " 'ȧ',\n",
       " 'ả']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internal_links = pdf.query('SourcePageId==290')['DestinationPageName'].to_list()\n",
    "print (len(internal_links))\n",
    "pyperclip.copy(json.dumps(sorted(internal_links), indent=2))\n",
    "sorted(internal_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000000 lines. Time taken: 0.2552937984466553 minutes.\n",
      "Processed 20000000 lines. Time taken: 0.4910214861234029 minutes.\n",
      "Processed 30000000 lines. Time taken: 0.7426513274510701 minutes.\n",
      "Processed 40000000 lines. Time taken: 1.0243276198705038 minutes.\n",
      "Processed 50000000 lines. Time taken: 1.3267032305399578 minutes.\n",
      "Processed 60000000 lines. Time taken: 1.6137513518333435 minutes.\n",
      "Processed 70000000 lines. Time taken: 1.9017590920130412 minutes.\n",
      "Processed 80000000 lines. Time taken: 2.191321623325348 minutes.\n",
      "Processed 90000000 lines. Time taken: 2.490134263038635 minutes.\n",
      "Processed 100000000 lines. Time taken: 2.7884846766789755 minutes.\n",
      "Processed 110000000 lines. Time taken: 3.0721547563870746 minutes.\n",
      "Processed 120000000 lines. Time taken: 3.351078001658122 minutes.\n",
      "Processed 130000000 lines. Time taken: 3.6386491854985556 minutes.\n",
      "Processed 140000000 lines. Time taken: 3.9113255540529885 minutes.\n",
      "Processed 150000000 lines. Time taken: 4.21607338587443 minutes.\n",
      "Processed 160000000 lines. Time taken: 4.496213320891062 minutes.\n",
      "Processed 170000000 lines. Time taken: 4.777543584505717 minutes.\n",
      "Processed 180000000 lines. Time taken: 5.058374945322672 minutes.\n",
      "Processed 190000000 lines. Time taken: 5.3447688817977905 minutes.\n",
      "Processed 200000000 lines. Time taken: 5.633587229251861 minutes.\n",
      "Processed 210000000 lines. Time taken: 5.937698400020599 minutes.\n",
      "Processed 217873771 lines. Time taken: 6.183644851048787 minutes.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save page_inlink_counts.tsv:\n",
    "PageId, NumInlinks\n",
    "\"\"\"\n",
    "\n",
    "page_inlink_counts = {}\n",
    "start_time = time.time()\n",
    "line_no = 0\n",
    "with open(data_root_dir + 'page_links.tsv', 'r') as f:\n",
    "    for line in f:\n",
    "        line_no += 1\n",
    "        if line_no==1: continue\n",
    "        source_page, destination_page = line.split('\\t')\n",
    "        destination_page = int(destination_page)\n",
    "        page_inlink_counts[destination_page] = page_inlink_counts.get(destination_page, 0) + 1\n",
    "        if line_no % 10000000 == 0:\n",
    "            print(f\"Processed {line_no} lines. Time taken: {(time.time() - start_time) / 60} minutes.\")\n",
    "            # break\n",
    "\n",
    "print (f\"Processed {line_no} lines. Time taken: {(time.time() - start_time) / 60} minutes.\")\n",
    "\n",
    "with open(data_root_dir + 'page_inlink_counts.tsv', 'w') as f:\n",
    "    f.write(\"PageId\\tNumInlinks\\n\")\n",
    "    for page_id, num_inlinks in sorted(page_inlink_counts.items()):\n",
    "        f.write(f\"{page_id}\\t{num_inlinks}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6349743"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(page_inlink_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 lines in 0.6364568471908569 minutes.\n",
      "Processed 2000000 lines in 1.1012573758761088 minutes.\n",
      "Dumping data for part 0 containing 934933 category partial jsons.\n",
      "Finished part 0 in 1.5530202349026998 minutes\n",
      "Processed 3000000 lines in 1.9946431159973144 minutes.\n",
      "Processed 4000000 lines in 2.4322856227556864 minutes.\n",
      "Dumping data for part 1 containing 885607 category partial jsons.\n",
      "Finished part 1 in 2.8973546028137207 minutes\n",
      "Processed 5000000 lines in 3.1667434334754945 minutes.\n",
      "Processed 6000000 lines in 3.5835325280825296 minutes.\n",
      "Dumping data for part 2 containing 830371 category partial jsons.\n",
      "Finished part 2 in 3.9203997294108075 minutes\n",
      "Processed 7000000 lines in 4.218890984853108 minutes.\n",
      "Processed 8000000 lines in 4.613288362820943 minutes.\n",
      "Dumping data for part 3 containing 855587 category partial jsons.\n",
      "Finished part 3 in 4.940384646256764 minutes\n",
      "Processed 9000000 lines in 5.235450784365336 minutes.\n",
      "Processed 10000000 lines in 5.667127799987793 minutes.\n",
      "Dumping data for part 4 containing 945175 category partial jsons.\n",
      "Finished part 4 in 6.02795999844869 minutes\n",
      "Processed 11000000 lines in 6.366511865456899 minutes.\n",
      "Processed 12000000 lines in 6.740390900770823 minutes.\n",
      "Dumping data for part 5 containing 963213 category partial jsons.\n",
      "Finished part 5 in 7.116111703713735 minutes\n",
      "Processed 13000000 lines in 7.420632270971934 minutes.\n",
      "Processed 14000000 lines in 7.824355951944987 minutes.\n",
      "Dumping data for part 6 containing 980604 category partial jsons.\n",
      "Finished part 6 in 8.205620956420898 minutes\n",
      "Processed 15000000 lines in 8.525156577428183 minutes.\n",
      "Processed 16000000 lines in 8.873788162072499 minutes.\n",
      "Dumping data for part 7 containing 879264 category partial jsons.\n",
      "Finished part 7 in 9.267593530813853 minutes\n",
      "Processed 17000000 lines in 9.57073496580124 minutes.\n",
      "Processed 18000000 lines in 9.993673006693522 minutes.\n",
      "Dumping data for part 8 containing 900920 category partial jsons.\n",
      "Finished part 8 in 10.310697865486144 minutes\n",
      "Processed 19000000 lines in 10.641562215487163 minutes.\n",
      "Processed 20000000 lines in 10.99091682434082 minutes.\n",
      "Dumping data for part 9 containing 795220 category partial jsons.\n",
      "Finished part 9 in 11.249552460511525 minutes\n"
     ]
    }
   ],
   "source": [
    "# combine information from full dataset and save category pages from the processed summaries\n",
    "\n",
    "\"\"\"\n",
    "prepare category page details\n",
    "1. List of sub-categories\n",
    "2. List of articles\n",
    "3. List of parent categories\n",
    "4. List of internal links on category page\n",
    "\n",
    "To parallelize, \n",
    "1. dump data for each partition separately\n",
    "2. process categories based on their hashes in N parts and dumps complete info for each category\n",
    "\"\"\"\n",
    "\n",
    "def get_empty_category_data():\n",
    "    return {\n",
    "        'sub_categories': set(),\n",
    "        'internal_links': [],\n",
    "        'articles': [],\n",
    "        'parent_categories': set()\n",
    "    }\n",
    "\n",
    "start_time = time.time()\n",
    "processed_line_count = 0\n",
    "norm = wiki_analysis_utils.normalized_page_name\n",
    "for i in range(10):\n",
    "    category_name_to_data = {}\n",
    "    with open(data_root_dir + f'processed_summaries/part-{i}.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            page = json.loads(line)\n",
    "            namespace = page['namespace']\n",
    "            title = page['title']\n",
    "            page_id = page['page_id']\n",
    "            redirect_title = page['redirect_title']\n",
    "\n",
    "            if namespace == 14:\n",
    "                category_name = norm(title)\n",
    "                category_name_to_data[category_name] = category_name_to_data\\\n",
    "                    .get(category_name, get_empty_category_data())\n",
    "                category_data = category_name_to_data[category_name]\n",
    "                category_data['internal_links'].extend(map(norm, page['internal_links']))\n",
    "                category_data['parent_categories'].update(map(norm, page['categories']))\n",
    "                category_data['category_id'] = int(page_id)\n",
    "                for parent_category in page['categories']:\n",
    "                    parent_category = norm(parent_category)\n",
    "                    category_name_to_data[parent_category] = category_name_to_data\\\n",
    "                        .get(parent_category, get_empty_category_data())\n",
    "                    category_name_to_data[parent_category]['sub_categories'].add(category_name)\n",
    "            elif namespace == 0 and not redirect_title:\n",
    "                for category in page['categories']:\n",
    "                    category = norm(category)\n",
    "                    category_name_to_data[category] = category_name_to_data\\\n",
    "                        .get(category, get_empty_category_data())\n",
    "                    page_id = int(page_id)\n",
    "                    page_id = redirect_pid_to_pid.get(page_id, page_id)\n",
    "                    category_name_to_data[category]['articles'].append((title, page_id))\n",
    "            processed_line_count += 1\n",
    "            if processed_line_count % 1000000 == 0:\n",
    "                print(f\"Processed {processed_line_count} lines in {(time.time() - start_time) / 60} minutes.\")\n",
    "    os.makedirs(data_root_dir+'tmp/', exist_ok=True)\n",
    "    print (f\"Dumping data for part {i} containing {len(category_name_to_data)} category partial jsons.\")\n",
    "    with open(data_root_dir + f'tmp/partial_category_page_data_part-{i}.txt', 'w') as out_f:\n",
    "        for cat_name, data in category_name_to_data.items():\n",
    "            data[\"category_name\"] = cat_name\n",
    "            data['sub_categories'] = list(data['sub_categories'])\n",
    "            data['parent_categories'] = list(data['parent_categories'])\n",
    "            out_f.write(json.dumps(data)+\"\\n\")\n",
    "    print(f\"Finished part {i} in {(time.time() - start_time) / 60} minutes\")\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished part 0 in 2.061130122343699 minutes\n",
      "Finished part 1 in 3.9293649673461912 minutes\n",
      "Finished part 2 in 5.823503355185191 minutes\n",
      "Finished part 3 in 8.542663621902467 minutes\n",
      "Finished part 4 in 10.408614214261373 minutes\n",
      "Finished part 5 in 12.252984670797984 minutes\n",
      "Finished part 6 in 14.039662718772888 minutes\n",
      "Finished part 7 in 15.66956444978714 minutes\n",
      "Finished part 8 in 17.287773271401722 minutes\n",
      "Finished part 9 in 18.950868968168894 minutes\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(data_root_dir+'category_pages/', exist_ok=True)\n",
    "start_time = time.time()\n",
    "for partition in range(NUM_PARTITIONS):\n",
    "    category_name_to_data = {}\n",
    "    for i in range(10):\n",
    "        with open(data_root_dir + f'tmp/partial_category_page_data_part-{i}.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line==\"\": continue\n",
    "                data = json.loads(line)\n",
    "                if (hash(data['category_name']) % NUM_PARTITIONS) != partition:\n",
    "                    continue\n",
    "                if data['category_name'] not in category_name_to_data:\n",
    "                    category_name_to_data[data['category_name']] = get_empty_category_data()\n",
    "                category_data = category_name_to_data[data['category_name']]\n",
    "                if 'category_id' in data:\n",
    "                    category_data['category_id'] = data['category_id']\n",
    "                category_data['sub_categories'].update(data['sub_categories'])\n",
    "                category_data['parent_categories'].update(data['parent_categories'])\n",
    "                category_data['internal_links'].extend(data['internal_links'])\n",
    "                category_data['articles'].extend(data['articles'])\n",
    "    with open(data_root_dir + f'category_pages/part-{partition}.txt', 'w') as out_f:\n",
    "        for cat_name, data in category_name_to_data.items():\n",
    "            data[\"category_name\"] = cat_name\n",
    "            data['sub_categories'] = list(data['sub_categories'])\n",
    "            data['parent_categories'] = list(data['parent_categories'])\n",
    "            out_f.write(json.dumps(data)+\"\\n\")\n",
    "    print(f\"Finished part {partition} in {(time.time() - start_time) / 60} minutes\")\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed till part 0 in 0.07508935133616129 minutes\n",
      "Processed till part 1 in 0.1482577443122864 minutes\n",
      "Processed till part 2 in 0.2128618319829305 minutes\n",
      "Processed till part 3 in 0.2742927074432373 minutes\n",
      "Processed till part 4 in 0.35173648595809937 minutes\n",
      "Processed till part 5 in 0.47813774744669596 minutes\n",
      "Processed till part 6 in 0.5374097108840943 minutes\n",
      "Processed till part 7 in 0.595884398619334 minutes\n",
      "Processed till part 8 in 0.6622572024663289 minutes\n",
      "Processed till part 9 in 0.7481602390607198 minutes\n"
     ]
    }
   ],
   "source": [
    "# check for a few category pages that they look good\n",
    "\n",
    "selected_categories = [\"Platonic solids\", \"Sampling (statistics)\", \"Drama films\", \"Comedy novels\"]\n",
    "category_pages = wiki_analysis_utils.load_category_pages(data_root_dir, selected_categories=selected_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sub_categories': ['sampling techniques',\n",
       "  'empirical evidence',\n",
       "  'sample statistics',\n",
       "  'survey methodology'],\n",
       " 'internal_links': [],\n",
       " 'articles': [['Census', 6889],\n",
       "  ['Sampling bias', 17692],\n",
       "  ['Rock paper scissors', 27032],\n",
       "  ['Statistical unit', 27580],\n",
       "  ['Stratified sampling', 27596],\n",
       "  ['Infrastructure bias', 47280],\n",
       "  ['Sampling (statistics)', 160361],\n",
       "  ['Autodidacticism', 255591],\n",
       "  ['Opinion poll', 277315],\n",
       "  ['Margin of error', 277379],\n",
       "  ['Self-selection bias', 292154],\n",
       "  ['Lottery machine', 379930],\n",
       "  ['Selection bias', 394392],\n",
       "  ['Coin flipping', 494410],\n",
       "  ['Sampling distribution', 520670],\n",
       "  ['Recall bias', 1360950],\n",
       "  ['Survivorship bias', 1745325],\n",
       "  ['Sample size determination', 1776839],\n",
       "  ['Sampling error', 1955561],\n",
       "  ['Sampling frame', 2050041],\n",
       "  ['Odds and evens (hand game)', 2234844],\n",
       "  ['Sampling fraction', 2719222],\n",
       "  [\"Whipple's index\", 4039291],\n",
       "  ['Selective recruitment', 5054888],\n",
       "  ['Scale analysis (statistics)', 6055749],\n",
       "  ['Expander walk sampling', 6245532],\n",
       "  ['Multi-attribute global inference of quality', 7257602],\n",
       "  ['Acquiescence bias', 9092040],\n",
       "  ['Imperfect induction', 9350286],\n",
       "  ['Replication (statistics)', 10306262],\n",
       "  ['Sampling risk', 10533559],\n",
       "  ['Statistical benchmarking', 12106854],\n",
       "  ['Healthy user bias', 13408012],\n",
       "  ['Sampling design', 15061620],\n",
       "  [\"Gy's sampling theory\", 15132999],\n",
       "  ['Inherent bias', 15515980],\n",
       "  ['Kleroterion', 15626359],\n",
       "  ['Item tree analysis', 16569923],\n",
       "  ['Heckman correction', 16728089],\n",
       "  ['Acceptable quality limit', 17216134],\n",
       "  ['Balanced repeated replication', 18286082],\n",
       "  ['Sortition', 19288053],\n",
       "  ['Correct sampling', 20878968],\n",
       "  ['Sampling probability', 20900922],\n",
       "  ['Horsengoggle', 21348069],\n",
       "  ['Microdata (statistics)', 21725173],\n",
       "  ['Oversampling and undersampling in data analysis', 22101888],\n",
       "  ['Civic lottery', 22163364],\n",
       "  ['Unmatched count', 24479528],\n",
       "  ['Acceptance sampling', 25719356],\n",
       "  ['Judgment sample', 26271750],\n",
       "  ['Coverage error', 26513269],\n",
       "  ['Horvitz–Thompson estimator', 28014764],\n",
       "  ['Lot quality assurance sampling', 31548264],\n",
       "  ['Measuring attractiveness by a categorical-based evaluation technique',\n",
       "   45481361],\n",
       "  ['Variables sampling plan', 46818101],\n",
       "  [\"Cohen's h\", 47072912],\n",
       "  ['Prior-independent mechanism', 50984744],\n",
       "  ['Flow sampling', 53547121],\n",
       "  ['Stock sampling', 54248805],\n",
       "  ['Failure bias', 57231221],\n",
       "  ['NQuery Sample Size Software', 57449714],\n",
       "  ['Sufficient similarity', 60469653],\n",
       "  ['Stratified randomization', 63588277],\n",
       "  ['Strong and weak sampling', 66207446],\n",
       "  ['Common source bias', 68025570],\n",
       "  ['Drawing lots (decision making)', 74306685],\n",
       "  ['Area sampling frame', 75539112]],\n",
       " 'parent_categories': ['statistical methods', 'data collection in research'],\n",
       " 'category_id': 6536652,\n",
       " 'category_name': 'sampling (statistics)'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_pages['selected'][1]\n",
    "# category_pages['random'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed till part 0 in 0.06616474787394205 minutes\n",
      "Processed till part 1 in 0.12248515288035075 minutes\n",
      "Processed till part 2 in 0.18007128636042277 minutes\n",
      "Processed till part 3 in 0.2350926915804545 minutes\n",
      "Processed till part 4 in 0.40694777965545653 minutes\n",
      "Processed till part 5 in 0.5122623840967814 minutes\n",
      "Processed till part 6 in 0.5809729059537252 minutes\n",
      "Processed till part 7 in 0.6555250962575276 minutes\n",
      "Processed till part 8 in 0.7199227015177408 minutes\n",
      "Processed till part 9 in 0.7967670241991679 minutes\n",
      "2346796 {'MissingCategoryId': 22359}\n",
      "2346796 2346796\n"
     ]
    }
   ],
   "source": [
    "# load category name to id mappings from the category pages\n",
    "categories, failure_counts = wiki_analysis_utils.load_category_name_to_id_map(data_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed till part 0 in 0.14185847441355387 minutes\n",
      "Processed till part 1 in 0.29345881938934326 minutes\n",
      "Processed till part 2 in 0.43326702117919924 minutes\n",
      "Processed till part 3 in 0.5495299696922302 minutes\n",
      "Processed till part 4 in 0.7334250688552857 minutes\n",
      "Processed till part 5 in 0.8907455643018086 minutes\n",
      "Processed till part 6 in 1.0160117149353027 minutes\n",
      "Processed till part 7 in 1.177340281009674 minutes\n",
      "Processed till part 8 in 1.280317223072052 minutes\n",
      "Processed till part 9 in 1.3832293351491292 minutes\n",
      "Failure counts: {'Unknown Category': 22359, 'Unknown SubCategory': 0, 'Unknown ParentCategory': 170572}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Obtain and save these mappings:\n",
    "\n",
    "- category_id -> number of pages\n",
    "- category_id -> sub category ids, parent category ids\n",
    "- category_id -> page_ids\n",
    "\"\"\"\n",
    "\n",
    "category_id_to_data = {}\n",
    "\n",
    "start_time = time.time()\n",
    "failure_counts = {\n",
    "    'Unknown Category': 0,\n",
    "    'Unknown SubCategory': 0,\n",
    "    'Unknown ParentCategory': 0\n",
    "}\n",
    "log_level = 'ERROR'\n",
    "for partition in range(NUM_PARTITIONS):\n",
    "    with open(data_root_dir + f'category_pages/part-{partition}.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            if line=='': continue\n",
    "            category_json = json.loads(line)\n",
    "            category_data = {\n",
    "                'page_ids': [],\n",
    "                'sub_category_ids': [],\n",
    "                'parent_category_ids': []\n",
    "            }\n",
    "            category_id = categories['name_to_id'].get(category_json['category_name'], None)\n",
    "            if category_id is None:\n",
    "                if log_level in ['WARN', 'INFO']:\n",
    "                    print(f\"Category {category_json['category_name']} not found in category name to id mapping.\")\n",
    "                failure_counts['Unknown Category'] += 1\n",
    "                continue\n",
    "            category_id_to_data[category_id] = category_data\n",
    "            for _, page_id in category_json['articles']:\n",
    "                category_data['page_ids'].append(page_id)\n",
    "            for sub_category in category_json['sub_categories']:\n",
    "                sub_category_id = categories['name_to_id'].get(sub_category, None)\n",
    "                if sub_category_id is None:\n",
    "                    if log_level in ['WARN', 'INFO']:\n",
    "                        print(f\"Sub category {sub_category} not found in category name to id mapping.\")\n",
    "                    failure_counts['Unknown SubCategory'] += 1\n",
    "                    continue\n",
    "                category_data['sub_category_ids'].append(sub_category_id)\n",
    "            for parent_category in category_json['parent_categories']:\n",
    "                parent_category_id = categories['name_to_id'].get(parent_category, None)\n",
    "                if parent_category_id is None:\n",
    "                    if log_level in ['WARN', 'INFO']:\n",
    "                        print(f\"Parent category {parent_category} not found in category name to id mapping.\")\n",
    "                    failure_counts['Unknown ParentCategory'] += 1\n",
    "                    continue\n",
    "                category_data['parent_category_ids'].append(parent_category_id)\n",
    "    print(f\"Processed till part {partition} in {(time.time() - start_time) / 60} minutes\")\n",
    "\n",
    "print (f\"Failure counts: {failure_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save these mappings to tsv files\n",
    "\n",
    "with open(data_root_dir + 'category_id_to_stats.tsv', 'w') as f:\n",
    "    f.write(\"CategoryId\\tNumPages\\tNumSubCategories\\tNumParentCategories\\n\")\n",
    "    for category_id, data in category_id_to_data.items():\n",
    "        f.write(f\"{category_id}\\t{len(data['page_ids'])}\\t{len(data['sub_category_ids'])}\\t{len(data['parent_category_ids'])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_root_dir + 'category_id_to_page_ids.tsv', 'w') as f:\n",
    "    f.write(\"CategoryId\\tPageId\\n\")\n",
    "    for category_id, data in category_id_to_data.items():\n",
    "        for page_id in data['page_ids']:\n",
    "            f.write(f\"{category_id}\\t{page_id}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_root_dir + 'category_id_to_sub_category_ids.tsv', 'w') as f:\n",
    "    f.write(\"CategoryId\\tSubCategoryId\\n\")\n",
    "    for category_id, data in category_id_to_data.items():\n",
    "        for sub_category_id in data['sub_category_ids']:\n",
    "            f.write(f\"{category_id}\\t{sub_category_id}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_root_dir + 'category_id_to_parent_category_ids.tsv', 'w') as f:\n",
    "    f.write(\"CategoryId\\tParentCategoryId\\n\")\n",
    "    for category_id, data in category_id_to_data.items():\n",
    "        for parent_category_id in data['parent_category_ids']:\n",
    "            f.write(f\"{category_id}\\t{parent_category_id}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
