
# Papers / Ideas

## Surveys

## Architecture

- [Attention OG] [Attention Is All You Need](https://arxiv.org/pdf/1706.03762) (2017)
    - Main experiment problem: Machine Translation
    - Encoder-Decoder architecture with attention mechanism
        - encoder takes input sparse (w_1, w_2, ..., w_n) and converts it into dense representation (z_1, z_2, ..., z_n)
        - decoder takes dense representation and generates output sparse (y_1, y_2, ..., y_m)
        - both encoder and decoder have 6 layers each
        - embedding dimension, d_model = 512
    - encoder
        - input = token embeddings + positional encodings
        - output of hidden layer with contextual embeddings
        - layer architecture
            - Multi-Head Self-Attention
                - not masked. Every word can see every other word
                - queries, keys, and values all come from the same input
            - Residual Connection
            - Layer Normalization
                - LayerNorm(x + SubLayer(x))
            - Feed Forward Network
                - FFN(x) = ReLU(x W_1 + b_1) W_2 + b_2
                - appied independently to each position
            - Residual Connection + Layer Normalization
    - decoder (causal / auto-regressive)
        - input: output token embeddings shifted 1 position to the right, encoder output, positional encodings
        - layer architecture
            - (decoder only) Multi-Head Self-Attention  // TODO: why is this layer present? Presumably because this one attends to decoder only and the next one attends to encoder only. So there is a separate layer for each instead of a single layer that attends to both. 
                - only on previous words and not encoder output
                - each word can only see previous words
                - queries, keys, and values come from decoder previous words only. No encoder output is used here
            - residual connection + layer normalization
            - (encoder-decoder both) Multi-Head Self-Attention
                - on both encoder output and previous words
                - queries come from previous decoder layer,
                - keys and values come from encoder output
                - intuition is for the decoder to attend to and incorporate information from the encoder
            - residual connection + layer normalization
            - Feed Forward Network
            - residual connection + layer normalization
        - final is linear layer with softmax
    - attention
        - d_k = d_v = d_model / h = 64 (h=8 heads)
        - scaled dot-product attention
            - MaskedAttention (X_O Wq_i, X_I Wk_i, X_I Wv_i) = (softmax((X_O Wq_i) (X_I Wk_i)^T / sqrt(d_k) + Mask) (X_I Wv_i))
            - X_I is N_I x d_model matrix representing input to the layer. N_I = input sequence length. It is maximum length of the sequence given input to the layer. Paddings are masked and future words are masked (in case of decoder)
            - Wq_i is d_model x d_k matrix representing matrix that transforms input to query
                - X_O is N_O x d_k matrix representing subset of input sequence that is used to calculate output sequence. N_O = output sequence length
                    - for the layer that takes both encoder output and previous words, N_I = max_encoder_input_length + max_decoder_input_length and N_O = max_decoder_input_length
                    - only use decoder emebeddings as queries
                - X_O Wq_i is N_O x d_k matrix representing queries for each element in the output sequence
            - Wk_i is d_model x d_k matrix representing matrix that transforms input to key
                - X_I Wk_i is N_I x d_k matrix representing keys for each element in the input sequence
            - attention weights: 
                - A = (X_O Wq_i) (X_I Wk_i)^T . M / sqrt(d_k) is N_O x N_I matrix representing attention weights for each element in the sequence
                - M masks the future words and paddings
                - A_lm = (row l of X_O Wq_i) . (column m of (X_I Wk_i)^T) / sqrt(d_k) + M_lm
                    - (row l of X_O Wq_i) is 1 x d_k matrix representing the query for l-th element in the output sequence
                    - (column m of (X_I Wk_i)^T) is d_k x 1 matrix representing the key for m-th element in the input sequence
                    - M_lm -infinity if l^th shouldn't be attending to m^th element. Softmax will make these weights 0
                    - A_lm is unnormlized attention weight that l^th output pays to m^th input
            - (X_I Wv_i) is N_I x d_v matrix representing values for each element in the input sequence that it can contribute to any element of output sequence that attends to it
            - head_i = softmax(A) (X_I Wv_i) is N_O x d_v matrix representing output of the layer for this head
                - (softmax(A) (X_I Wv_i))_xy = sum_m softmax(A)_xm (X_I Wv_i)_my
                    - this is taking weighted average of value vectors depending on attention weights
        - MultiHead Self Attention
            - Concat(head_1, head_2, ..., head_h) W^O is h * d_v x d_model matrix to combine output of all heads
    - weight tying: same embedding matrix used for both encoder and decoder
    - positional encodings
        - non-learnable and fixed sin and cos functions of different frequencies
        - added to input embeddings
        - designed so that PE_pos+k = W * PE_pos, i.e. positional embeddings can be tanformed to any other position by linear trasnformations only
        - fixed sin-based positional embeddings may allow for generalization to longer sequences not present in training data
        - learned embeddings tend to look like these fixed embeddings naturally
    - interpretability
        - attention heads are clearly performing different functions relating syntactic and semantic structure of the sentence // TODO: explore this literature 
    - training
        - datasets
            - WMT 2014 English-German
                - 4.5 million sentence pairs
                - 37k BPE vocabulary (shared source and target)
            - WMT 2014 English-French
                - 36 million sentence pairs
                - 32k word-peice vocabulary (shared source and target)
        - hardware
            - 8 NVIDIA P100 GPUs
            - 12 hours (100k steps total and 0.4 seconds per step) for small models
            - 3.5 days (300k steps total and 1.0 seconds per step) for big models
        - hyper params
            - 25000 tokens per batch
            - Adam optimizer with beta1=0.9, beta2=0.98, epsilon=10^-9
            - lrate schedule, learning rate = d_model^-0.5 * min(step_num^-0.5, step_num * warmup_steps^-1.5)
                - slowly increase learning rate for first warmup_steps (=4000) steps and then decrease it proportional to inverse square root of step number
            - regularization
                - dropout rate = 0.1 to each sublayer just before adding residual connection and then layer normalization
                - label smoothing = 0.1  // TODO: read up on this
                    - flip 10% of the one-hot target vector to 0.1
        - results
            - had better BLEU score than previous state-of-the-art models
        - ablation study and observations
            - bigger models are better
            - tuning h, dk, dv is important given size is fixed
            - using fixed positional encodings is as good as learned ones

- [FLASH ATTN] [FLASHATTENTION: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf) [NIPS 2022]
    - TODO: read and summarize
        - faster implementation of attention in GPUs
        - has 3x speed on GPT
        - low level discussion can help understand what exactly goes on inside a GPU to implement attention

- [LoRA] [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685)            
    - TODO: read and summarize
        - can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times



## Mainstream LLMs

- [GPT2] [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) 
    - TODO: read and summarize
        - first paper that shows zero-shot learning
        - fundamental shift from fine-tuning towards zero-shot or few-shot learning

- [GPT3] [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165)
    - TODO: read and summarize
        - better performance than GPT-2
        - more concretizing prompt engineering approach (v/s fine tuning)
        - still can't reach SOTA on most benchmarks though

- [GPT4] [GPT-4 Technical Report](https://arxiv.org/pdf/2303.08774)
    - TODO: read and summarize
        - how much detail do these papers even contain?
        - abstract mentions no details of comaprison of current models with SOTA on various benchmarks

- [LLaMa] [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971)
    - TODO: read and summarize
        - open source model with weights available
        - uses publically available datasets only
        - outperforms GPT3
        - should be a better resource to learn how an LLM works

- [LLaMa2] [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288)
    - TODO: read and summarize
        - did llama-1 have no chat RLHF?
        - abstract says outperforms open source models so probably doesn't outperform GPT-4

- [LLaMa3] [The Llama 3 Herd of Models](https://scontent-xsp1-2.xx.fbcdn.net/v/t39.2365-6/452387774_1036916434819166_4173978747091533306_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=7qSoXLG5aAYQ7kNvgFUJ-mf&_nc_ht=scontent-xsp1-2.xx&gid=AO4mOvzW0LidiGAXOi6LOhU&oh=00_AYDN7sR1GJX6VVz-NsU5gu_zHQw8QnIBIBgyEdHe-yXAOQ&oe=66ABF94D)
    - TODO: read and summarize
        - native support for multilinguality, coding, reasoning, and tool usage
        - context window of up to 128K tokens
        - comparable performance with GPT-4
        - includes  Llama Guard 3 model for input and output safety
        - integration of image, video, and speech capabilities into Llama 3 via a compositional approach

- [Mistal7B] [Mistral 7B](https://arxiv.org/pdf/2310.06825)
    - TODO: read and summarize
        - outperforms 13B llama2
        - tricks for faster inferences
            - grouped-query attention (GQA), sliding window attention (SWA)
        - natively handles arbitrary length inputs (like RNN)
        - Mistral 7B – Instruct models that outperform Llama 2 13B – chat model on human and automated benchmarks
            - would be interesting to see how these benchmarks are designed

- [Mixtral] [Mixtral of Experts](https://arxiv.org/pdf/2401.04088)
    - TODO: read and summarize
        - what exactly is mixture of experts?
        - outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks
        - Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks
        - We also provide a model finetuned to follow instructions, Mixtral 8x7B – Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B – chat model on human benchmarks

## Fine Tuning

- [BERT OG] [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)
    - feature based: use pre-trained model's output as static features for downstream tasks
    - fine tuning: fine-tune all parameters of pre-trained model on downstream task
    - pretraining
        - architecture
            - same as attention is all you need with no masking
            - L = num of transformer layers, H = embedding size, feed forward size = 4*H, A = num attention heads
            - BERT BASE (L=12, H=768, A=12, Total Parameters=110M)  // same size as GPT
            - BERT LARGE (L=24, H=1024, A=16, Total Parameters=340M)
            - word piece tokenizer with 30k token vocabulary
            - input / output representations
                - able to represent single sentence or a pair of sentences
                - first token is always [CLS] (classification token)
                - [CLS] <sentence-1> [SEP] <sentence-2> [SEP] (sentence 2 is optional)
                - add learned embeddings for segment ids (0 for sentence 1, 1 for sentence 2)
                    - input embeddings = token embeddings + segment embeddings + positional embeddings
        - masked language model
            - 15% of input tokens are masked, 80% of masked tokens are replaced with [MASK], 10% of masked tokens are replaced with random token, 10% of masked tokens are left as is
                - [MASK] will never appear during fine tuning. Thats why 20% samples don't have it so the model doesn't get confused during fine tuning
        - next sentence prediction
            - binary classification: 50% of the time, sentence 2 is the actual next sentence of sentence 1, 50% of the time, sentence 2 is a random sentence from the corpus
            - shown very beneficial for QA and NLI tasks
            - [CLS] token's output is used for the classification task
                - [CLS] not a meaningful sentence representation without fine tuning and presence of a sentence pair
        - data
            - BooksCorpus (800M words) + English Wikipedia (2.5B words) (include on text passages. Ignore lists, tables, etc.)
    - fine tuning
        - tune all parameters end-to-end
        - not very expensive. Most reported tasks finish in a few hours on a single GPU
        - input wiring for different tasks
            - sentence pairs in paraphrasing
            - hypothesis-premise pairs in entailment
            - question-passage pairs in question answering
            - a degenerate text-∅ pair in text classification or sequence tagging
        - output wiring for different tasks
            - [CLS] token's output for classification tasks
    - experiments
        - beats all existing baselines on 11 NLP tasks
        - GLUE (General Language Understanding Evaluation)
            - single sentence
                - CoLA The Corpus of Linguistic Acceptability: "They drank the pub dry." v/s "They drank the pub.". Is the sentence grammatically correct?
                - SST-2 The Stanford Sentiment Treebank. Predict movie review sentiment
            - similarity and paraphrasing
                - MRPC Microsoft Research Paraphrase Corpus. Predict if two sentences are paraphrases
                - QQP Quora Question Pairs. Predict if two questions are semantically equivalent
                - STS-B Semantic Textual Similarity Benchmark. Predict similarity score between two sentences. 
                    - dissimilar: "A man is smoking" v/s "A man is skating."
                    - similar: "A plane is taking off" v/s "An air plane is taking off"
            - inference
                - MNLI Multi-Genre Natural Language Inference. Predict entailment, contradiction, or neutral relationship between two sentences
                - QNLI The Stanford Question Answering Dataset. question-paragraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question (written by an annotator).
                - RTE The Recognizing Textual Entailment (RTE) datasets come from a series of annual textual entailment challenges
                - WNLI The Winograd Schema Challenge (Levesque et al., 2011) is a reading comprehension task in which a system must read a sentence with a pronoun and select the referent of that pronoun from a list of choices
            - batch size 32, 3 epochs, learning rate tuned from {5e-5, 4e-5, 3e-5, 2e-5} on dev set
            - BERT large was unstable on small datasets so pick best from randomized restarts on shuffled data
            - BERT large always outperformed BERT base, especially when fine tuned on small datasets
        - SQuAD (Stanford Question Answering Dataset)
            - 100k crowdsourced question-paragraph pairs
                - given question and paragraph, predict start and end token of the answer in the paragraph
            - plugin question as sentence A and paragraph as sentence B
            - introduce two new tokens [START] and [END] to predict start and end of the answer
                - training: cross entropy loss on correct start and end token locations
                - inference: pick i, j (i < j) that maximizes S . T_i + E . T_j
            - SQuAD v2
                - adds possibility of no answer
                - in case of no answer, train S.C and E.C to maximize at [CLS] token
                - inference: s_null = S . C + E . C and pick i, j that maximizes S . T_i + E . T_j or s_null, whichever is higher
        - SWAG (Situations With Adversarial Generations)
            - 113k sentence pairs
            - given a sentence, predict the most plausible continuation among 4 choices
            - plugin sentence as sentence A and candidate continuation as sentence B
            - use output of [CLS] to score each candidate  // TODO: what exactly is the loss function?
    - ablation study
        - removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD
        - left to right always performs worse than bidirectional
        - bigger models are better on all tasks, even when training data is very small
        - feature based approaches also work, though not as well as fine tuning
            - pick final 2-3 layers of BERT and use their output as features for training a new model from scratch on these static precomputed features
             

- [T5] [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683)
    - convert every NLP problem into a text-to-text problem
        - formulate your problem as a text-to-text problem
        - fine tune T5 on your dataset. Eg: for translation, input = "translate English to German: That is good.", output = "Das ist gut."
        - use T5 to generate output for your input during inference
    - comparison with BERT
        - tasks: BERT -> masked language model and next sentence prediction, T5 -> text-to-text. T5 is broader
        - T5 is focuses more on Text generation. BERT is encoder only. You can't use it for something like machine translation without building a decoder on top of it
            - T5: translation, summarization, question answering
            - BERT: classification, question answering
    - architecture
        - similar to attention is all you need and BERT
        - simplified layer normalization with no bias term
        - residual connection is *after* layer normalization
        - different positional embeddings than original paper
            - relative positional embeddings  // TODO: read up on this
            - different embeddings for offset between positions of a "query" and "key" in self-attention
                - simple version used: simply a scalar that gets added to the corresponding logit inside the attention function (??? is it added to A_lm, where A_lm is how much l^th output pays attention to m^th input?)
            - share positional parameters across all layers
            - only 32 embeddings that scale upto 128 positional difference. Beyond which, all far away positions are mapped to the same embeddings
            - context length is not fixed to 128 because subsequent layers can attend to far away positions through previous layers if they wish to
        - key differences from "Attention is all you need"
            - remove bias term from layer norm
            - add residual connection after layer norm
            - use a different positional embedding scheme
        - both encoder and decoder as similar in size to BERT_base
            - 12 layers (layer = self-attention, optional encoder-decoder attention, and a feed-forward network)
            - d_model = 768, d_kv = 64, d_ff = 3072, num_heads = 12, dropout = 0.1 everywhere
            - 220 million parameters (2x BERT_base)
        - causal (left to right) cross entropy loss on decoder output
    - colossal clean crawled corpus (C4)
        - tricks to remove noise like
            - retained lines that ended in a terminal punctuation mark, discarded any page with fewer than 3 sentences, removed any page that contained any bad word
            - detect and remove boilerplate policy notices like "terms of service" and "privacy policy"
            - ...
        - keep only english
        - extracted from April 2019
        - 750 GB (common crawl dumps 20TB data daily)
    - downstream tasks
        - machine translation, summarization, question answering, text classification
        - GLUE, SuperGLUE
            - Coreference resolution (dereference pronouns)
        - CNN/Daily Mail abstractive summarization
        - SQuAD question answering
        - WMT English to German, French, and Romanian translation
    - input-output format
        - text-to-text format: input = text and output = text
        - encode task in prompt
            - input = "translate English to German: That is good.", output = "Das ist gut."
            - classification
                - input = "mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity", output = "entailment"
                - if model outputs wrong word then it is considered wrong though authors never saw this happen
            - input = "summarize: You know I used to hate action thrillers. I specifically liked ...", output = "The movie was great. It had a lot of action."
            - input = "stsb sentence1: The rhino grazed on the grass. sentence2: A rhino is grazing in a field", output = "3.8" (sentence similarity task)
    - training
        - teacher forcing (feed the correct output instead of previous model output to the model during training)
        - AdaFactor optimizer, trained for 2^19 = 524288 steps on C4 before fine tuning
        - batch size = 128 and max sequence length = 512
        - greedy decoding at test time
        - pack multiple sequence into entry of the batch ot maximize utilization. Each batch has ~ 512 * 128 = 65k tokens
        - total budget = 34B tokens (less than BERT which used 137B tokens)
        - inverse square root learning rate schedule with warmup: 1 / sqrt(max(n, k)). lr = 0.01 for 10^4 steps and then decay
        - tokenizer: SentencePiece. classified C4 pages into english, german, french, and romanian
            - train sentencepiece model on 10 part english, 1 part german, 1 part french, 1 part romanian
        - denoising objective
            - randomly sample 15% of tokens in input to drop out
            - replace each dropped out span with a single sentinel token
            - output: dropped out token spans delimited by the same sentinel token used in input plus a final sentinel token to mark end of output
    - results
        - pretraining provides significant gains on all benchmarks
            - gain is minimal in WMT English to French that already has a large dataset
        - denoising loss function performs better on downstream task than vanilla next word prediction
        - reducing layers hurts performance bad -> bigger models are better
        - different objectives compared
            - prefix lm: "Thank you for inviting" -> " me to your party."
            - BERT-style: "Thank you <M> <M> me to your party apple week ." -> "(original text)"
            - MASS-style: "Thank you <M> <M> me to your party <M> week" -> "(original text)"
            - Deshuffling: " party me for your to . last fun you inviting week Thank" - "(original text)"
            - variants of BERT-style
                - I.i.d. noise, replace spans: "Thank you <X> me to your party <Y> week ." -> "<X> for inviting <Y> last <Z>"
                - I.i.d. noise, drop tokens: "Thank you me to your party week ." -> "for inviting last"
            - Random spans: "Thank you <X> to <Y> week ." -> "<X> for inviting me <Y> your party last <Z>"
        - BERT wins out of [BERT, prefix LM, deshuffling]. Slightly outperforms prefix lm. deshuffling performs the worst
        - for variants of BERT, I.i.d. noise, replace spans performs the bestt. Difference wasn't much
        - corruption rate had limited effect on downstream performance. Except too high corruption rate (50%) hurt performance
        - finally, random spans performed similarly as I.i.d. noise, replace spans. So, T5 uses it
        - Architecture Ablation Summary: 
            - model size is a big factor
            - deshuffling is bad
            - rest everything doesn't matter much
        - effect of data type
            - pre-training on in-domain unlabeled data can improve performance on downstream tasks
                - news works for news, books work for books, etc.
        - effect of data size
            - very low loss on small datasets -> evidence of memorization
            - bigger datasets are better
        - comparision of fine tuning methods
            - alternatives
                - fine tune all parameters (encoder + decoder)
                - adapter layers: freeze pretrained network and only add new layers on top during fine tuning
                - gradual unfreezing
            - observation: fine tuning all parameters is the best
        - multi-task learning framework
            - alternative to pretrain-then-fine-tune framework
                - allowed to train on unsupervised tasks too
            - important considerations
                - how much data from each task? Avoid over- or under- fitting to any one task
                    - difficult task may need more data
                - "task interference" or "negative transfer"
            - techniques
                - Examples-proportional mixing: sample tasks in proportion to their data size. Put artificial limit K on huge tasks like self-superivsed learning or else they'll dominate the training data set
                - Equal mixing: sample tasks uniformly
                - Temperature-scaled mixing: rescale softmax temperature to control how much model focuses on each task. T=1 -> examples-proportional mixing and T = infinity -> uniform mixing
            - observations
                - multi-task performs worse than pretrained-then-fine-tune
                - equal mixing many times have awful performance
                    - low resource tasks overfit and high resource tasks underfit
                - artifical limit "K" for examples-proportional mixing has a sweet spot
                - temperature-scaled mixing performs well on most tasks with T=2
                - multi-task model performs worse than single task model trained on a single task
        - Combining Multi-Task Learning with Fine-Tuning
            - 3 variants
                - multi-task training:
                    - pretrain on examples-proportional mixing (K=2^19) and then fine tune on each task
                    - hope is that seeing downstream task during pretraining will help during fine-tuning
                - "leave-one-out" multi-task training:
                    - pretrain on examples-proportional mixing (K=2^19) but skip the task on which you want to fine tune
                    - compare this with previous variant to see if seeing downstream task during pretraining helps
                - pre-train on supervised task only before fine-tuning
            - results
                - multi-task pre-training + fine-tuning performs same as unsupervised pre-training + fine-tuning
                - "leave-one-out" training is only slightly worse than multi-task training
                - unsupervised pre-training helps in most tasks except for translation
        - Scaling Laws
            - which scale factors give most bang for the buck?
            - baseline = 220M parameters, pre-training for 2^19 steps and fine-tuning for 2^18 steps
            - variants
                - BERT_LARGE like model (d_ff = 4096, d_model = 1024)
                    - Baseline_2x: has ~ 2x more parameters than baseline
                    - Baseline_4x: has ~ 4x more parameters than baseline
                - 3 ways of using 4x computation
                    - train baseline for 4x more steps
                    - train baseline_2x for 2x more steps
                    - train baseline_4x for 1x more steps
                    - NOTE: trainng more steps = seeing more data as C4 is huge
                - ensemble methods for 4x computation
                    - 4 seperately trained models and combine their outputs
                    - 1 single pretrained model + 4 fine-tuned ensembles that are then combined (NOTE: this doesn't utilize 4x computation budget)
                - 4x batch size instead of 4x training steps
            - results
                - both increasing training time (or data) and model size help
                    - better than only increasing one of them and keeping other constant
                    - no clear winner between training steps and model size. Both are complementary and add to the performance
                - no clear winner between training steps and batch size
                - 4x ensemble method also have similar performance and even best in some cases
                - 4x ensemble on fine-tuned models performs better than baseline
                - ensembling works and is probably an orthogonal direction to scale
    - final design based on these learnings
        - objective
            - replace i.i.d denoising in baseline with random span corruption
                - span corruption objective with mean span length = 3 and corrupt rate = 15%
                - new objective is slightly better in both performance and efficiency (due to shorter target lengths)
        - longer training
            - C4 is much bigger than the initial dataset used by the baseline
            - pre-train on 1 million steps, batch size = 2^11 = 2048, sequence length = 512 => total 1 trillion pre-training tokens (32x more than baseline)
            - did not include in-domain data (wiki, books, etc.) in pre-training and used C4 only
                - they'd be diluted in the huge C4 dataset without repetitions
                - repetitions could hurt performance
                - gains from in-domain data are small compared to gains from more data
            - model sizes: train different variants
                - base: 220 M parameters 
                    - Same as baseline model
                - small: 60 M parameters
                    - d_model = 512 (instead of 768), d_ff = 2048 (instead of 3072), num_heads = 8 (instead of 12), num_layers = 6 (instead of 12)
                - large: 770 M parameters 
                    - similar to BERT_LARGE
                    - d_model = 1024 (instead of 768), d_ff = 4096 (instead of 3072), num_heads = 16 (instead of 12), num_layers = 24 (instead of 12)
                - 3B and 13B parameters
                    - d_model is still 1024, num_layers is still 24 like BERT_LARGE
                    - 3B: 32 heads and d_ff = 16,384
                    - 13B: 128 heads and d_ff = 65,536
                    - d_ff is increased because TPUs that google uses are more efficient for these large dense matrix multiplications
        - multi-task pretraining
            - though it perform similar to single-task pretraining, it is allows for "tracking" model peroformance on downstream tasks during training process
            - used standard examples-proportional mixing with K tuned based on model size
        - fine-tune on individual glue tasks
            - need to be extra careful for small datasets
                - reduce batch size to 8 and frequently checkpoint to get model parameters before overfitting
        - Beam search (instead of greedy decoding)
    - final results
        - achieve sota in most tasks except WMT translations tasks (probably due to more English during pre-training)
        - biggest model performs best on all tasks

## Multi-Modal Models

- [CLIP] [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020v1)
    - TODO: read and summarize
        - beats the original ResNet-50 on ImageNet in zero shot
        - uses image caption pairs from internet
        - publically available weights

## Distillation

- [DistilBERT] [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/pdf/1910.01108)
    - TODO: read and summarize
        - see what different distillation methods are available
        - how much speed up do they provide?


# Links

