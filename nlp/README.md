
# NLP Book Notes

## Section-1: Text Processing

- Regular Expressions
    - applications
        - extract info: dates, times, emails, phone numbers
        - validate correctness of data. eg: email, phone number
        - tokenizer built using optimized DFA regex
- Tokenization
    - what counts as a word? -> depends on application
        - Understand the dataset
            - Motivation of data creation, situation of data collection, language variety (language, dialect, slang, etc.), speaker demographics, collection process, annotation process, distribution (copyright / IP restrictions, etc.)
        - Understand the task
            - punctuations / special characters: useful for $45.55, 01/01/2020, abc@xyz.com, etc.
            - lemmatization
                - useful for inverted index
            - depends on language
                - for chinese, may use the characters directly
                - for japanese, the tokenizer is very complex and is a separate trained model by itself
        - aim to balance between
            - tokenizing too much: deep learning model would invest too much time in learning that most combinations are not useful and only some are useful. Given a sequence of characters like: 'n', 'e', 'w', ' ', 'y', 'o', 'r', 'k', there are catalan number of unique ways to split it. But only 3-4 are useful. So, the model would have to learn that most of the splits are not useful. This is a waste of time and resources.
            - tokenizing too little: if "to be or not to be" is a single token then it appears very infrequently in the corpus and so would have less data to learn its meaning. Eg: say the task is machine translation. The sentence "to be or not to be" appears only 2-3 times in training data with long translations. It will be difficult to learn exactly why it was translated that way with jus tthese samples. But, words like "to", "be", etc. appear a lot and their translation meaning could be generalized and useful for the bigger sentence "to be or not to be" as well.
    - Herdan/Heap's law: `V = k * N^b`
        - for large corpus, `b` is between 0.67 and 0.75 sp vocab size grows faster than square root of corpus size
    - top-down (rule based)
        - simple lowercase, remove special characters, split on whitespace
        - nltk.regexp tokenize
        - Penn Treebank tokenization standard (hand-crafted rules)
    - bottom-up (unsupervised)
        - Byte Pair Encoding (BPE)
            - starting from individual characters, merge most frequent pair of sub-words
        - [Unigram Language Modelling](https://arxiv.org/pdf/1804.10959)
            - TODO: read up on botton-up tokenizers in more detail
- Word Normalization
    - convert USA, US, U.S.A., etc. to a normalized form, say, United States of America
        - looses information but useful if the lost information is not important for the task
            - positive example: aggressive normalization in inverted search index (drop stop words, case-folding, lemmatization, etc.)
            - negative example: in sentiment analysis, the capitalization of words is important. Eg: "I am not happy" vs "I am NOT happy"
    - case-folding
    - lemmatization
- Minimum Edit Distance
    - dynamic programming
    - Viterbi algorithm is a probabilistic extension of minimum edit distance
        - Instead of computing the minimum edit distance between two strings, Viterbi computes the maximum probability alignment of one string with another

## Section-2: N-gram Language Models

- Applications
    - large language models
    - correct grammar / spelling mistakes: Their are two midterms
    - speech recognition: I will be back soonish and not I will be bassoon dish
    - augmentative and alternative communication systems
- model
    - markov assumption on chain rule of conditional probability
    - MLE estimate: `P(w_i | w_{i-1}) = count(w_{i-1}, w_i) / count(w_{i-1})`
- linguistic phenomenon captured
    - syntactic
        - what comes after eat is usually a noun or an adjective
        - what comes after to is usually a verb
    - context in which the language is used. Eg: if the data is about personal assistant task, sentences may have high probability of starting with "I", as in, "I want to..."
    - cultural context. Eg: probability of "Chinese" might be higher than "English" for the sentence, "I am looking for a good _ restaurant"
- evaluation
    - extrinsic: improve performance on downstream task
    - intrinsic: perplexity
        - perplexity is the inverse probability of the test set, normalized by the number of words. Or log likelihood per word.
            - important to avoid contamination of training and test data
        - The test set should reflect the language we want to use the model for
        - perplexity as weighted average branching factor of a language model
            - TODO: undestand the intuitive link between this definition and the definition of perplexity as normalized inverse likelihood probability of the test set
        - connected to entropy. TODO: don't quite understand this
            - entropy is the average number of bits needed to encode a word
            - perplexity is 2^entropy
- unknown words
    - closed vocabulary: defined only when test set contains only known words
        - subword tokenizers like BPE are like this
    - open vocabulary
        - add `UNK` token in vocab
            - A language model can achieve low perplexity by choosing a small vocabulary and assigning the unknown word a high probability.
            - Language models are comparable (through perplexity) only if they have exact same vocabulary
- smoothing
    - shave off some probability mass from frequent events and give it to unseen events
    - laplace smoothing
        - p(w_i | w_{i-1}) = (count(w_{i-1}, w_i) + 1) / (count(w_{i-1}) + V)
        - add-k smoothing
            - p(w_i | w_{i-1}) = (count(w_{i-1}, w_i) + k) / (count(w_{i-1}) + kV)
            - k=1 moves lots of probability mass from frequent events to unseen events
            - tune k on validation set
    - back off: move to lower order model if higher order model has zero probability
        - need to shave off some probability mass from higher order model to give to lower order model to make sure the sum of probabilities is 1
        - Katz backoff, Good-Turing, Good-Turing backoff
            - P(w_i | w_{0:i-1}) = P'(w_i | w_{i-1}) if count(w_{0:i}) > 0 else alpha(w_{0:i-1}) * P(w_i | w_{1:i-1})
            - P' and alpha are picked so that the sum of probabilities is 1
            - many sophisticated variations of this. TODO: read up on them
        - Huge n-gram language model
            - efficiency is important: 4-8 bit quantized probabilities, use 64-biit hash ids instead of strings for words, store word strings in disk, store ngram in reverse tries
            - use of cache- keep only n-grams with count > k
            - entropy based pruning, approximate LM using bloom filters. TODO: what?
            - stupid back-off: relax the constraint that the sum of probabilities should be 1. Works well in practice. TODO: read up
    - interpolation: linear combination of higher and lower order models
        - learn weights on validation set
            - EM algorithm: fix ngram probabilities and learn interpolation weights. Then fix interpolation weights and learn ngram probabilities
    - absolute discounting
        - GOOD IDEA: split dataset 50:50 and compare counts for same ngrams in the held out set to identify correct discounting
            - observation: absolute discounting is a good model for the held out set
        - p(w_i | w_{i-1}) = (count(w_{i-1}, w_i) - d) / count(w_{i-1}) + lambda(w_{i-1}) * p(w_i | w_{i-1})
            - not too bad for high count n-grams but doesn't help much for low count n-grams
    - Kneser-Ney smoothing
        - Pcontinuation: I can’t see without my reading _. If bigram count is 0, then KONG (from HONG KONG) has more probability then glasses in unigram
            - P_KN(w_i | w_{i-1}) = max(count(w_{i-1}, w_i) - d, 0) / count(w_{i-1}) + lambda(w_{i-1}) * Pcontinuation(w_i)

## Section-3: Text Classification

- example applications: sentiment classification, spam detection, language identification, authorship attribution, assign topic/category to a document, part of speech tagging, entity detection
    - period disambiguation, word tokenization
    - emotion detection, word sense disambiguation, semantic role labelling
- generative v/s discriminative classifiers
    - discriminative tend to perform better and are more commonly used
    - TODO: in which strictly classification scenarios would generative classifiers be better? Why not always use discriminative classifiers?
- naive bayes
    - P(c | doc) = P(doc | c) * P(c) / P(doc)
    - P(doc | c) = P(w_1, w_2, ..., w_n | c) = P(w_1 | c) * P(w_2 | c) * ... * P(w_n | c)
        - P(c) = count(c) / count(all docs)
        - P(w | c) = count(w, c) / count(c)
        - smooth probabilities of 0 probabilities in P(w | c)
    - tips and tricks
        - Binary Multinomial Naive Bayes: representing doc as simply a set of distinct words may work better than keep track of counts
        - drop unknown words in test set
        - removing stop words usually doesn't help in practice so they are mostly kept
        - deal with negation (especially in sentiment analysis)
            - didn't like this movie , but I -> didn't NOT_like NOT_this NOT_movie , but I
                - works well in practice
        - lexicon features: use of sentiment lexicons
            - eg: counts of positive / negative words. Use predetermined lists of positive / negative words
    - types of features
        - word counts, word presence
        - lexicon features (eg: lists of positive / negative words)
        - non-textual features (eg: HTML text to image ratio in spam detection)
        - spam detection
            - Contains phrases of urgency like “urgent reply”
            - All capital letters words in subject
            - Claims you can be removed from the list
        - character / sub-word level features
            - language identification
            - eg: classify if a word is name or not. Classify if a word is a location, pokemon, etc.
    - interpretted as a language model: P(doc) = sum_c P(doc | c) * P(c)
- classification metrics
    - accuracy, precision, recall, F-alpha score
    - harmonic mean is used because the harmonic mean of two values is closer to the minimum of the two values than the arithmetic mean is. Thus it weighs the lower of the two numbers more heavily, which is more conservative approach.
    - multiclass classification
        - macroaveraging
            - obtain binary confusion matrix for each class separately
            - compute metric (precision, recall, f1score, etc.) for each class separately
            - take average of the metric to obtain the final metric
        - microaveraging
            - obtain binary confusion matrix for each class separately
            - sum the confusion matrices
            - compute metric (precision, recall, f1score, etc.) for the summed confusion matrix
            - dilutes / hides performance on low sample count classes
- hypothesis test to compare classifier model performance (TODO: add more rigous to this than the book does)
    - why do we need it? Test set might be too small to infer if classifier A is better than B
    - Effect size, Expected_delta(X) = M(A, X) - M(B, X)
        - X is a random variable representing the test set 
        - Random variable, M(A, X) is espected performance of system A on test set X
            - it can be anything: accuracy, precision, recall, f1score, etc.
        - x = our test set, delta(x) is observed effect size
    - null hypothesis H0, Expected_delta(X) <= 0  // would like to reject this. Intuition: easier to reject it if observed effect size is large
        - P(delta(X) >= delta(x) | H0 is true) = p-value
        - if p-value is less than alpha (a small number like 0.01), then reject null hypothesis
            - why? because the observed effect size is so large that it is extremely unlikely to have occured if the null hypothesis was true
        - if p-value is greater than alpha, then fail to reject null hypothesis
            - can't conclude anything in this case
            - null hypothesis may be true or not true
            - there simply isn't enough evidence to say anything
            - we could've observe this effect size by random chance
    - which hypothesis test to use?
        - in NLP, we usually avoid parametric tests like t-tests, ANOVAs, etc. as they usually make assumptions about the data distribution which may not be true
        - non-parametric tests are more common
            - artifically create versions of test set to estimate p-value
            - approximate randomization (TODO: read)
            - bootstrap test
                - bootstrapping: repeatedly drawing large numbers of samples with replacement (called bootstrap samples) from an original set
                - only assumption: original sample is representative of the population
                - original test set, x
                - B(x) = {x_1, x_2, ..., x_B} is the bootstrap sample. x_i is a sample drawn with replacement from x. |x_i| = |x|
                - expected delta(x_i) = delta(x)
                - p-value = probability that observed delta on booststrap samples is greater than (oberserved delta on original test set) + (expected delta on bootstrap samples)
                    = |{i: delta(x_i) >= delta(x) + delta(x)}| / B
                - example: B = 10,000 so if p-value <= 0.01, then we can reject null hypothesis
- logistic regression
    - discriminative model. Directly model P(c|d) instead of modelling P(d|c) and P(c) separately
    - P(y=1 | x) = sigmoid(w^T f(x) + b) = 1 - P(y=0 | x)
    - features
        - problem: classify is a period is a sentence terminator or not
            - Is the word before the period a capitalized or not?
            - brigram template: create a new feature for each bigram apearing before a period
        - document as set / bag of words / ngrams (sklearn.feature_extraction.text)
            - tf-idf
            - vector similarity
        - stop words 
        - remove very rare / common words
        - stemming / lemmatization (make features denser)
        - part of speech tags
        - various parts of scores of naive bayes classifier
        - lexicons (or other features) built from external available datassources of matching domain
            - https://www.nltk.org/api/nltk.corpus.html (switchboard, twitter, etc.) TODO: investigate different types of available data and their potential uses in building ML systems
    - need to standard scale / normalize (in 0 to 1) features before running logistic regression
    - can handle correlated features (unlike naive bayes)
        - but naive bayes can work well on small datasets or short documents
        - naive bayes is easy to implement and faster to train
    - multiclass logistic regression
        - one-vs-all: train a binary classifier for each class
        - softmax: generalize logistic regression to multiple classes
            - P(y=i | x) = exp(w_i^T f(x) + b_i) / sum_j exp(w_j^T f(x) + b_j)
            - cross entropy loss: -sum_i y_i log(P(y=i | x))
                - convex, so gradient descent will converge to global minimum
    - regularization
        - L1: Lasso
        - L2: Ridge
        - Elastic Net: L1 + L2
        - early stopping
        - dropout
    - using word embeddings
        - tf-idf vector has |V| basis vectors (one for each word in the vocabulary). Document vector is a linear combination of these basis vectors
        - replace these basis vectors with word embeddings to get document embeddings
        - learn which direction in the document embedding space is best for classification
- alternative models for discriminative classifiers
    - random forest
    - gbt
    - svm
        
## Section-4: Vector Representations

- distributional hypothesis
    - words that appear in similar contexts have similar meanings
- representation learning
    - self-supervised methods (eg: predict the next word in a sentence, context words given a word, word given the context words)
    - compared to painfully hand-crafted features
- aspects of word meanings
    - similarity (cat and dog), synonyms, antonyms (hot and cold), positive and negative (happy and sad), hypernyms (cat and animal), meronyms (finger and hand), etc.
    - understanding of world (eg: buy, sell, pay -> If I buy something from you, you’ve probably sold it to me, and I likely paid you)
    - word sense ("cat and mouse" v/s "keyboard and mouse")
    - lemma-wordforms (mouse: mice, run: ran, sing: sung, sang)
    - principle of contrast: difference in spelling -> difference in meaning (eg: H2O in scientific context v/s water in everyday context)
    - relatedness / association (eg: coffee and cup, scalpel and surgeon)
        - semantic field
            - hospitals (surgeon, scalpel, nurse, anesthetic, hospital), restaurants (waiter, menu, plate, food, chef), or houses (door, roof, topic models kitchen, family, bed)
            - Topic models: Latent Dirichlet Allocation (LDA), Non-negative Matrix Factorization (NMF)
        - semantic frame: (in auction, buyer buys goods from seller by paying money)
    - connotation: related to a writer or reader’s emotions, sentiment, opinions, or evaluations
- term-document matrix
    - Count[w][d] = number of times word w appears in document d
    - encode document meaning as a column vector of this matrix
        - use cosine similarity to find similar documents
    - encode word meaning as a row vector of this matrix
        - use cosine similarity to find similar words
    - problems: high dimensionality, sparsity, doesn't capture word similarity 
- term-term matrix
    - Count[w1][w2] = number of times word w1 appears in same context as word w2
    - context can be documents, sentences, paragraphs, etc.
- tf-idf
    - trade-off: 
        - high frequency words in context are important to encode meaning
        - uninformative ubiquitous words like "the", "a", etc. are not important but appear frequently
    - approach-1: take log of term frequency
    - approach-2: normalize term frequency by document frequency
        - tf-idf(w, d) = tf(w, d) * idf(w)
        - idf(w) = log(N / df(w))
            - N is number of documents
            - df(w) is number of documents containing word w
- pointwise mutual information (PMI)
    - PMI(w1, w2) = log(P(w1, w2) / (P(w1) * P(w2)))
    - Negative PMI tends to be less reliable
        - PPMI(w1, w2) = max(PMI(w1, w2), 0)
    - problem: PMI is high for rare words that co-occur with other rare words
        - solution: PMI_alpha(w1, w2) = log(P(w1, w2) / (P(w1) * P_alpha(w2)))
            - P_alpha(w2) = count(w2) ^ alpha / Sum_w count(w) ^ alpha
            - alpha = 0.75 is a common choice. Increases probability of rare words, hence reducing PMI for rare words
    - document vector = centroid of word vectors (same as mean)
        - use cosine similarity to find similar documents (information retrieval, news recommendation, plagiarism detection, etc.)
- embeddings
    - dense vectors (unlike sparse vectors in term-document matrix)
        - work better than sparse vectors in practice in all tasks
            - requires fewers parameters to learn
            - captures similarity better 
    - svd based methods [5]
        - word-document matrix
        - window based co-occurrence matrix
        - both these embeddings encode semantic and syntactic information
        - problems:
            - matrix is very sparse and high dimensional
            - quadratic cost of SVD makes it infeasible for large datasets
            - requires hacks to account for term frequency imbalances
            - matrix dimensions changes often
        - iteration based methods solve these issues more elegantly
    - word2vec
        - static (instead of dynamic and contextual like in BERT)
        - skip-gram with negative sampling (SGNS)
            - (self-supervision) given a word, w, learn a classifier to distinguish between context words and noise words
                - neural language model: predict next word given previous words in huge text
            - context word pairs -> positive sample, random word pairs -> negative sample
            - train logistic regression classifier and use weights as word embeddings
            - P(+|w, c) = sigmoid(context_emb(w)^T target_emb(c))
                - NOTE: two types of embeddings per word: as a context word and as a target word
            - negative sampling details
                - sample k negative words for each positive word
                    - k=5-20 for small datasets, k=2-5 for large datasets
                    - high k ensures that each words appears as a negative sample often. Else their target embeddings will be garbage
                - sample negative words from weighted unigram, P_alpha(w2) = count(w2) ^ alpha / Sum_w count(w) ^ alpha
                    - alpha = 0.75. Increases probability of rare words, or else we'll get a lot of "the", "a", etc. as negative samples
            - Loss = -log(P(+|w, c)) - sum_{i=1}^k log(P(-|w, n_i))
                = -log(sigmoid(context_emb(w)^T target_emb(c))) - sum_{i=1}^k log(sigmoid(-context_emb(w)^T target_emb(n_i)))
                    - move context_emb(w) and target_emb(c) closer, and context_emb(w) and target_emb(n_i) farther
                - gradients
                    - dL/dtarget_emb(c) = context_emb(w) * (-1) * sigmoid' / sigmoid = (sigmoid(context_emb(w)^T target_emb(c)) - 1) * context_emb(w)
                        - the more different context_emb(w) and target_emb(c) are, the more the gradient will be
                    - dL/dtarget_emb(n_i) = sigmoid(context_emb(w)^T target_emb(n_i)) * context_emb(w)
                        - the more same context_emb(w) and target_emb(n_i) are, the more the gradient will be
                    - dL/dcontext_emb(w) = (sigmoid(context_emb(w)^T target_emb(c)) - 1) * target_emb(c) + sum_{i=1}^k sigmoid(context_emb(w)^T target_emb(n_i)) * target_emb(n_i)
                        - with high k, we care more about the negative samples than the positive samples
                    - gradient saturation is a non-issue
            - start context and target embeddings randomly and apply gradient descent
            - final word embedding
                - context_emb(w) + target_emb(w)
                - OR, throw away context embeddings and use target embeddings as word embeddings
                - [How to combine context and target embeddings](https://datascience.stackexchange.com/questions/78055/word2vec-usefulness-of-context-vectors-in-classification)
                    - context^T(w1) . target(w2) -> this is a useful metric of similar that we ultimately want
                    - but. Can't keep two copies of embeddings for each word
                    - context and target embeddings are in same semantic space so can keep only one to approximate context^T(w1) . target(w2) with context^T(w1) . context(w2)
                    - average is also acceptable (and may even give a bit boost) but is again not as ideal as keeping both
            - PROBLEMs 
                - no good way to deal with unknown words
                - sparsity due to rich morphology (eg: run, ran, running, etc.)
                - each training run may produce very different embeddings. Best to run multiple times onf bootstrap samples and average the embeddings
            - can be seen as implicitly optimizing a shifted version of PPMI. TODO: what?
    - continuous bag of words (cbow) 
        - predict the target word given the context words
        - [Helpful intuition from this answer](https://ai.stackexchange.com/questions/18634/what-are-the-main-differences-between-skip-gram-and-continuous-bag-of-words)
            - CBOW task is much simpler than the skip-gram task
                - [original paper](https://arxiv.org/pdf/1301.3781) says CBOW took hours to converge while skip-gram took days
                - CBOW is better at
                    - syntactically equivalent words (eg: United States and USA)
                - Skip-Gram is better at
                    - semantically related words that are not similar (eg: coffee and cup)
                    - less prone to overfitting frequent words. CBOW assigns high scores to frequent words
        - TODO: read more
    - fasttext ([Reference 1](https://amitness.com/posts/fasttext-embeddings)) ([Main Paper](https://arxiv.org/pdf/1607.04606))
        - deals with OOV words, sparsity due to rich morphology
            - allows for sharing embeddings across words, so helps with OOV words
        - treat a word as a bag of character n-grams
            - eg: "where" -> <wh, whe, her, ere, re>
            - include all n-grams from 3 to 6 characters
            - also include the full word as a special n-gram too
        - number of n-grams can explode
            - hashing: map n-grams to a fixed number of buckets (eg: 2 million used in the paper)
        - uses skip-gram with negative sampling (SGNS) for training (TODO: why not cbow?)
            - replace, context_emb(w)^T target_emb(c) -> subword_emb(w)^T target_emb(c)
                - subword_emb(w) = sum of embeddings of all subword hashbuckets as described above
                - subword information is not applied on target words (presumably due to efficiency reasons. Target vectors are probably thrown away. TODO: confirm this)
        - experimental insights
            - does better than word2vec on these
                - syntactic tasks. cat:cats::dog:?
                - morphologically rich languages (eg: Czech. German)
                - does something meaningful for OOV words
            - does worse than word2vec on these
                - semantic tasks. man:king::woman:?
                - 1.5 times slowers than regular skip-gram
        - TODO: understand fast cpu based implementations of these word embeddings used in these papers
    - glove TODO
    - Semantic properties
        - short context window -> syntactic relationships (same part of speech, similar syntactic role, etc.)
            - window +- 2: Hogwarts -> Sunnydale, Evernight
        - long context window -> topical relationships (but not similar in meaning)
            - window +- 5: Hogwarts -> Dumbledore, Malfoy, Half-Blood
        - first order co-occurrence: words that appear in contexts of each other
        - second order co-occurrence: words that have similar contextual neighbors
        - directions in embedding space may encode semantic properties like
            - male-female: king-man+woman = queen
            - country-capital: paris-france+germany = berlin
            - comparative-superlative: good-better+bad = worse
    - evaluation
        - intrinsic
            - hand annotated datasets of semantic similarity of words (eg: TOEFL dataset)
            - hand annotated analogy task samples
    - Hierarchical softmax ([word2vec paper](https://arxiv.org/pdf/1310.4546) [Reference blog 1](https://talbaumel.github.io/blog/softmax/) [Reference blog 2](https://leimao.github.io/article/Hierarchical-Softmax/))
        - TODO: understand this. Not easy to understand. 
            - Seems like the tree structure used affects the final accuracy. Bad tree structure can lead to bad accuracy
            - also hacks like subsampling frequent words, negative sampling, etc. are used to make model more accurate and faster
    - [Using word embeddings for text classification](https://arxiv.org/pdf/1607.01759)
        - BoW + Linear classifier (logistic regression, svm, etc.) is a good baseline, but
            - don't share features between multiple classes -> problem for low sample count classes
        - Architecture
            - average pool token embeddings
                - use fastext subword tokens + n-grams (for contextual information) + hashing trick (to reduce vocab size)
            - train k (or log(k)) logistic regression classifiers for k classes
                - use hierarchical softmax for handling large number of classes
                - TODO: not clear how hierarchical softmax works

## Section-5: Neural Networks in NLP and Neural Language Models

- neural networks
    - key advantage: automatically learn useful representations through back propagation. So no need for feature engineering
    - need non-linearity and depth to learn complex non-linear functions
        - eg: xor function doesn't have linear decision boundary
    - activation functions
        - sigmoid, tanh, relu, leaky relu, elu, selu, softmax
            - sigmoid is not used often in deep networks due to vanishing gradient problem
            - tanh(x) = 2 * sigmoid(2x) - 1 is very similar to sigmoid and is almost always better
                - it too has vanishing gradient problem though
            - relu is most commonly used
    - multi-layer perceptron
        - hidden layer, hidden units, fully connected
            - h = activation(Wx + b)
        - output layer
            - y = softmax(Wx + b)
        - 2-layer network example
            - h = g(Wx + b)  // don't count input layer in numbering
            - z = Uh  // layer-1
            - y = softmax(z)  // layer-2
- Neural Networks for Classification
    - 2-layer newtrok with hand crafted features
        - x in R_n (n features) are hand-crafted features
        - h = g(Wx + b)
        - z = Uh  // like k logistic regression classifiers for k classes
        - y = softmax(z)
    - represent words as embeddings
        - 2-layer pooling based network
            - x = pooling(enb(x_1), emb(x_2), ..., emb(x_n))
                - pooling can be average, sum, max, etc.
            - h = g(Wx + b)
            - z = Uh
            - y = softmax(z)
            - embeddings can be learned from scratch or pretrained
                - pretraining (TODO: try different approaches on sample tasks sentiment analysis and product category classification and see which works best)
                    - initialize embeddings with word2vec, fasttext, glove, etc. and fine-tune for the task
                    - make embeddings static (don't update them during training)
                        - once the logistic regression classifier is trained, make the embeddings tunable and train the whole network again
    - cross entropy loss
        - Binary cross entropy loss, L(y_pred, y_true) = -(y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred)) = negative log likelihood
    - backpropagation
        - vanishing gradient problem on 2-layer networks
            - h = g(Wx + b), z = Uh, y_pred = softmax(z)
            - dL / dW = dL / dy_pred * dy_pred / dz * dz / dh * dh / dW
                - say Wx+b is very high and g is sigmoid. Then dh/dW vanishes and hence dL/dW vanishes
                - dh / dW = sigmoid'(Wx+b) * x = sigmoid(Wx+b) * (1 - sigmoid(Wx+b)) * x
                    - if Wx+b is very high/very low, either sigmoid(Wx+b) or (1 - sigmoid(Wx+b)) will be close to 0
    - optimization tricks
        - initialize with small random weights
        - normalize input features to have mean 0 and variance 1
        - regularization techniques
            - drop out (randomly set some activations to 0 during training)
            - L1, L2 regularization
            - early stopping
            - batch normalization ([Reference blog 1](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338))
                - it makes training faster and stable, allows for wide range of learning rates, no final accuracy drop, etc.
                - almost always makes deep networks better
                    - sigmoid based activations got same performance as relu based activations in an experiment with batch normalization
                - TODO: understand, implement, and study this on a deep architecture (eg: image net, mnist, etc.) and develop an intuition for why this works
                    - understand hypothesis for why it works and some ways to test them
                        - BN reduces internal covariate shift
                        - BN mitigates interdependency between hidden layers during training
                        - BN makes the optimization landscape smoother
        - hyperparameter tuning
            - learning rate, batch size, number of hidden units, number of hidden layers, etc.
            - grid search, random search, bayesian optimization
- Neural Language Models (TODO: train neural language models on toy datasets and compare with ngram language models)
    - Many advantages over ngram language models
        - can capture long range dependencies
        - can capture semantic similarity, syntactic similarity, etc. (all the things word embeddings can capture)
        - can generalize to unseen contexts:
            - training has: I have to make sure that the cat gets fed
            - test has: I forgot to make sure that the dog gets _
                - bigram dog gets has 0 probability in training data but cat and dog have similar word embeddings
    - Much more complex than ngram language models. If ngram works for the problem then use ngram instead
    - architecture:
        - task is to model: P(w_i | w_{i-1}, w_{i-2}, ..., w_{i-k}) (same as ngram language model)
        - example for k=3
            - convert w1, w2, w3 -> e1, e2, e3 (embeddings)
            - h = g(W1 e1 + W2 e2 + W3 e3 + b)
            - z = Uh
            - y = softmax(z)
    - training
        - self-supervision
            - embeddings can be learned from scratch or intialized (and frozen) pretrained embeddings can be used
        - input a very long text, concatenating all the sentences, starting with random weights, and then iteratively moving through the text predicting each word w

## Section-6: Sequence Labeling for Parts of Speech and Named Entities

- sequence labeling
    - assign a label to each word in a sequence
    - eg: POS tagging, NER, trigger word detection, etc.
    - generative models: HMM,
    - Discriminative models: CRF
    - neural models: Transformer, RNN, LSTM, etc.
- part-of-speech tagging (POS)
    - survived 2000 years without much change so must be useful
    - noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection
        - noun: person, place, thing. Eg: cat, dog, table, usa, etc.
            - common noun: cat, mango, algorithm, pacing, etc.
            - count nount (goat/goats, friend/friends, etc.), mass noun (water, air, etc.)
            - proper noun: names of specific people, places, etc. Eg: John, New York, etc.
        - verb: action. Eg: run, jump, eat, etc.
            - phrasal verb: verb + particle. Eg: run into, run over, etc.
                - particle: preposition or adverb that follows the verb: run into, run over, etc.
            - auxiliary verb: mark semantic features of a main verb such as its tense. Eg: is, are, was, were, etc.
                - modal: marks mood or attitude. Eg: can, could, may, might, must, shall, should, will, would, etc.
                - copula: links the subject of a sentence to a subject complement. Eg: is, are, was, were, etc.
        - adjective: describes a noun. Eg: big, small, red, blue, etc.
        - adverb: describes a verb. Eg: quickly, slowly, etc.
            - very loosely defined
            - directional / locative (home, here, downhill), degree (very, somewhat, etc.), manner (quickly, slowly, etc.), frequency (always, never, etc.), time (now, yesterday, etc.), etc.
        - pronoun: replaces a noun for efficiency. Eg: he, she, it, etc.
            - personal: I, you, he, she, it, we, they, etc.
            - possessive: mine, yours, his, hers, its, ours, theirs, etc.
            - wh: who, whom, whose, which, what, etc.
        - preposition: shows relationship between a noun and another word. Eg: in, on, at, etc.
            - spatial (in, on, at, etc.), temporal (before, after, etc.), directional (to, from, etc.), agency (by, with, etc.), etc.
        - conjunction: joins words, phrases, or clauses. Eg: and, but, or, etc.
            - coordinating (and, but, or, etc.): joins words, phrases, or clauses of equal importance
            - subordinating (because, although, etc.): joins a dependent clause to an independent clause
                - complementizer: that, if, etc.
        - interjection: expresses emotion. Eg: wow, ouch, oh, umm, etc.
            - small open class
            - greetings (hello, hi, etc.), exclamations (wow, ouch, etc.), question responses (yes, no, uh-huh), etc.
        - determiner: introduces a noun. Eg: a, an, the, this, that, etc.
    - gives information about the word and its context
        - likely neighbors: adjective before a noun, verb after a pronoun, etc.
        - syntactic role: subject, object, etc.
    - disambiguation task
        - words are ambiguous: "mouse" can be a computer mouse or a rodent
    - Very easy problem
        - most algorithms (HMM, CRF, BERT) perform at human level accuracy (97-98%) in English
        - most word types (~85%) are unambiguous (eg: "Janet" is a proper noun)
        - ambiguous word types (~15%) account for 50-60% words
            - most are easy to disambiguate from context (eg: "I saw a mouse" v/s "Let me fetch my saw")
        - simple bsaeline: assign most frequent POS tag to each word
            - has 92% accuracy
- open vs closed class words
    - open class words: nouns, verbs, adjectives, adverbs, etc.
    - closed class words: pronouns, prepositions, conjunctions, interjections, etc.
        - short, frequent, function words
- Named entity recognition (NER)
    - person, organization, location, date, time, money, percent, facility, geo-political entity, etc.
    - eg: "I am going to New York tomorrow" -> "I am going to [LOCATION] tomorrow"
    - useful for question answering, information extraction, stance detection, etc.
        - sentiment analysis (w.r.t. a particular entity)
        - extract events, relationships, etc.
        - link unstructured text to structured data like knowledge bases, wikipedia, etc.
    - tag spans of text with: PER (person), ORG (organization), LOC (location), GPE (geo-political entity)
        - decide where word boundaries are
        - ambiguity ('JFK' can be a person or a locstion / airport)
        - BIO (Begin, Inside, Outside) tagging
            - IO (Inside, Outside), BIOES (Begin, Inside, Outside, End, Single)
- Hidden Markov Model (HMM)
    - generative model
    - components
        - markov chain for hidden states (POS tags, NER tags, etc.)
            - state transition graph
            - initial state probabilities
        - observation likelihoods / emission probabilities for each state (word given state)
        - assumptions:
            - first order markov assumption: P(s_i | s_{i-1}, s_{i-2}, ..., s_1) = P(s_i | s_{i-1}) 
            - output independence assumption: P(w_i | s_i to s_1, w_{i-1}, w_{i-2}, ..., w_1) = P(w_i | s_i)
    - parameter estimation
        - MLE: count based
            - P(w_i | s_i) = count(w_i, s_i) / count(s_i)
    - decoding
        - given a sequence of observed outputs (words), find the most likely sequence of hidden states (POS tags)
            - argmax s P(s | w) = argmax s P(w | s) P(s) 
                = argmax s P(w_1 | s_1) P(s_1) P(w_2 | s_2) P(s_2 | s_1) ... P(w_n | s_n) P(s_n | s_{n-1})
            - Viterbi algorithm: maintain DP[t][j] = max P(s_1, s_2, ..., s_t = j, w_1, w_2, ..., w_t)
    - bunch of problems
        - unknown words (eg: novel unseen nouns with undefined probabilities)
- Conditional Random Fields (CRF)
    - discriminative model
    - linear chain CRF 
        - estimate P(s | w) directly
        - P(s | w) = exp(sum_k w_k F_k(s, w)) / Z(w)
            - F_k(s, w) is a global feature function
            - Z(w) is a normalization constant
        - linear chain CRF
            - F_k(s, w) = sum_i f_k(s_i, s_{i-1}, w, i)
                - f_k(s_i, s_{i-1}, w, i) is a local feature function
                - eg: f_k(s_i, s_{i-1}, w, i) = 1 if s_i = noun and s_{i-1} = verb and w_i = "run" else 0
                - feature templates:
                    - pick commonly occuring combos of: < y_i, x_i >, < y_i, y_{i-1} >, < y_i, x_i, x_{i-1} >, etc.
                - word shape features
                    - capitalization, digit, hyphen, etc.
                    - X.X.X., Xx, Xd-d, XXdd-dd, etc.
                    - suffix like -ly, -ing, -ed, etc.
                    - prefix like un-, re-, etc.
                    - short word, long word, etc.
                - especially important for NER: gazetteer, name-list
            - this allows for using viterbi algorithm for decoding
                - general CRF without this assumption require complex decoding and are rarely used
            - training same as logistic regression
        - inference
            - s* = argmax_s P(s | w) = argmax_s exp(sum_k w_k F_k(s, w)) / Z(w)
                = argmax_s sum_k w_k F_k(s, w)
                = argmax_s sum_k w_k sum_i f_k(s_i, s_{i-1}, w, i)
                = argmax_s sum_i sum_k w_k f_k(s_i, s_{i-1}, w, i)
            - can be solved using viterbi / DP algorithm
- metrics
    - POS tagging: accuracy
    - NER: F1 score

## Section-7: RNNs and LSTMs

- RNN
    - h_t = g(Wx_t + Uh_{t-1} + b)  // kind of like HMM but with continuous hidden states
    - y_t = softmax(Vh_t + c)
    - backpropagation through time (BPTT)
        - forward pass: compute h_t, y_t, loss
        - backward pass: compute gradients, update weights
    - language modelling
        - e_t = E w_t  // d_u x 1 = d_u x V * V x 1 
        - h_t = g(W e_t + U h_{t-1} + b)  // d_h x 1 = d_h x d_u * d_u x 1 + d_h x d_h * d_h x 1 + d_h x 1 
        - y_t = softmax(V h_t + c)  // V x 1 = V x d_h + V x 1
        - P(w_t+1 | w_{t}, w_{t-1}, ..., w_1) = softmax(y_t)
        - cross entropy loss: -sum_i log(P(w_i = observed_w i | w_{i-1}, w_{i-2}, ..., w_1))
        - training
            - self-supervision: predict next word given previous words
            - teacher forcing: use true previous words to predict next word
                - in h_i = g(W w_i + U h_{i-1} + b), use actual w_{i-1} instead of predicted w_{i-1} from previous step
                - it is done to stabilize training
                - TODO: won't this affect text generation as the errors will accumulate? Read more on why this is done.
            - Weight tying
                - make the input embeddings and emitter / output embeddings matrix same since the inputs and outputs are in the same space only
                    - e_t = E w_t
                    - h_t = g(W e_t + U h_{t-1} + b)
                    - y_t = softmax(E h_t + c)
                - reduces number of parameters significantly
                - boosts model performance
    - sequence labelling
        - final softmax is over target labels. No teacher forcing and no weight tying
    - sequence classification
        - remove intermediate output tokens and keep a feed forward network and softmax on last hidden layer output only
            - e_t = E w_t  // d_u x 1 = d_u x V * V x 1 
            - h_t = g(W e_t + U h_{t-1} + b)  // d_h x 1 = d_h x d_u * d_u x 1 + d_h x d_h * d_h x 1 + d_h x 1 
            - y = softmax(V h_T + c)
            - instead of h_T, use some sort of pooling (average, element wise max, etc.) of h_1 to h_T
                - TODO: when to use which?
    - text generation
        - initialize h_1
            - either use last hidden state from some contextual text (eg: text summarization task)
            - or start from the start token
        - forward propagation and sample on each step. Feed output from previous step into generation for next step
    - stacked RNN
        - feed output of 1 RNN as input to another
        - usually performs better than single layer RNN
    - bidirectionary RNN
        - applicable when we have access to the entire sequence right away. eg: POS tagging, NER, classification, etc.
        - combine two disconnected rnn: one with inputs left to right and another with inputs right to left
            - different ways to combine: concatenate, sum, mean, etc.
        - shown to work well for sequence classification tasks
            - problem (in unidirectional sequence classification): final hidden state is bias towards the end of the sequence
                - bidirectional RNN solves it by combining final hidden state of forward and backward RNNs
- LSTM
    - vanilla RNNs tend to not capture long range dependencies well
        - difficult task because hidden state does two task simultaneously: predict current word and remember past words
        - vanishing gradient problem
            - multiple multiplications make gradient contribution for far away words very small
        - TODO: find papers that analyze this
    - architecture
        - decouple output emitting hidden state and context vector
            - maintain context vector
            - use context vector to obtain hidden vector, which is used to output
        - in each step
            - build current context from previous context
                - first forget some information from previous context using forget gate
                - then add some new information based on the current input using add gate
                - combine the two to obtain the current context
            - use output gate to select information from current context
            - emit output based on the hidden state
        - Forget gate, f_t = sigmoid(U_f h_{t-1} + W_f x_t)   // why use h_{t-1} and not c_{t-1}?
            - p_t = c_{t-1} . f_t  // preserved context. "." is element wise multiplication
        - Add gate, a_t = sigmoid(U_a h_{t-1} + W_a x_t) 
            - e_t = tanh(U_e h_{t-1} + W_e x_t)  // new context
            - ae_t = a_t . e_t    // new information to add to context. "." is element wise multiplication
        - Current context, c_t = p_t + ae_t
        - Output gate, o_t = sigmoid(U_o h_{t-1} + W_o x_t)
            - h_t = o_t . tanh(c_t)  // hidden state
        - y_t = softmax(V h_t + b)  // output
    - all principles of RNNs are applicable as-is to LSTM because the changes are within a cell only
        - LSTM tends to perform better than vanilla RNNs in practice and is default choice for RNN-based architectures
- Encoder-Decoder architecture
    - used for seq2seq tasks like machine translation, summarization, question answering, dialog, etc.
        - output sequence don't have 1:1 correspondence with input sequence and can be longer / shorter
    - encoder: RNN / LSTM
        - input sequence -> context vector (last hidden state)
    - decoder: RNN / LSTM
        - context vector -> output sequence 
            - context vector is concatenated with the input at each step to prevent it from diluting due to vanishing gradient problem
        - decoder works exactly like a RNN language model (teacher forcing, weight tying, etc.)
- Attention mechanism
    - problem with encoder-decoder architecture
        - encoder context vector is a bottleneck and has to capture all information from the input sequence
        - this is difficult for long sequences
    - replace static context vector with dynamic context vector 
        - weighted average of encoder hidden states
        - different weighted average for each output token in decoder
    - at step i in decoder
        - score(hd_{i-1}, he_j) = hd_{i-1} . he_j
            - option-2: score(hd_{i-1}, he_j) = hd_{i-1} . W_a . he_j  // this allows for different dimensions for hidden states in encoder and decoder
        - attention = softmax(score) // over all j
        - context = sum_j attention_j * he_j

## Section-8: Transformers and Large Language Models

- people acquire lots of knowledge very rapidly as they age but use only a small fraction of it in their daily lives
    - "How much do we know at any time? Much more, or so I believe, than we know we know." -Agatha Christie, The Moving Finger
    - Vocab size of an american english speaking 20 year old is ~50,000 words (50000 / (365 * 20) = 6.8 words per day)
        - but use ~2000 words actively in daily life
    - students learn a lot of information in school, university, etc. that is not actively used in daily life
    - this observation is one of the motivation of "distributional hypothesis"
        - learn enough so that it becomes possible to know about a word from its context, without any real world knowledge of the word
    - LLMs
        - uses self-attention (derived from attention mechanism) 
            - build contextual representations of a word’s meaning that integrate information from surrounding words [6]
        - shows remarkable performance on a wide range of NLP tasks (because of knowledge in pretraining)
            - especially in text generation tasks: summarization, translation, question answering, chatbots
            - almost any NLP task can be modeled as next word prediction: text classification (sentiemnt analysis, spam detection, etc.), sequence labelling (POS tagging, NER, etc.), etc.
                - Sentiment classification: The sentiment of the sentence "I like Jackie Chan" is: ____
                - Question answering: Q: Who wrote the book ‘‘The Origin of Species"? A: ____
- Transformers
    - intuition
        - sequence of contextual embeddings representing contextual meaning for each word in the input sequence (TODO: try to observe this in an actual pretrained model. Does an something like the animation of word embedding transformation from [6] actually happen?)
            - I walked along the *pond*, and noticed that one of the trees along the *bank* had fallen into the *water* after the storm.
                - update contextual embeddings for bank to be more similar to pond and water than to money based on contextual words "pond" and "water"
            - The *keys* to the cabinet *are* on the table
                - words like *are* are quite ambiguous without context so these should change more than nouns like USA (TODO: verify this on real data)
        - causal or backward looking or autoregressive self-attention
            - GPT is unidirectional like this, but BERT is bidirectional
            - computation for each step is independent of the other steps (within a layer)
                - a layer can be parallely computed (unlike RNNs)
    - causal self-attention layer
        - input: x_1, x_2, ..., x_n, output: a_1, a_2, ..., a_n  
            - a_i is attention weighted sum of all x_j's that come before x_i (j<=i)
        - version-1:
            - score(x_i, x_j) = x_i . x_j  // TODO: why is x_i the key? x_i is simply the previous word. Why is it so privileged that we're deciding attention based on it only?
            - alpha_ij = softmax(score(x_i, x_j)), for all j<=i
            - a_i = sum_j alpha_ij * x_j, for all j<=i
        - version-2:
            - break down the roles played by x_i, x_j (just like how LSTM separates forget, add, and output gates)
                - query: in version-1, x_i acts as a query when computing alpha_ij = softmax(score(x_i, x_j)), for all j<=i
                    - q_i = W_q x_i
                - key: in version-1, x_j acts as a key, that is compared against a query, when computing alpha_ij = softmax(score(x_i, x_j)), for all j<=i
                    - k_j = W_k x_j
                - value: in version-1, x_j acts as a value, that is weighted by the attention score, when computing a_i = sum_j alpha_ij * x_j
                    - v_j = W_v x_j
                - instead of using input vectors as it is for all three roles, use different linear transformations of it for different roles
                    - TODO: eigen values and eigen vectors of W_q, W_k, W_v define which dimensions in the input vector embedding space are important for query, key, and value roles respectively. Study this in an actual model
                        - in logistic regession, weight vector is kind of a filter that identifies the most important dimensions in the input vector space
                            - this is similar to filter like role played by a convolutional kernel in a CNN
                        - a fully connected layer, h = Wx, is a linear transformation from input space to output space
                            - sub-space spanned by small eigen values of W is squished into non-exitence and is not utilized in the output space
                            - sub-space spanned by large eigen values of W is stretched and is utilized more in the output space
                            - study eigen values of learned W to understand what the model is doing. Are there only a few large eigen values? Are there many small eigen values?
                                - most activations act as a gate and only keep big positive values and discards negative values
                                    - this means direction of eigen vectors also matter
                                    - a matrix is like a high-pass conv filter that detects 0/1 OR dot product between input vector and eigen vectors
                                - just like how logistic regression finds the single most predictive direction in the embedding space, a fully connected layer finds weighted set of most predictive directions (like eigenfaces, pca, nmf, etc.)
                            - imagine training a deep fully connected network (without activations) represented by matrices: W1 -> W2 -> W3 -> W4
                                - how is it different from logistic regression?
                                - W1 discards some directions and amplifies some directions
                                    - logistic regression is extreme case of this where only one direction is amplified and rest all are discarded
                                - W2, W3, W4 do the same thing slowly and converge to the direction that logistic regression would have converged to
                                - now say we add activations between W1, W2, W3, W4
                                    - each activation throws away negative half of the direction it keeps and both halves of the direction it throws away
                                - each layer necessarily discards information
                                    - so add residual connections to pass information through
                                - a deep fc network can do this
                                    - discard complex non-linear decision boundaries
                                    - zoom in on particular areas of the input space to make them more high resolution
            - score(x_i, x_j) = q_i . k_j / sqrt(d_k)  // d_k is the dimension of the key vector. Needed because the scale blow-up in high dimensions will skew the softmax
            - alpha_ij = softmax(score(x_i, x_j)), for all j<=i
            - a_i = sum_j alpha_ij * v_j, for all j<=i
        - parallelizing for efficient GPU implementation
            - X is N x d  // N = number of vectors in sequence, d = embedding dimension
            - Q = X W_Q is N x d_k  // W_Q is d x d_k where d_k is key dimension
            - K = X W_K is N x d_k  // W_K is d x d_k 
            - V = X W_V is N x d_v  // W_V is d x d_v where d_v is value dimension
            - M = Q K^T / sqrt(d_k) is N x N  // M_ij = Q_i . K_j / sqrt(d_k) = alpha_ij
            - M = M + INF_UPPER_TRIANGULAR  // set upper triangular to -inf to prevent attending to future words
            - A = SelfAttention(Q, K, V) = softmax(M) V is N x d_v  // A_i = sum_j alpha_ij * V_j
        - multi-head attention
            - like multiple CNN filters
            - head_i = SelfAttention(Q_i, K_i, V_i) 
            - A = MultiHeadAttention(X) = Concat(head_1, head_2, ..., head_h) W_o  // W_o is h * d_v x d
                - W_o converts h * d_v to d dimensions to match the input dimension
    - transformer block
        - standard design
            - MultiHeadAttention
            - Residual connection  // simply add the input of a layer to its output to pass information from lower layers to higher layers
            - Layer Normalization
            - Feed Forward Network
                - computed parallelly for each position
                - fully-connected 2-layer network
                - X_i is d x 1 // input to the feed forward network
                - L1_i = activation(W1 X_i + b1) is d_ff x 1 // W1 and b1 are shared across all positions i
                    - d_ff is often higher dimension than d (eg: d = 512, d_ff = 2048)
                    - TODA: why? all this is quite mysterious
                - L2_i = activation(W2 L1_i + b2) is d x 1 // W2 and b2 are shared across all positions i
                - same weights for each position but different weights for each layer
                - TODO: why do this? What is this layer doing?
            - Residual connection
            - Layer Normalization
                - improves training of deep networks
                    - keeps the layer outputs in a range that facilitates gradient-based training
                - x is d x 1 // input to the layer normalization is a vector output at a position
                - standard_scaled(x) = (x - mean(x)) / sqrt(var(x) + epsilon)  // epsilon is a small number to prevent division by zero
                - LayerNorm(x) = standard_scaled(x) * gamma + beta  // gamma and beta are learned parameters
                - TODO: learn more about what this is
            - pre-normalization
                - add another layer normalization before the first multi-head attention
                    - equivalently, add layer normalization before multi-head attention in each layer and add one more layer normalization after the last residual conection after the last feed forward network
        - stack multiple transformer blocks
            - input and output of each block is same dimension for this purpose
            - T5, GPT 3-small -> 12 layers
            - GPT 3-large -> 96 layers
    - residual stream view
        - track a token's path from input to output
            - t_0 = x_i // input token embedding
            - t_1 = x_i + MultiHeadAttention(x_i) // attention output
                - this is the only place where the token interacts with the other (previous) tokens
            - t_2 = LayerNorm(t_1) // layer normalization
            - t_3 = FeedForward(t_2) // feed forward output
            - t_4 = t_2 + t_3 // residual connection
            - t_5 = LayerNorm(t_4) // layer normalization
        - resdiual stream transforms the current token embedding to the next token embedding step-by-step
            - TODO: would be interesting to visualize this on an actual model
    - input embeddings (token + position)
        - embedding matrix, E is V x d  // V = vocab size, d = embedding dimension
        - positional embeddings methods
            - firstly, why we need positional embeddings
                - transformer doesn't have any notion of order of words in the input sequence so it cannot leverage inherent structure in the language that is based on word order
                    - example: because of how english works, attention should bias more toward close context words, instead of far away words, for the most part unless there is a specific reason to do otherwise
                    - without positional embeddings, the model would not even have any input from which to learn this bias
                    - so may be next word prediction is impossible / senseless without providing positional information in the input
            - intuitively, it should not require lots of dimensions. Only log(S) bits are needed where S is the sequence length
            - absolute position (simplest)
                - matrix, P is S x d  // S = max sequence length, d = embedding dimension
                    - P_i is d x 1 embedding for position i. Similar to how E["fish"] is d x 1 embedding for word "fish"
                - input matrix, X is S x d  // S = max sequence length, d = embedding dimension
                    - X_i = E[w_i] + P_i  // E[w_i] is d x 1 embedding for word w_i
                - problem: training data would have more examples of shorter sequences than longer sequences
                - GPT folows this (presumably because they have lots of data for each sequence length)
                    - although P_i is d dimensional, its intrinsic dimension is much lower as not much information is needed to represent a position
                    - [GPT-2 learned positional embeddings that look like helix](https://www.lesswrong.com/posts/qvWP3aBDBaqXvPNhS/gpt-2-s-positional-embedding-matrix-is-a-helix)
                        - first few positions differ from the helix pattern 
                        - TODO: form an hypothesis for why this is. Look at 3d PCA of openly available LLMs. Compare it with the fixed sin-cosine positional embeddings used by original paper. Are they both equivalent?
            - fixed position (sinusoidal) ([Ref blog 1](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/) [Attention paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf))
                - a non-learnable fixed hand crafted vector per position
                - its like binary encoding of the position number except in float using sin-cos of geometrically varying frequencies
                - P_{pos+k} can be obtained from P_k using a linear transformation so the model should be able to learn this matrix
                - original paper found that the final model performance didn't change much with this as compared to absolute position embeddings
    - Language modelling head
        - head: final layer that predicts the next word (or does any other task we want to do like classification)
        - input, h^L_N is 1 x d last output in sequence of the last transformer block 
        - logits, l = h^L_N . E^T  // E^T is V x d is unembedding matrix
            - weight tying. This is same as the input embedding matrix E. So inputs and outputs are in the same space
            - logit lens: tool for debugging / interpreting the model
                - simply applying the unembedding matrix to the output of any layers to see what words it may represents
                - doesn't always work but is a useful tool to understand the model
        - final output, y = softmax(l)
- Large Language Models sampling methods
    - random sampling
        - doesn't work very well because the tail is long and fat
    - main trade-off: quality v/s diversity
        - prioritize quality: weight more likely words more
            - rated by people as more coherent, more accurate, more factual, etc.
            - but are also rated as more boring, more repetitive, less creative, etc.
    - top-k sampling
        - sample from top k most likely words
        - k = 1 is greedy sampling, k = V is random sampling
    - top-p sampling
        - issues with top-k
            - if k is too small and probability mass is flat, then the model will be forced to sample from a very small set of words
            - if k is too large and probability mass is concentrated, then the model may sample from a very large set of words unlikely words
        - instead pick the smallest set of words that have cumulative probability mass > p
    - temperature sampling
        - scale the logits by a temperature parameter
        - higher temperature -> more uniform distribution
            - T > 1 gives flatter distribution than the original distribution
        - lower temperature -> more peaky distribution
            - as temperature approaches 0, it becomes greedy sampling
- LLM Training
    - self-supervision
    - teacher forcing
    - cross entropy loss:
        - input: "so long and thanks for"
        - output: "long and thanks for all"
        - loss: -sum_i log(P(w_i = observed_w i | w_{i-1}, w_{i-2}, ..., w_1))
    - Parallelize: calculation of each sequence index is independent of the other sequence indices
    - usually trained with full context
        - small documents are concatenated, with separators, to form a single long document
        - small contexts are also anyway getting trained due to the autoregressive nature of the model
    - batch size is usually very big (large GPT-3 uses batch size of 3.2 million tokens)
    - training data
        - text scraped from web + carefully curated data
            - common crawl
                - clean and dedup: remove non-natural language like code, offensive word blocklists, etc.
            - wikipedia, books
        - training data likely contains data helpful for many NLP tasks
            - question-answer pairs, dialogues, translations, documents with their summaries
- Scaling laws
    - 3 major factors determine the performance of a model
        - model size
        - data size
        - compute size
    - usually follows power law. Loss scale with 1 / (S^alpha) where S is the size of the model, data, or compute
        - TODO: find good resources on this. Is alpha usually sublinear or superlinear?

## Section-9: Fine-Tuning and Masked Language Models

- main focus of Bidirectional encoders is computing contextual embeddings
    - main focus of autoregressive models is predicting the next word
        - causal models like GPT-2, GPT-3, etc. -> decoder only
        - bidirectional models like BERT -> encoder only
    - contextual embeddings produced by bidirectional masked language models work better than ones produced by causal / left to right language models
        - because they can leverage information from both sides of the word
        - eg: assigning NER tags requires information from both sides of the word
- architecture
    - don't mask out the upper triangular part of the attention matrix
    - original BERT paper specs
        - English-only subword vocab sized 30,000 using wordpeice algorithm
        - 12 transformer layers (with 12 attention heads each)
        - hidden size of 768
    - input length is fixed (512 tokens for BERT)
- training
    - cloze task
        - predict 1 or more masked words in a sentence
        - other variants of recovering from corruption
            - substitute a word with a random word
            - reorder words, delete words, extraneous word insertion
    - Masked Language Modelling in BERT
        - randomly select a word for corruption (15% of words selected in BERT)
            - BERT-like models are very sample inefficient. Only 15% of the data is used for training
        - 3 ways to corrupt it
            - replace with [MASK] token (80% of the time in BERT)
            - replace with a random word (10% of the time in BERT) sampled from token unigram probabilities
            - keep the word as it is (10% of the time in BERT)
    - MLM: loss function
        - cross entropy loss on the corrupted words only
    - Next sentence prediction
        - Tasks relating to a pair of sentences
            - paraphrase detection (detecting if two sentences are paraphrases of each other)
            - entailment detection (detecting if one sentence entails from or contradicts the other)
            - discourse coherence (detecting if two sentences form a coherent discourse)
        - goal: capture knowledge required for these tasks in contextual embeddings
            - input: pair of sentences. eg: [CLS] sentence1 [SEP] sentence2 [SEP]
                - output embedding of [CLS] token is used for classification
            - output: 0 or 1 if sentence2 is the next sentence after sentence1
        - final BERT loss is joint loss of MLM and NSP combined
    - training data
        - original BERT paper used BooksCorpus (800M words) and English Wikipedia (2.5B words)
        - XLM-R used 100 languages and 300 billion tokens from common crawl
        - 512 token inputs
        - some models like RoBERTa drop next sentence prediction task
        - large batch sizes (8K, 32K tokens)
        - what data to use for creating vocab for multiple languages?
            - divide corpus into languages
            - rescale prior probabilities of words in each language like: p_l = c_l / sum_i c_i -> p_l_adjusted = p_l^alpha / sum_i p_i^alpha
            - alpha between 0 and 1 (like 0.3) boosts low data languages
        - pros of multilinguality
            - avoid the need to maintain 100s of monolingual models
            - improve performance on low resource languages by leveraging information from similar languages that have more data
        - curse of multilinguality
            - for large languages, performan on each language degrades
            - multilingual accent: grammatical structures in higherresource languages (often English) bleed into lower-resource languages
- contextual embeddings
    - output of the final transformer layer
        - another option: average the embeddings at that position over last 4 layers
    - static word embeddings (word2vec, fasttext, etc.) -> meaning of word types (eg: bank)
    - contextual embeddings (BERT, GPT, etc.) -> meaning of word instances (eg: "bank" in the sentences "I walked along the bank" and "I deposited money in the bank")
    - word-sense disambiguation
        - polysemy: words have multiple meanings (eg: bank)
        - cluster embeddings of a word the word "die" to see these sentences clusters
            - German article: "Die" is a cluster
            - "a playing die" is another cluster
            - "single person dies" and "multiple people die" is another cluster with the two forms also seperable within it
        - best performing algorithms
            - from supervised labels, (sentence, target word, word sense), obtain embeddings for senses by averaging context embeddings of target words
            - at test time, output nearest sense embedding to the context embedding of the target word in the test sentence
    - word similarity
        - measure similarity between two contextual embeddings of word instances of the same underlying word type
        - anisotropy
            - contextual embeddings of all words tend to extremely similar
            - anisotropy = expected value of cosine similarity between two random words
                - cause of anisotropy: some dimensions have very large magnitude and high variance
                - TODO: what is this? Look into it. Isn't there layer norm right at the end?
            - standard scaling helps with aniostropy by normalizing variances across dimensions and prevent large magnitude dimensions from dominating cosine similarity
- fine tuning
    - always prepend [CLS] token to the input sequence because BERT expects it
    - sequence classification
        - prepend [CLS] token to the input sequence
        - add a linear layer on top of the output of [CLS] token at the beginning of the sequence
            - y = softmax(W o_[CLS] + b)  // o_[CLS] is the output of the [CLS] token
        - [CLS] encodes sentence because of the next sentence prediction task
    - pairwise sequence classification
        - paraphrase detection (are the two sentences paraphrases of each other?), logical entailment (does sentence A logically entail sentence B?), and discourse coherence (how coherent is sentence B as a follow-on to sentence A?)
        - structure input same as NSP: [CLS] sentence1 [SEP] sentence2 [SEP]
        - add a linear layer on top of the output of [CLS] token at the beginning of the sequence
    - sequence labelling
        - prepend [CLS] token to the input sequence 
        - train linear unembedding layer on top of the output of each token
            - y_i = softmax(W o_i + b)  // o_i is the output of the i-th token
            - W is K x d, K is number of classes
        - converting labelled data to BERT input is straightforward
        - converting BERT output back to sequence labelling output like NER is not straightforward
            - for a span tagging like Loc[Mt. Sanitas], keep the output of tokens "Mt" and "San" only. Ignore the output of "##itas" and "."
                - also possible to combine probabilities somehow
- span-based masking
    - most tasks (NER, POS tagging, etc.) are span-based and not single token masks
    - SpanBERT: during pretraining, mask out entire spans of words instead of single words
        - sample span length from a geometric distribution biased towards smaller spans (max = 10)
        - sample regime for masking
            - 80% of the time, mask out the span
            - 10% of the time, replace the span with random words from unigram distribution
            - 10% of the time, keep the words as it is
        - add another component to the loss function that predicts tokens inside the span using context embeddings of the span boundaries
            - loss = MLM loss + span based loss
            - this is added because MLM loss doesn't capture the span-based structure of the data // TODO: this is not very clear. Need to read more resources on this topic
            - L_SBO = sum_i L_SBO(x_i) for i in span x_s, x_{s+1}, ..., x_e
            - L_SBO(x_i) = -log(P(x_i | x_s, x_e, POS_{i-s+1}))   // model should be able to predict specific words inside the span using the context embeddings of the span boundaries
                - POS_{i-s+1} is the frozen relative positional embedding (the sin-cos thing)
                - P(x_i | x_s, x_e, POS_{i-s+1}) = softmax(W_V FFN([x_s; x_e; POS_{i-s+1}]) + b)
                    - FFN is a feed forward network
                    - W_V is a learned weight matrix  // TODO: no weight tying here?
                    - [x_s; x_e; POS_{i-s+1}] is the concatenation of the embeddings
    - fine-tuning 
        - general framework
            - identify all spans
            - classify each span as a label
            - determine final output by combining the predictions of all spans
        - input: (x_1, x_2, ..., x_T)
        - generate all possible spans of length <= L, S(x) from x
        - use contextualized embeddings to represent a span
            - spanRep_ij = [s_i; e_j; g_ij]
                - s_i = FFN_start(z_i)
                - e_j = FFN_end(z_j)
                - g_ij = SelfAttention(z_i, z_j)
            - for NER task, span is classified as the label (PER, LOC, etc.)
                - y_ij = softmax(FFN(spanRep_ij))
        - advantages over BIO sequence labelling
            - BIO is prone to label mismatch problem. One wrong label results in the whole sample being wrong
                - label is assigned at the span level so this problem is mitigated
            - Can also represent nested entities
                - eg: Jan Villanueva of United Airlines Holding discussed
                    - Jan Villanueva is a person
                    - United Airlines Holding is an organization
                    - United Airlines is also an organization
                - BIO can't represent such nested entities but span-based labelling can



# References

- [1] https://web.stanford.edu/~jurafsky/slp3/
- [2] "Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists" by Alice Zheng and Amanda Casari
- [3] "Text Mining with R: A Tidy Approach" by Julia Silge and David Robinson
- [4] "Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning" by Benjamin Bengfort, Rebecca Bilbro, and Tony Ojeda
- [5] CS 224D: Deep Learning for NLP [notes](http://cs224d.stanford.edu/lecture_notes/notes1.pdf) [notes](https://cs224d.stanford.edu/lecture_notes/LectureNotes2.pdf)
- [6] [3B1B GPT intuition](https://www.youtube.com/watch?v=wjZofJX0v4M&ab_channel=3Blue1Brown)