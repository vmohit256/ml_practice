{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a sentiment classification model for 50k IMDB movie reviews dataset using only simple statistical techniques.\n",
    "\n",
    "Evaluate its performance and identify what features work and what don't.\n",
    "\n",
    "Reference notebooks to learn from:\n",
    "- https://www.kaggle.com/code/majinx/nlp-imdb-reviews-prediction-multiple-models\n",
    "- https://www.kaggle.com/code/yasserh/imdb-movie-rating-sentiment-analysis\n",
    "- https://www.kaggle.com/code/dmid2qwde/imdb-best-accuracy-0-89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re, string\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize as nltk_tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag as nltk_pos_tagger\n",
    "from nltk.stem import PorterStemmer\n",
    "import random, time\n",
    "from scipy import sparse\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_pdf = pd.read_csv('../../local/data/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
    "\n",
    "# split into train and test data. avoid looking at the test data\n",
    "train_pdf = full_data_pdf.sample(frac=0.8, random_state=0)\n",
    "test_pdf = full_data_pdf.drop(train_pdf.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Observations:\n",
    "- there are some html tags. Mostly <br /> tags\n",
    "- sometimes the review has rating inside it like 7/10, 10/10, RATING: 10 of 10. That can be useful signal\n",
    "\"\"\"\n",
    "\n",
    "train_pdf.info()\n",
    "\n",
    "# see sample reviews\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(train_pdf.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to find simple discriminatory features\n",
    "# plot num words and num chars distribution by class. May be one class has longer reviews\n",
    "train_pdf['num_words'] = train_pdf['review'].apply(lambda x: len(x.split()))\n",
    "train_pdf['num_chars'] = train_pdf['review'].apply(lambda x: len(x))\n",
    "sns.kdeplot(data=train_pdf, x='num_words', hue='sentiment', common_norm=False)\n",
    "plt.show()\n",
    "sns.kdeplot(data=train_pdf, x='num_chars', hue='sentiment', common_norm=False)\n",
    "plt.show()\n",
    "# both classes have similar distribution. So, length of review is not a good discriminator\n",
    "\n",
    "# regex that matches html tags like <.*>. What all tags like this are there? Do they help discriminate?\n",
    "html_tag_re = re.compile(r'<.*?>')\n",
    "html_tags = Counter()\n",
    "for review in train_pdf['review']:\n",
    "    html_tags.update(html_tag_re.findall(review))\n",
    "print(html_tags)\n",
    "# only <br /> tag has significant counts. Others can be ignored or removed. In any case, they have very low counts\n",
    "# does <br /> only appear in pairs? Like <br /><br />\n",
    "\n",
    "br_tag_pairs_counts = sum([review.count('<br /><br />') for review in train_pdf['review']])\n",
    "print (2 * br_tag_pairs_counts - html_tags['<br />']) # yes. pretty much\n",
    "\n",
    "# does the presence of <br /> tag help discriminate?\n",
    "train_pdf['num_br_tags'] = train_pdf['review'].apply(lambda x: x.count('<br />'))\n",
    "sns.countplot(data=train_pdf, x='num_br_tags', hue='sentiment')\n",
    "plt.show() # not at all. Both classes have similar distribution\n",
    "\n",
    "# sometimes reviews have websites inside them. Show counts of websites\n",
    "website_re = re.compile('https?://\\S+|www\\.\\S+')\n",
    "websites = Counter()\n",
    "for review in train_pdf['review']:\n",
    "    websites.update(website_re.findall(review))\n",
    "print(websites)\n",
    "print(sum(websites.values())) # very low counts. Can be ignored\n",
    "\n",
    "# may be capitalization amount can help discriminate\n",
    "train_pdf['num_upper'] = train_pdf['review'].apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "sns.kdeplot(data=train_pdf, x='num_upper', hue='sentiment', common_norm=False)\n",
    "plt.show() # negative reviews have more slightly more capitalization but not much\n",
    "\n",
    "# try to extract ratings from reviews like 7/10, 2.5/4, RATING: 10 of 10, 5 out of 5\n",
    "def extract_ratings(review):\n",
    "    rating_re = re.compile(r'(RATING:?\\s*)?\\b(-?\\d+(\\.\\d+)?)\\s*(/|(out)\\s*(of))\\s*\\d+(\\.\\d+)?(\\s|\\.)', re.IGNORECASE)\n",
    "    match = rating_re.search(review)\n",
    "    ratings = []\n",
    "    while match:\n",
    "        ratings.append(review[match.start():match.end()])\n",
    "        review = review[:match.start()] + review[match.end():]\n",
    "        match = rating_re.search(review)\n",
    "    return ratings\n",
    "\n",
    "train_pdf['ratings'] = train_pdf['review'].apply(extract_ratings)\n",
    "rating_counts = Counter()\n",
    "for ratings in train_pdf['ratings']:\n",
    "    rating_counts.update(ratings)\n",
    "print (f\"Total ratings: {sum(rating_counts.values())}\\n\\n\")\n",
    "print(rating_counts)\n",
    "\n",
    "# plot ratings distribution by class\n",
    "train_pdf['num_ratings'] = train_pdf['ratings'].apply(len)\n",
    "# show bar plot of num ratings distribution by class\n",
    "sns.countplot(data=train_pdf, x='num_ratings', hue='sentiment')\n",
    "plt.show() # weird! Why is even this balanced between the two classes :D? Was the data synthetically generated?\n",
    "\n",
    "# show word clouds of ratings by class\n",
    "# positive reviews rating word cloud\n",
    "positive_ratings = [rating for rating in train_pdf[train_pdf['sentiment'] == 'positive']['ratings']]\n",
    "positive_ratings = [rating.replace(' ', '_').replace(':', '') for ratings in positive_ratings for rating in ratings]\n",
    "positive_ratings = ' '.join(positive_ratings)\n",
    "wordcloud = WordCloud(width=800, height=400).generate(positive_ratings)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# negative reviews rating word cloud\n",
    "negative_ratings = [rating for rating in train_pdf[train_pdf['sentiment'] == 'negative']['ratings']]\n",
    "negative_ratings = [rating.replace(' ', '_').replace(':', '') for ratings in negative_ratings for rating in ratings]\n",
    "negative_ratings = ' '.join(negative_ratings)\n",
    "wordcloud = WordCloud(width=800, height=400).generate(negative_ratings)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# alright! ratings are present in some reviews. They are useful discriminators but the count is pretty low\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop feature computation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pipeline structure:\n",
    "\n",
    "- text preprocessing\n",
    "    - remove html tags\n",
    "    - remove websites\n",
    "    - tunable\n",
    "        - extract ratings and inject as new synthetic normalized words inside the review\n",
    "        - try negation handling like NOT_good, NOT_boring, etc. as described in the NLP book\n",
    "        - tag with POS tags\n",
    "        - lowercasing\n",
    "        - remove punctuations and special characters\n",
    "        - remove stopwords\n",
    "        - lemmatization\n",
    "- feature computation\n",
    "    - tokenizer: r\"(?u)\\b\\w\\w+\\b\" (default)\n",
    "    - count vectorizer\n",
    "        - tune ngram range: (1, 1), (1, 2), (1, 3)\n",
    "        - set binary to true or false\n",
    "        - set min_df to values like 1, 2, 5\n",
    "        - set max_features to values like 1000, 5000, 10000\n",
    "    - tfidf transformer\n",
    "        - skip completely\n",
    "        - try different norms: l1, l2, none\n",
    "        - set use_idf to true or false\n",
    "        - set smooth_idf to true or false\n",
    "        - set sublinear tf to true or false\n",
    "    - lexicon features\n",
    "        - raw counts of ratings in Afinn, Bing, NRC lexicons\n",
    "            - unique words in the lexicon that are present in the review\n",
    "        - normalize\n",
    "            - divide by the number of words in the review\n",
    "- model training\n",
    "    - LogisticRegression\n",
    "        - tweak hyperparameters\n",
    "        - know if its underfitting or overfitting\n",
    "    - MultinomialNB\n",
    "        - ensure each feature is a count\n",
    "        - tweak alpha (smoothing parameter)\n",
    "    - BernoulliNB\n",
    "        - ensure each feature is binary\n",
    "        - tweak alpha (smoothing parameter)\n",
    "    - RandomForest\n",
    "        - tweak hyperparameters\n",
    "    - GradientBoosting\n",
    "        - tweak hyperparameters\n",
    "- model evaluation\n",
    "    - metrics:\n",
    "        - accuracy, precision, recall, F1-score\n",
    "        - confusion matrix\n",
    "        - ROC-AUC\n",
    "    - compare metrics between training and dev set to detect overfitting\n",
    "    - show feature importance\n",
    "- model selection\n",
    "    - tweak each model type separately\n",
    "        - try different preprocessing and hyperparameters that work best for that model\n",
    "\"\"\"\n",
    "_=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_tag_re = re.compile(r'<.*?>')\n",
    "website_re = re.compile('https?://\\S+|www\\.\\S+')\n",
    "rating_re = re.compile(r'(RATING:?\\s*)?\\b(-?\\d+(\\.\\d+)?)\\s*(/|((out)?\\s*(of)))\\s*\\d+(\\.\\d+)?[^/]', re.IGNORECASE)\n",
    "nltk_porter_stemmer = PorterStemmer()\n",
    "nltk_stop_word_list = set(stopwords.words('english'))\n",
    "\n",
    "def extract_ratings(review):\n",
    "    match = rating_re.search(review)\n",
    "    ratings = []\n",
    "    while match:\n",
    "        ratings.append(review[match.start():match.end()])\n",
    "        review = review[:match.start()] + review[match.end():]\n",
    "        match = rating_re.search(review)\n",
    "    return ratings, review\n",
    "\n",
    "def handle_negation(text):\n",
    "    # Define negation tokens and punctuation marks\n",
    "    negation_tokens = [\"n't\", \"not\", \"no\", \"never\"]\n",
    "    punctuation_marks = ['.', ',', ';', ':', '!', '?']\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk_tokenizer(text)\n",
    "    \n",
    "    # Initialize variables\n",
    "    negation = False\n",
    "    result_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        # If the token is a punctuation mark, reset negation flag\n",
    "        if token in punctuation_marks:\n",
    "            negation = False\n",
    "        \n",
    "        # If negation is active, prepend \"NOT_\" to the token\n",
    "        if negation:\n",
    "            result_tokens.append(\"NOT_\" + token)\n",
    "        else:\n",
    "            result_tokens.append(token)\n",
    "        \n",
    "        # If the token is a negation token, activate negation flag\n",
    "        if token.lower().split('_')[0] in negation_tokens:\n",
    "            negation = True\n",
    "    \n",
    "    # Join the tokens back into a string\n",
    "    result_text = ' '.join(result_tokens)\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "afinn = pd.read_csv('../../local/data/lexicons/sentiment_analysis/Afinn.csv', encoding='ISO-8859-1')\n",
    "afinn['source'] = 'Afinn'\n",
    "bing = pd.read_csv('../../local/data/lexicons/sentiment_analysis/Bing.csv')\\\n",
    "        .rename(columns={'sentiment': 'value'})\n",
    "bing['source'] = 'Bing'\n",
    "nrc = pd.read_csv('../../local/data/lexicons/sentiment_analysis/NRC.csv')\\\n",
    "    .rename(columns={'sentiment': 'value'})\n",
    "nrc['source'] = 'NRC'\n",
    "sentiment_lexicons = pd.concat([afinn, bing, nrc], ignore_index=True)\n",
    "sentiment_lexicons['word'] = sentiment_lexicons['word'].str.lower()\n",
    "sentiment_lexicons['label'] = sentiment_lexicons['source'] \\\n",
    "    + '_' + (sentiment_lexicons['value']).astype(str)\n",
    "sentiment_lexicons = sentiment_lexicons[['word', 'label']].drop_duplicates()\n",
    "label_to_words = sentiment_lexicons.groupby('label')['word'].apply(set).to_dict()\n",
    "\n",
    "def compute_lexicon_feature(text, label, normalize=False):\n",
    "    text_words = set(text.lower().split())\n",
    "    label_words = label_to_words[label]\n",
    "    common_words = len(text_words.intersection(label_words))\n",
    "    if normalize:\n",
    "        return common_words / len(text_words)\n",
    "    return common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, config):\n",
    "    # remove html tags\n",
    "    text = text.replace('<br /><br />', ' ')\n",
    "    text = html_tag_re.sub('', text)\n",
    "\n",
    "    # remove websites\n",
    "    text = website_re.sub('', text)\n",
    "\n",
    "    ratings = []\n",
    "\n",
    "    # extract ratings\n",
    "    extract_ratings_flag = config.get('text_preprocessing', {}).get('extract_ratings', False)\n",
    "    if extract_ratings_flag:\n",
    "        ratings, text = extract_ratings(text)\n",
    "\n",
    "    # add POS tags\n",
    "    pos_tagging_flag = config.get('text_preprocessing', {}).get('pos_tagging', False)\n",
    "    if pos_tagging_flag:\n",
    "        tagged_tokens = nltk_pos_tagger(nltk_tokenizer(text))\n",
    "        text = ' '.join([token + (\"_\"+tag if tag!=token else '') for token, tag in tagged_tokens])\n",
    "\n",
    "    # handle negation\n",
    "    negation_handling_flag = config.get('text_preprocessing', {}).get('negation_handling', False)\n",
    "    if negation_handling_flag:\n",
    "        text = handle_negation(text)\n",
    "\n",
    "    # lower casing\n",
    "    lowercasing_flag = config.get('text_preprocessing', {}).get('lowercasing', False)\n",
    "    if lowercasing_flag:\n",
    "        text = text.lower()\n",
    "\n",
    "    # remove punctuations and special characters\n",
    "    remove_puncs_n_sp_ch_flag = config.get('text_preprocessing', {}).get('remove_puncs_n_sp_ch', False)\n",
    "    if remove_puncs_n_sp_ch_flag:\n",
    "        punctuation_without_underscore = string.punctuation.replace('_', '')\n",
    "        text = text.translate(str.maketrans('', '', punctuation_without_underscore))\n",
    "\n",
    "    # remove stopwords\n",
    "    remove_stopwords_flag = config.get('text_preprocessing', {}).get('remove_stopwords', False)\n",
    "\n",
    "    # lemmitization\n",
    "    stemming_flag = config.get('text_preprocessing', {}).get('stemming', False)\n",
    "\n",
    "    if remove_stopwords_flag or stemming_flag:\n",
    "        tokens = nltk_tokenizer(text)\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            is_not = token.startswith('not_')\n",
    "            token = token.split('not_')[-1]\n",
    "            pos_decoration = ''\n",
    "            if '_' in token:\n",
    "                pos_decoration = token.split('_')[-1]\n",
    "                token = token.split('_')[0]\n",
    "            if remove_stopwords_flag and token in nltk_stop_word_list: continue\n",
    "            if stemming_flag:\n",
    "                token = nltk_porter_stemmer.stem(token)\n",
    "            new_tokens.append(f\"{'not_' if is_not else ''}{token}{'_' + pos_decoration if pos_decoration else ''}\")\n",
    "            \n",
    "        # tokens = [lemmatizer.lemmatize(token) for token in tokens if token.split('not_')[-1].split('_')[0] not in stop_words]\n",
    "        text = ' '.join(new_tokens)\n",
    "\n",
    "    if extract_ratings_flag:\n",
    "        # inject ratings as synthetic words\n",
    "        for rating in ratings:\n",
    "            rating = rating.strip().strip('.')\n",
    "            # replace spaces and tabs with underscores\n",
    "            rating = rating.replace(' ', '_')\n",
    "            text += f\" RATING_START_{rating}_END\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_features(train_pdf, pipeline_config):\n",
    "    train_pdf = train_pdf.copy()\n",
    "    trained_assets = {}\n",
    "    train_pdf['word_count'] = train_pdf['processed_review'].apply(lambda x: len(x.split()))\n",
    "    train_pdf['char_count'] = train_pdf['processed_review'].apply(lambda x: len(x))\n",
    "    for label in label_to_words.keys():\n",
    "        train_pdf[label] = train_pdf['processed_review']\\\n",
    "            .apply(lambda x: compute_lexicon_feature(x, label, normalize=True))\n",
    "    for cname in ['word_count', 'char_count'] + list(label_to_words.keys()):\n",
    "        trained_assets[cname+\"_range\"] = (train_pdf[cname].min(), train_pdf[cname].max())\n",
    "    if 'count_vectorizer' in pipeline_config['feature_computation']:\n",
    "        cv_config = pipeline_config['feature_computation']['count_vectorizer']\n",
    "        tokenizer = pipeline_config['feature_computation'].get('tokenizer', None)\n",
    "        if tokenizer is None:\n",
    "            pass\n",
    "        elif tokenizer == 'nltk':\n",
    "            tokenizer = nltk_tokenizer\n",
    "        elif tokenizer == 'split':\n",
    "            tokenizer = lambda x: x.split()\n",
    "        else: raise ValueError(f\"Unknown tokenizer: {tokenizer}\")\n",
    "        count_vectorizer = CountVectorizer(tokenizer=tokenizer,\n",
    "                                            lowercase=False,\n",
    "                                            ngram_range=cv_config.get('ngram_range', (1, 1)),\n",
    "                                            binary=cv_config.get('binary', False),\n",
    "                                            max_df=cv_config.get('max_df', 1.0),\n",
    "                                            min_df=cv_config.get('min_df', 1),\n",
    "                                            max_features=cv_config.get('max_features', None))\n",
    "        count_vectorizer.fit(train_pdf['processed_review'])\n",
    "        tfidf_config = cv_config.get('tfidf_transformer', {})\n",
    "        tfidf_transformer = TfidfTransformer(use_idf=tfidf_config.get('use_idf', True),\n",
    "                                            smooth_idf=tfidf_config.get('smooth_idf', True),\n",
    "                                            sublinear_tf=tfidf_config.get('sublinear_tf', False),\n",
    "                                            norm=tfidf_config.get('norm', 'l2'))\n",
    "        tfidf_transformer.fit(count_vectorizer.transform(train_pdf['processed_review']))\n",
    "        trained_assets['count_vectorizer'] = count_vectorizer\n",
    "        trained_assets['tfidf_transformer'] = tfidf_transformer\n",
    "    return trained_assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(data_pdf, trained_assets, pipeline_config):\n",
    "    for feature in pipeline_config['feature_computation'].get('custom_features', []):\n",
    "        if feature == 'word_count':\n",
    "            data_pdf['feat_word_count'] = data_pdf['processed_review'].apply(lambda x: len(x.split()))\n",
    "        elif feature == 'word_count_minmax_scaled':\n",
    "            min_val, max_val = trained_assets['word_count_range']\n",
    "            data_pdf['feat_word_count_minmax_scaled'] = data_pdf['processed_review']\\\n",
    "                .apply(lambda x: (min(len(x.split()), max_val) - min_val) / (max_val - min_val + 1e-6))\n",
    "        elif feature == 'char_count':\n",
    "            data_pdf['feat_char_count'] = data_pdf['processed_review'].apply(lambda x: len(x))\n",
    "        elif feature == 'char_count_minmax_scaled':\n",
    "            min_val, max_val = trained_assets['char_count_range']\n",
    "            data_pdf['feat_char_count_minmax_scaled'] = data_pdf['processed_review']\\\n",
    "                .apply(lambda x: (min(len(x), max_val) - min_val) / (max_val - min_val + 1e-6))\n",
    "        elif feature == 'lexicon_features':\n",
    "            for label in label_to_words.keys():\n",
    "                data_pdf[f'feat_{label}'] = data_pdf['unprocessed_review']\\\n",
    "                    .apply(lambda x: compute_lexicon_feature(x, label, normalize=False))\n",
    "        elif feature == 'lexicon_features_minmax_scaled':\n",
    "            for label in label_to_words.keys():\n",
    "                min_val, max_val = trained_assets[label+'_range']\n",
    "                data_pdf[f'feat_{label}_minmax_scaled'] = data_pdf['unprocessed_review']\\\n",
    "                    .apply(lambda x: (min(compute_lexicon_feature(x, label, normalize=False), max_val) - min_val) / (max_val - min_val + 1e-6))\n",
    "        elif feature == 'lexicon_features_normalized':\n",
    "            for label in label_to_words.keys():\n",
    "                data_pdf[f'feat_{label}_normalized'] = data_pdf['unprocessed_review']\\\n",
    "                    .apply(lambda x: compute_lexicon_feature(x, label, normalize=True))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown feature: {feature}\") \n",
    "    \n",
    "    additional_features = list(filter(lambda x: x.startswith('feat_'), data_pdf.columns))\n",
    "    if 'count_vectorizer' in pipeline_config['feature_computation']:\n",
    "        count_vectorizer = trained_assets['count_vectorizer']\n",
    "        tfidf_transformer = trained_assets['tfidf_transformer']\n",
    "        X = count_vectorizer.transform(data_pdf['processed_review'])\n",
    "        X = tfidf_transformer.transform(X)\n",
    "        if len(additional_features) > 0:\n",
    "            X_additional_features = data_pdf[additional_features].values\n",
    "            X = sparse.hstack([X, X_additional_features], format='csr')\n",
    "        feat_id_to_name = {i: \"CountFeat_\"+name \n",
    "                           for i, name in enumerate(count_vectorizer.get_feature_names_out())}\n",
    "        feat_id_to_name.update({i+len(feat_id_to_name): name for i, name in enumerate(additional_features)})\n",
    "        return X, feat_id_to_name\n",
    "    else:\n",
    "        return sparse.csr_matrix(data_pdf[additional_features].values), \\\n",
    "            {i: name for i, name in enumerate(additional_features)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = {\n",
    "    'text_preprocessing': {\n",
    "        'extract_ratings': True,\n",
    "        'pos_tagging': True,\n",
    "        'negation_handling': True,\n",
    "        'remove_puncs_n_sp_ch': True,\n",
    "        'remove_stopwords': True,\n",
    "        'stemming': True,\n",
    "        'lowercasing': True,\n",
    "    },\n",
    "    'feature_computation': {\n",
    "        'tokenizer': 'split',\n",
    "        'custom_features': ['word_count', 'char_count', \n",
    "                            'word_count_minmax_scaled', 'char_count_minmax_scaled',\n",
    "                            'lexicon_features', \n",
    "                            'lexicon_features_minmax_scaled',\n",
    "                            'lexicon_features_normalized'],\n",
    "        'count_vectorizer': {\n",
    "            'ngram_range': (1, 3),\n",
    "            'binary': False,\n",
    "            'min_df': 2,\n",
    "            'max_df': 1.0,\n",
    "            'max_features': None,\n",
    "            'tfidf_transformer': {\n",
    "                'use_idf': True,\n",
    "                'smooth_idf': True,\n",
    "                'sublinear_tf': False,\n",
    "                'norm': 'l2'\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_pdf[['review', 'sentiment']].copy()\\\n",
    "    .rename(columns={'review': 'unprocessed_review'})\n",
    "train_data = data.sample(frac=0.8, random_state=0)\n",
    "dev_data = data.drop(train_data.index)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "dev_data = dev_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_to_process = []\n",
    "for index, row in train_data.iterrows():\n",
    "    reviews_to_process.append((index, row['unprocessed_review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def process_review(review_tuple):\n",
    "    index, review_text = review_tuple\n",
    "    processed_text = preprocess_text(review_text, pipeline_config)\n",
    "    return (index, processed_text)\n",
    "\n",
    "# Preparing the data (your existing code)\n",
    "reviews_to_process = []\n",
    "for index, row in train_data.iterrows():\n",
    "    reviews_to_process.append((index, row['unprocessed_review']))\n",
    "\n",
    "# Processing in parallel\n",
    "with ProcessPoolExecutor(max_workers=10) as executor:\n",
    "    results = list(executor.map(process_review, reviews_to_process[:1000]))\n",
    "\n",
    "# 'results' now contains the processed reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data['processed_review'] = train_data['review']\\\n",
    "    .apply(lambda x: preprocess_text(x, pipeline_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_assets = fit_features(train_data, pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, feat_id_to_name = compute_features(train_data, trained_assets, pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_train.shape)\n",
    "print (X_train.head())\n",
    "feat_id_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['processed_review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_config = {}\n",
    "cv = CountVectorizer(tokenizer=lambda x: x.split(),\n",
    "                                            lowercase=False,\n",
    "                                            ngram_range=cv_config.get('ngram_range', (1, 3)),\n",
    "                                            binary=cv_config.get('binary', False),\n",
    "                                            max_df=cv_config.get('max_df', 1.0),\n",
    "                                            min_df=cv_config.get('min_df', 1),\n",
    "                                            max_features=cv_config.get('max_features', None))\n",
    "\n",
    "start_time = time.time()\n",
    "cv.fit(train_data['processed_review'])\n",
    "print (f\"Time taken: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data['processed_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_config = cv_config.get('tfidf_transformer', {})\n",
    "tfidf_transformer = TfidfTransformer(use_idf=tfidf_config.get('use_idf', True),\n",
    "                                    smooth_idf=tfidf_config.get('smooth_idf', True),\n",
    "                                    sublinear_tf=tfidf_config.get('sublinear_tf', False),\n",
    "                                    norm=tfidf_config.get('norm', 'l2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = cv.transform(train_data['processed_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer.fit(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tfidf_transformer.transform(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (t[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dry run the pipeline on a sample data to see if its works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"www.youtube.com \" + train_pdf['review'].iloc[45] + \" RATING: 10 of 10\"\n",
    "\n",
    "print (text, '\\n\\n')\n",
    "\n",
    "preprocessed_text = preprocess_text(text, pipeline_config)\n",
    "print (preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_dataset = []\n",
    "tokens = preprocessed_text.split()\n",
    "random.seed(0)\n",
    "chunk_idx = 0\n",
    "while chunk_idx < len(tokens):\n",
    "    chunk_size = random.randint(3, 10)\n",
    "    chunk = ' '.join(tokens[chunk_idx:chunk_idx+chunk_size])\n",
    "    toy_dataset.append({\n",
    "        'unprocessed_review': ' '.join(text.split()[chunk_idx:chunk_idx+chunk_size]),\n",
    "        'processed_review': chunk,\n",
    "        'sentiment': 'positive' if random.random() > 0.5 else 'negative'\n",
    "    })\n",
    "    chunk_idx += chunk_size\n",
    "pdf = pd.DataFrame(toy_dataset)[:20]\n",
    "\n",
    "train = pdf.sample(frac=0.8, random_state=0)\n",
    "test = pdf.drop(train.index)\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_assets = fit_features(train, pipeline_config)\n",
    "X, feat_id_to_name = compute_features(train, trained_assets, pipeline_config)\n",
    "print (X)\n",
    "print (feat_id_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y = (train['sentiment'] == 'positive').astype(int)\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(train.reset_index(drop=True))\n",
    "\n",
    "print (X.shape, y.shape)\n",
    "for i in range(6):\n",
    "    print (i)\n",
    "    print (X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=nltk_tokenizer, ngram_range=(1, 1), binary=False, min_df=1, max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_wordnet_lemmatizer.lemmatize('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train_pdf['review'].iloc[0][:200]\n",
    "print (text, '\\n')\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print (tokens, '\\n')\n",
    "tags = nltk.pos_tag(tokens)\n",
    "print(tags, '\\n\\n')\n",
    "\n",
    "pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "tokens = re.findall(pattern, text)\n",
    "print (tokens, '\\n')\n",
    "tags = nltk.pos_tag(tokens)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text=str(text).lower()\n",
    "    text=re.sub('\\[.*?\\]', '', text)\n",
    "    text=re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text=re.sub('<.*?>+', '', text)\n",
    "    text=re.sub('[%s]' % re.escape(string.punctuation),'',text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sws=stopwords.words('english')\n",
    "lemma=WordNetLemmatizer()\n",
    "\n",
    "def preprocessdata(text):\n",
    "    text= ' '.join(word for word in text.split(' ') if word not in sws)\n",
    "    text= ' '.join(lemma.lemmatize(word) for word in text.split(' '))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = train_pdf.sample(10).copy()\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in pdf.iterrows():\n",
    "    print (index, end=\"\\n\")\n",
    "    print (row['review'])\n",
    "    print (\"\\n\\n--------------\\n\\n\")\n",
    "    print (preprocessdata(clean_text(row['review'])))\n",
    "    print (\"\\n\\n===================\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ['aa aa bb bb Cc cc cc dd', 'cc cc dd dd dd dd ee']\n",
    "cv = CountVectorizer(lowercase=False, ngram_range=(1, 3), max_features=6)\n",
    "X = cv.fit_transform(s)\n",
    "print(cv.get_feature_names_out())\n",
    "print (X)\n",
    "print(X.toarray())\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=False, norm=None)\n",
    "Y = tfidf.fit_transform(X)\n",
    "# print (tfidf.idf_)\n",
    "print (Y)\n",
    "print (Y.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=Pipeline([('bow',CountVectorizer()),('tfidf',TfidfTransformer()),('model',model)])\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred=pipe.predict(X_test)\n",
    "print('Accuracy Score: ',accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "plot_confusion_matrix(pipe,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
