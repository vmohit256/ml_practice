{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune GPT-2 on the shopping products dataset so that it can be used as a teacher model for simpler models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.manifold import TSNE, MDS, Isomap, LocallyLinearEmbedding\n",
    "from sklearn.decomposition import PCA, KernelPCA, FastICA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# nlp\n",
    "import nltk\n",
    "\n",
    "# deep learning\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments, AutoModel\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import PreTrainedTokenizerFast, GPT2TokenizerFast\n",
    "\n",
    "# local imports\n",
    "import importlib\n",
    "# add '../../deep_learning/src/' to path\n",
    "import sys\n",
    "sys.path.insert(1, '../../')\n",
    "import deep_learning.src.nlp.models as nlp_models\n",
    "_ = importlib.reload(nlp_models)\n",
    "\n",
    "import deep_learning.src.nlp.training as nlp_training\n",
    "_ = importlib.reload(nlp_training)\n",
    "\n",
    "import deep_learning.src.nlp.preprocessing as local_preprocessing\n",
    "_ = importlib.reload(local_preprocessing)\n",
    "\n",
    "import deep_learning.src.nlp.analysis as dl_analysis\n",
    "_ = importlib.reload(dl_analysis)\n",
    "\n",
    "# basic data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string, os, pickle\n",
    "from IPython.display import display, HTML\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_info = {}\n",
    "with open('local/model_comparison/dataset_to_info.pkl', 'rb') as f:\n",
    "    dataset_to_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding padding token\n",
      "Vocab size: 50258\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# Add a new padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    print (\"Adding padding token\")\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "vocab = tokenizer.get_vocab()\n",
    "print (f\"Vocab size: {len(vocab)}\")\n",
    "id_to_token = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path, tokenizer, block_size = 40):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer = tokenizer,\n",
    "        file_path = file_path,\n",
    "        block_size = block_size,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_gpt2_text_samples(dataset, split, dataset_to_info):\n",
    "    output_dir = f'local/gpt2_finetuning/{dataset}/{split}/'\n",
    "    lm_prediction_targets = dataset_to_info[dataset]['data'][split]['raw_data']['lm_prediction_targets']\n",
    "\n",
    "    output_path = f'{output_dir}/lm_prediction_targets.txt'\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, 'w') as f:\n",
    "        for target in lm_prediction_targets:\n",
    "            f.write(target + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in dataset_to_info:\n",
    "    for split in dataset_to_info[dataset]['data']:\n",
    "        dump_gpt2_text_samples(dataset, split, dataset_to_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mohitvyas\\Miniconda3\\envs\\py39\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = 'amazon'\n",
    "root_dir = f'local/gpt2_finetuning/{dataset}/'\n",
    "train_output_dir = f'{root_dir}/train/model_checkpoints/'\n",
    "\n",
    "train_dataset = load_dataset(f'{root_dir}/train/lm_prediction_targets.txt', tokenizer)\n",
    "val_dataset = load_dataset(f'{root_dir}/val/lm_prediction_targets.txt', tokenizer)\n",
    "test_dataset = load_dataset(f'{root_dir}/test/lm_prediction_targets.txt', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('local/gpt2_finetuning/amazon//train/model_checkpoints//tokenizer/tokenizer_config.json',\n",
       " 'local/gpt2_finetuning/amazon//train/model_checkpoints//tokenizer/special_tokens_map.json',\n",
       " 'local/gpt2_finetuning/amazon//train/model_checkpoints//tokenizer/vocab.json',\n",
       " 'local/gpt2_finetuning/amazon//train/model_checkpoints//tokenizer/merges.txt',\n",
       " 'local/gpt2_finetuning/amazon//train/model_checkpoints//tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save tokenizer\n",
    "tokenizer.save_pretrained(train_output_dir+'/tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = load_data_collator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=train_output_dir,\n",
    "    overwrite_output_dir=False,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    max_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    learning_rate=5e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e9b2218e6140dab0e10b0c58ca8614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105733bc922a42f09a3c919226a0d7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.3516130447387695, 'eval_runtime': 54.8732, 'eval_samples_per_second': 199.77, 'eval_steps_per_second': 24.985, 'epoch': 0.02}\n",
      "{'train_runtime': 74.7645, 'train_samples_per_second': 10.7, 'train_steps_per_second': 1.338, 'train_loss': 4.630201416015625, 'epoch': 0.02}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=4.630201416015625, metrics={'train_runtime': 74.7645, 'train_samples_per_second': 10.7, 'train_steps_per_second': 1.338, 'total_flos': 16330752000000.0, 'train_loss': 4.630201416015625, 'epoch': 0.018145527127563055})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text from trained checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_dir, checkpoint=None, device='cuda'):\n",
    "    if checkpoint is None:\n",
    "        # get latest checkpoint\n",
    "        checkpoints = list(filter(lambda x: x.startswith('checkpoint-'), os.listdir(model_dir)))\n",
    "        checkpoints = [(int(x.split('-')[1]), x) for x in checkpoints]\n",
    "        latest_checkpoint = max(checkpoints, key=lambda x: x[0])[0]\n",
    "        model_path = f'{model_dir}/checkpoint-{latest_checkpoint}'\n",
    "    else:\n",
    "        model_path = f'{model_dir}/checkpoint-{checkpoint}'\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(train_output_dir)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(train_output_dir+'/tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(sequence, max_length, model, tokenizer):\n",
    "    inputs = tokenizer(f'{sequence}', return_tensors='pt', padding=True)\n",
    "    ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    print (attention_mask)\n",
    "    final_outputs = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        max_length=max_length,\n",
    "        attention_mask=attention_mask,\n",
    "        pad_token_id=model.config.eos_token_id,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    output_text = tokenizer.decode(final_outputs[0], skip_special_tokens=True)\n",
    "    return final_outputs, output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1]])\n"
     ]
    }
   ],
   "source": [
    "sequence = tokenizer.eos_token\n",
    "max_len = 60\n",
    "final_outputs, output_text = generate_text(sequence, max_len, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure perplexities on train, validation and test sets using the vocab used by the n-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_split = 'val'\n",
    "lm_target_input_ids = dataset_to_info[dataset]['data'][eval_split]['lm_target_input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755\n"
     ]
    }
   ],
   "source": [
    "batches = []\n",
    "attention_masks = []\n",
    "batch_size = 32\n",
    "maxlen = dataset_to_info[dataset]['configs']['maxlen'] + 2\n",
    "for i in range(0, len(lm_target_input_ids), batch_size):\n",
    "    batch = lm_target_input_ids[i:i+batch_size]\n",
    "    attention_mask = [[1] * len(x) for x in batch]\n",
    "    for j in range(len(batch)):\n",
    "        batch[j] = batch[j][:maxlen] \n",
    "        attention_mask[j] = attention_mask[j][:maxlen]\n",
    "        batch[j].extend([tokenizer.eos_token_id] * (maxlen - len(batch[j])))\n",
    "        attention_mask[j].extend([0] * (maxlen - len(attention_mask[j])))\n",
    "    batches.append(batch)\n",
    "    attention_masks.append(attention_mask)\n",
    "\n",
    "print (len(batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batches[0]\n",
    "batch_attention_mask = attention_masks[0]\n",
    "sentence = batch[0]\n",
    "sentence_attention_mask = batch_attention_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J-464)<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (sentence_attention_mask)\n",
    "tokenizer.decode(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_probs = None\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    ids = torch.tensor(batch).to('cuda')\n",
    "    attention_mask = torch.tensor(batch_attention_mask).to('cuda')\n",
    "    outputs = model(ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    next_token_probs = torch.nn.functional.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 50257)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_next_token_probs = next_token_probs.cpu().numpy()[0]\n",
    "sentence_next_token_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: <|endoftext|>\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄŠ: 0.0430183932185173\n",
      "\tA: 0.03331897035241127\n",
      "\tThe: 0.019795283675193787\n",
      "\tS: 0.0194003414362669\n",
      "\tT: 0.01738286018371582\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J\n",
      "top-5 tokens with their probabilities:\n",
      "\tACK: 0.04394003003835678\n",
      "\tAN: 0.033528704196214676\n",
      "\tER: 0.027945753186941147\n",
      "\t.: 0.02778715267777443\n",
      "\tAK: 0.01632264256477356\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ K: 0.03272165358066559\n",
      "\t.: 0.03002089262008667\n",
      "\tÄ S: 0.026225674897432327\n",
      "\tÄŠ: 0.022505376487970352\n",
      "\tON: 0.014905023388564587\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ Men: 0.1776643544435501\n",
      "\tÄ Women: 0.08563017845153809\n",
      "\tÄŠ: 0.04325247183442116\n",
      "\tÄ Mens: 0.024437248706817627\n",
      "\tÄ Shoes: 0.02308136224746704\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion T\n",
      "top-5 tokens with their probabilities:\n",
      "\t-: 0.6529910564422607\n",
      "\tshirt: 0.11977624893188477\n",
      "\tops: 0.04356880486011505\n",
      "\tshirts: 0.04006678983569145\n",
      "\tÄ Shirt: 0.03316032513976097\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄŠ: 0.23132897913455963\n",
      "\tÄ for: 0.14440898597240448\n",
      "\tÄ (: 0.036373015493154526\n",
      "\tÄ -: 0.025128621608018875\n",
      "\t,: 0.024622105062007904\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ Men: 0.5970619916915894\n",
      "\tÄ Women: 0.266658216714859\n",
      "\tÄ Kids: 0.011366598308086395\n",
      "\tÄ Girls: 0.010892272926867008\n",
      "\tÄ Boys: 0.009025810286402702\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄŠ: 0.26005908846855164\n",
      "\t's: 0.1593862622976303\n",
      "\tÄ (: 0.0533461794257164\n",
      "\t,: 0.03582886978983879\n",
      "\tÄ |: 0.03309106454253197\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women |\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ Men: 0.06406296789646149\n",
      "\tÄ Women: 0.058948270976543427\n",
      "\tÄ Je: 0.01996895670890808\n",
      "\tÄ Casual: 0.017590975388884544\n",
      "\tÄ Black: 0.01738683134317398\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | T\n",
      "top-5 tokens with their probabilities:\n",
      "\t-: 0.5238362550735474\n",
      "\tops: 0.197141632437706\n",
      "\tshirt: 0.04097934812307358\n",
      "\tÄ Shirt: 0.03556926175951958\n",
      "\tshirts: 0.014330880716443062\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ for: 0.48889994621276855\n",
      "\tÄ |: 0.06293061375617981\n",
      "\tÄ &: 0.0593637079000473\n",
      "\tÄŠ: 0.03858393430709839\n",
      "\tÄ For: 0.028163859620690346\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ Men: 0.7445201873779297\n",
      "\tÄ Women: 0.067792147397995\n",
      "\tÄ Kids: 0.031408876180648804\n",
      "\tÄ Boys: 0.028859592974185944\n",
      "\tÄ Girls: 0.019047394394874573\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ |: 0.4694337844848633\n",
      "\tÄŠ: 0.16260991990566254\n",
      "\tÄ &: 0.022559354081749916\n",
      "\tÄ (: 0.020265627652406693\n",
      "\t's: 0.01984153687953949\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women T\n",
      "top-5 tokens with their probabilities:\n",
      "\tops: 0.7661494612693787\n",
      "\t-: 0.10380499809980392\n",
      "\tshirts: 0.008033090271055698\n",
      "\tshirt: 0.0060036676004529\n",
      "\tÄ Shirt: 0.0057708085514605045\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ |: 0.462826132774353\n",
      "\tÄŠ: 0.1870785802602768\n",
      "\tÄ for: 0.09347806125879288\n",
      "\tÄ &: 0.026415830478072166\n",
      "\tÄ (: 0.01801086962223053\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ Men: 0.5168101787567139\n",
      "\tÄ Women: 0.1562604010105133\n",
      "\tÄ Kids: 0.015272154472768307\n",
      "\tÄ Regular: 0.012806981801986694\n",
      "\tÄ Boys: 0.011275594122707844\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ |: 0.47269412875175476\n",
      "\tÄŠ: 0.16008667647838593\n",
      "\tÄ (: 0.018906395882368088\n",
      "\tÄ &: 0.01665393076837063\n",
      "\tÄ Shoes: 0.012782462872564793\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Sty\n",
      "top-5 tokens with their probabilities:\n",
      "\tlish: 0.9957589507102966\n",
      "\tling: 0.0015346826985478401\n",
      "\tled: 0.0014304488431662321\n",
      "\tro: 0.00021313207980711013\n",
      "\tlee: 0.00018025876488536596\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ |: 0.10963405668735504\n",
      "\tÄŠ: 0.056407228112220764\n",
      "\t,: 0.03314163535833359\n",
      "\tÄ Printed: 0.02744939923286438\n",
      "\tÄ &: 0.026833591982722282\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish |\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ Men: 0.05000228434801102\n",
      "\tÄ Women: 0.04762377589941025\n",
      "\tÄ Casual: 0.029644139111042023\n",
      "\tÄ Printed: 0.02391205169260502\n",
      "\tÄ T: 0.02324364334344864\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ |: 0.15788237750530243\n",
      "\tÄ of: 0.12343169003725052\n",
      "\tÄ for: 0.0996154397726059\n",
      "\tÄŠ: 0.061863962560892105\n",
      "\t,: 0.041156865656375885\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ Women: 0.22612619400024414\n",
      "\tÄ Men: 0.2190185934305191\n",
      "\tÄ women: 0.08802823722362518\n",
      "\tÄ men: 0.013544443063437939\n",
      "\tÄ Girls: 0.008632523939013481\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ |: 0.4538738429546356\n",
      "\tÄŠ: 0.1172691211104393\n",
      "\t,: 0.0399685800075531\n",
      "\tÄ &: 0.02625366486608982\n",
      "\tÄ (: 0.014130732975900173\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls |\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ Casual: 0.03515734523534775\n",
      "\tÄ Women: 0.03365042433142662\n",
      "\tÄ Men: 0.03018798865377903\n",
      "\tÄ Top: 0.02474232390522957\n",
      "\tÄ T: 0.023900531232357025\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ for: 0.9402777552604675\n",
      "\tÄ of: 0.011872481554746628\n",
      "\tÄ with: 0.006320213433355093\n",
      "\tÄ to: 0.0033020672854036093\n",
      "\tÄŠ: 0.002000094624236226\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ Boys: 0.2566257119178772\n",
      "\tÄ Women: 0.23211632668972015\n",
      "\tÄ Men: 0.13596780598163605\n",
      "\tÄ Girls: 0.05870521813631058\n",
      "\tÄ boys: 0.024931617081165314\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ |: 0.48207321763038635\n",
      "\tÄŠ: 0.11525354534387589\n",
      "\tÄ Women: 0.01712908037006855\n",
      "\tÄ T: 0.012592090293765068\n",
      "\t's: 0.011117229238152504\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Sty\n",
      "top-5 tokens with their probabilities:\n",
      "\tlish: 0.9996334314346313\n",
      "\tled: 7.804881897754967e-05\n",
      "\tling: 5.196237543714233e-05\n",
      "\trene: 3.77223695977591e-05\n",
      "\t...: 3.339977047289722e-05\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ |: 0.49888935685157776\n",
      "\tÄŠ: 0.09124141186475754\n",
      "\tÄ Women: 0.025510426610708237\n",
      "\tÄ Men: 0.016481831669807434\n",
      "\tÄ Mens: 0.014812817797064781\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ |: 0.13869626820087433\n",
      "\tÄŠ: 0.08789342641830444\n",
      "\tÄ Women: 0.04690009728074074\n",
      "\tÄ Mens: 0.027350114658474922\n",
      "\tÄ Fashion: 0.024095362052321434\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (\n",
      "top-5 tokens with their probabilities:\n",
      "\tJ: 0.020951567217707634\n",
      "\tP: 0.02048356458544731\n",
      "\tS: 0.016927532851696014\n",
      "\tF: 0.013931326568126678\n",
      "\tM: 0.013302800245583057\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J\n",
      "top-5 tokens with their probabilities:\n",
      "\tB: 0.18877987563610077\n",
      "\tÄ B: 0.07305929064750671\n",
      "\tBL: 0.03757099062204361\n",
      "\t-: 0.034034356474876404\n",
      "\t.: 0.03157886490225792\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J-\n",
      "top-5 tokens with their probabilities:\n",
      "\tB: 0.26862403750419617\n",
      "\tF: 0.015738721936941147\n",
      "\tJ: 0.013721169903874397\n",
      "\tZ: 0.013625718653202057\n",
      "\tD: 0.011573263444006443\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J-464\n",
      "top-5 tokens with their probabilities:\n",
      "\t): 0.15927687287330627\n",
      "\t-: 0.08195557445287704\n",
      "\t_: 0.03926335647702217\n",
      "\t26: 0.012950148433446884\n",
      "\t01: 0.01269511692225933\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J-464)\n",
      "top-5 tokens with their probabilities:\n",
      "\tÄ |: 0.34809067845344543\n",
      "\tÄŠ: 0.3211340010166168\n",
      "\tÄ Women: 0.016821065917611122\n",
      "\tÄ (: 0.013332066126167774\n",
      "\tÄ Men: 0.010312444530427456\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J-464)<|endoftext|>\n",
      "top-5 tokens with their probabilities:\n",
      "\tA: 0.048344939947128296\n",
      "\tThe: 0.039116889238357544\n",
      "\tI: 0.019759301096200943\n",
      "\tAn: 0.012687191367149353\n",
      "\tM: 0.01223816443234682\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J-464)<|endoftext|><|endoftext|>\n",
      "top-5 tokens with their probabilities:\n",
      "\tA: 0.04838462918996811\n",
      "\tThe: 0.039168719202280045\n",
      "\tI: 0.019773410633206367\n",
      "\tAn: 0.012692473828792572\n",
      "\tM: 0.01225372590124607\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J-464)<|endoftext|><|endoftext|><|endoftext|>\n",
      "top-5 tokens with their probabilities:\n",
      "\tA: 0.04841356351971626\n",
      "\tThe: 0.039214275777339935\n",
      "\tI: 0.019795352593064308\n",
      "\tAn: 0.012697544880211353\n",
      "\tM: 0.012267698533833027\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J-464)<|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "top-5 tokens with their probabilities:\n",
      "\tA: 0.04845459386706352\n",
      "\tThe: 0.03925349935889244\n",
      "\tI: 0.019808046519756317\n",
      "\tAn: 0.012701909989118576\n",
      "\tM: 0.012292248196899891\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J-464)<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "top-5 tokens with their probabilities:\n",
      "\tA: 0.048416946083307266\n",
      "\tThe: 0.03921791538596153\n",
      "\tI: 0.01979280821979046\n",
      "\tAn: 0.012703374959528446\n",
      "\tM: 0.012291697785258293\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J-464)<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "top-5 tokens with their probabilities:\n",
      "\tA: 0.04842724651098251\n",
      "\tThe: 0.03924332186579704\n",
      "\tI: 0.01980018988251686\n",
      "\tAn: 0.012701618485152721\n",
      "\tM: 0.012312992475926876\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J-464)<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "top-5 tokens with their probabilities:\n",
      "\tA: 0.04840807244181633\n",
      "\tThe: 0.039273299276828766\n",
      "\tI: 0.01981063187122345\n",
      "\tAn: 0.012702888809144497\n",
      "\tM: 0.012332368642091751\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J-464)<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "top-5 tokens with their probabilities:\n",
      "\tA: 0.04845805466175079\n",
      "\tThe: 0.03931204974651337\n",
      "\tI: 0.019820649176836014\n",
      "\tAn: 0.012713868170976639\n",
      "\tM: 0.012365557253360748\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J-464)<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "top-5 tokens with their probabilities:\n",
      "\tA: 0.04846290498971939\n",
      "\tThe: 0.03933848813176155\n",
      "\tI: 0.01984487660229206\n",
      "\tAn: 0.012717761099338531\n",
      "\tM: 0.01239692885428667\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J-464)<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "top-5 tokens with their probabilities:\n",
      "\tA: 0.04838502034544945\n",
      "\tThe: 0.03932444378733635\n",
      "\tI: 0.019834160804748535\n",
      "\tAn: 0.012711766175925732\n",
      "\tM: 0.012413889169692993\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J-464)<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "top-5 tokens with their probabilities:\n",
      "\tA: 0.04843578487634659\n",
      "\tThe: 0.03941348195075989\n",
      "\tI: 0.019870726391673088\n",
      "\tAn: 0.012722967192530632\n",
      "\tM: 0.012451966293156147\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J-464)<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "top-5 tokens with their probabilities:\n",
      "\tA: 0.04845067858695984\n",
      "\tThe: 0.0394328236579895\n",
      "\tI: 0.019883207976818085\n",
      "\tAn: 0.012734649702906609\n",
      "\tM: 0.012488434091210365\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J-464)<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "top-5 tokens with their probabilities:\n",
      "\tA: 0.048448458313941956\n",
      "\tThe: 0.039473455399274826\n",
      "\tI: 0.0199135709553957\n",
      "\tAn: 0.01273814681917429\n",
      "\tM: 0.012535398826003075\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J-464)<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "top-5 tokens with their probabilities:\n",
      "\tA: 0.04847821220755577\n",
      "\tThe: 0.03949317708611488\n",
      "\tI: 0.019936900585889816\n",
      "\tAn: 0.012747623957693577\n",
      "\tM: 0.012589500285685062\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J-464)<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "top-5 tokens with their probabilities:\n",
      "\tA: 0.04851244017481804\n",
      "\tThe: 0.03950990363955498\n",
      "\tI: 0.019979001954197884\n",
      "\tAn: 0.012756720185279846\n",
      "\tM: 0.01264856569468975\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J-464)<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "top-5 tokens with their probabilities:\n",
      "\tA: 0.04857737571001053\n",
      "\tThe: 0.03955615311861038\n",
      "\tI: 0.020058780908584595\n",
      "\tAn: 0.012770094908773899\n",
      "\tM: 0.012737302109599113\n",
      "\n",
      "\n",
      "\n",
      "Context: <|endoftext|>J B Fashion Tops for Women | Tops for Women Tops for Women Stylish | top for Girls | top for Women Stylish Latest (J-464)<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "top-5 tokens with their probabilities:\n",
      "\tA: 0.04861058294773102\n",
      "\tThe: 0.039554815739393234\n",
      "\tI: 0.020116953179240227\n",
      "\tM: 0.012814458459615707\n",
      "\tAn: 0.012765862978994846\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show top-k most likely tokens for each context\n",
    "top_k = 5\n",
    "for i in range(len(sentence_next_token_probs)):\n",
    "    top_k_indices = np.argsort(sentence_next_token_probs[i])[::-1][:top_k]\n",
    "    top_k_probs = sentence_next_token_probs[i][top_k_indices]\n",
    "    top_k_tokens = [id_to_token[x] for x in top_k_indices]\n",
    "    print (f\"Context: {tokenizer.decode(batch[0][:i+1])}\")\n",
    "    print (f\"top-{top_k} tokens with their probabilities:\")\n",
    "    for j in range(top_k):\n",
    "        print (f\"\\t{top_k_tokens[j]}: {top_k_probs[j]}\")\n",
    "    print ('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50257])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 36, 50257])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 36])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
